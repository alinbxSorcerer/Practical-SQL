<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis" lang="en-us" xml:lang="en-us">
<head>
<title>Practical SQL: A Beginner’s Guide to Storytelling with Data</title>
<link href="../styles/9781593278458.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:c3d53d4c-356a-4b39-bef0-591c04016b72" name="Adept.expected.resource"/>
</head>
<body>
<h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_155"/><strong><span class="big">10</span></strong><br/><strong>STATISTICAL FUNCTIONS IN SQL</strong></h2>
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>
<p class="noindent">A SQL database isn’t usually the first tool a data analyst chooses when performing statistical analysis that requires more than just calculating sums and averages. Typically, the software of choice would be full-featured statistics packages, such as SPSS or SAS, the programming languages R or Python, or even Excel. However, standard ANSI SQL, including PostgreSQL’s implementation, offers a handful of powerful stats functions that reveal a lot about your data without having to export your data set to another program.</p>
<p class="indent">In this chapter, we’ll explore these SQL stats functions along with guidelines on when to use them. Statistics is a vast subject worthy of its own book, so we’ll only skim the surface here. Nevertheless, you’ll learn how to apply high-level statistical concepts to help you derive meaning from your data using a new data set from the U.S. Census Bureau. You’ll also learn to use SQL to create comparisons using rankings and rates with FBI crime data as our subject.</p>
<h3 class="h3" id="lev158"><span epub:type="pagebreak" id="page_156"/>Creating a Census Stats Table</h3>
<p class="noindent">Let’s return to one of my favorite data sources, the U.S. Census Bureau. In <a href="ch04.xhtml#ch04">Chapters 4</a> and <a href="ch05.xhtml#ch05">5</a>, you used the 2010 Decennial Census to import data and perform basic math and stats. This time you’ll use county data points compiled from the 2011–2015 American Community Survey (ACS) 5-Year Estimates, a separate survey administered by the Census Bureau.</p>
<p class="indent">Use the code in <a href="ch10.xhtml#ch10list1">Listing 10-1</a> to create the table <span class="literal">acs_2011_2015_stats</span> and import the CSV file <em>acs_2011_2015_stats.csv</em>. The code and data are available with all the book’s resources at <em><a href="https://www.nostarch.com/practicalSQL/">https://www.nostarch.com/practicalSQL/</a></em>. Remember to change <span class="codeitalic">C:\YourDirectory\</span> to the location of the CSV file.</p>
<p class="programs">  CREATE TABLE acs_2011_2015_stats (<br/>    <span class="ent">➊</span> geoid varchar(14) CONSTRAINT geoid_key PRIMARY KEY,<br/>      county varchar(50) NOT NULL,<br/>      st varchar(20) NOT NULL,<br/>    <span class="ent">➋</span> pct_travel_60_min numeric(5,3) NOT NULL,<br/>      pct_bachelors_higher numeric(5,3) NOT NULL,<br/>      pct_masters_higher numeric(5,3) NOT NULL,<br/>      median_hh_income integer,<br/>    <span class="ent">➌</span> CHECK (pct_masters_higher &lt;= pct_bachelors_higher)<br/>  );<br/><br/>  COPY acs_2011_2015_stats<br/>  FROM '<em>C:\YourDirectory\</em>acs_2011_2015_stats.csv'<br/>  WITH (FORMAT CSV, HEADER, DELIMITER ',');<br/><br/><span class="ent">➍</span> SELECT * FROM acs_2011_2015_stats;</p>
<p class="listing" id="ch10list1"><em>Listing 10-1: Creating the Census 2011–2015 ACS 5-Year stats table and import data</em></p>
<p class="indent">The <span class="literal">acs_2011_2015_stats</span> table has seven columns. The first three columns <span class="ent">➊</span> include a unique <span class="literal">geoid</span> that serves as the primary key, the name of the <span class="literal">county</span>, and the state name <span class="literal">st</span>. The next four columns display the following three percentages <span class="ent">➋</span> I derived for each county from raw data in the ACS release, plus one more economic indicator:</p>
<p class="hang"><span class="codestrong">pct_travel_60_min</span> The percentage of workers ages 16 and older who commute more than 60 minutes to work.</p>
<p class="hang"><span class="literal"><strong>pct_bachelors_higher</strong></span> The percentage of people ages 25 and older whose level of education is a bachelor’s degree or higher. (In the United States, a bachelor’s degree is usually awarded upon completing a four-year college education.)</p>
<p class="hang"><span class="codestrong">pct_masters_higher</span> The percentage of people ages 25 and older whose level of education is a master’s degree or higher. (In the United States, a master’s degree is the first advanced degree earned after completing a bachelor’s degree.)</p>
<p class="hang"><span class="codestrong">median_hh_income</span> The county’s median household income in 2015 inflation-adjusted dollars. As you learned in <a href="ch05.xhtml#ch05">Chapter 5</a>, a median value is the midpoint in an ordered set of numbers, where half the values are larger than the midpoint and half are smaller. Because averages can be <span epub:type="pagebreak" id="page_157"/>skewed by a few very large or very small values, government reporting on economic data, such as income, tends to use medians. In this column, we omit the <span class="literal">NOT NULL</span> constraint because one county had no data reported.</p>
<p class="indentt">We include the <span class="literal">CHECK</span> constraint <span class="ent">➌</span> you learned in <a href="ch07.xhtml#ch07">Chapter 7</a> to check that the figures for the bachelor’s degree are equal to or higher than those for the master’s degree, because in the United States, a bachelor’s degree is earned before or concurrently with a master’s degree. A county showing the opposite could indicate data imported incorrectly or a column mislabeled. Our data checks out: upon import, there are no errors showing a violation of the <span class="literal">CHECK</span> constraint.</p>
<p class="indent">We use the <span class="literal">SELECT</span> statement <span class="ent">➍</span> to view all 3,142 rows imported, each corresponding to a county surveyed in this Census release.</p>
<p class="indent">Next, we’ll use statistics functions in SQL to better understand the relationships among the percentages.</p>
<div class="sidebar">
<p class="sidebart"><strong>THE DECENNIAL U.S. CENSUS VS. THE AMERICAN COMMUNITY SURVEY</strong></p>
<p class="spara">Each U.S. Census data product has its own methodology. The Decennial Census is a full count of the U.S. population, conducted every 10 years via a form mailed to every household in the country. One of its primary purposes is to determine the number of seats each state holds in the U.S. House of Representatives. In contrast, the ACS is an ongoing annual survey of about 3.5 million U.S. households. It enquires into details about income, education, employment, ancestry, and housing. Private-sector and public-sector organizations alike use ACS data to track trends and make various decisions.</p>
<p class="spara1">Currently, the Census Bureau packages ACS data into two releases: a 1-year data set that provides estimates for geographies with populations of 20,000 or more, and a 5-year data set that includes all geographies. Because it’s a survey, ACS results are estimates and have a margin of error, which I’ve omitted for brevity but which you’ll see included in a full ACS data set.</p>
</div>
<h4 class="h4" id="lev159"><em>Measuring Correlation with corr(Y, X)</em></h4>
<p class="noindent">Researchers often want to understand the relationships between variables, and one such measure of relationships is <em>correlation</em>. In this section, we’ll use the <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> function to measure correlation and investigate what relationship exists, if any, between the percentage of people in a county who’ve attained a bachelor’s degree and the median household income in that county. We’ll also determine whether, according to our data, a better-educated population typically equates to higher income and how strong the relationship between education level and income is if it does.</p>
<p class="indent">First, some background. The <em>Pearson correlation coefficient</em> (generally denoted as <em>r</em>) is a measure for quantifying the strength of a <em>linear <span epub:type="pagebreak" id="page_158"/>relationship</em> between two variables. It shows the extent to which an increase or decrease in one variable correlates to a change in another variable. The <em>r</em> values fall between −1 and 1. Either end of the range indicates a perfect correlation, whereas values near zero indicate a random distribution with no correlation. A positive <em>r</em> value indicates a <em>direct relationship</em>: as one variable increases, the other does too. When graphed on a scatterplot, the data points representing each pair of values in a direct relationship would slope upward from left to right. A negative <em>r</em> value indicates an <em>inverse relationship</em>: as one variable increases, the other decreases. Dots representing an inverse relationship would slope downward from left to right on a scatterplot.</p>
<p class="indent"><a href="ch10.xhtml#ch10tab1">Table 10-1</a> provides general guidelines for interpreting positive and negative <em>r</em> values, although as always with statistics, different statisticians may offer different interpretations.</p>
<p class="tabcap" id="ch10tab1"><strong>Table 10-1:</strong> Interpreting Correlation Coefficients</p>
<table class="topbot-d">
<thead>
<tr>
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>Correlation coefficient (+/−)</strong></p></td>
<td class="table-h" style="vertical-align: top;"><p class="tab_th"><strong>What it could mean</strong></p></td>
</tr>
</thead>
<tbody>
<tr>
<td class="table-c" style="vertical-align: top;"><p class="taba">0</p></td>
<td class="table-c" style="vertical-align: top;"><p class="taba">No relationship</p></td>
</tr>
<tr>
<td class="table-c" style="vertical-align: top;"><p class="taba">.01 to .29</p></td>
<td class="table-c" style="vertical-align: top;"><p class="taba">Weak relationship</p></td>
</tr>
<tr>
<td class="table-c" style="vertical-align: top;"><p class="taba">.3 to .59</p></td>
<td class="table-c" style="vertical-align: top;"><p class="taba">Moderate relationship</p></td>
</tr>
<tr>
<td class="table-c" style="vertical-align: top;"><p class="taba">.6 to .99</p></td>
<td class="table-c" style="vertical-align: top;"><p class="taba">Strong to nearly perfect relationship</p></td>
</tr>
<tr>
<td class="table-ca" style="vertical-align: top;"><p class="taba">1</p></td>
<td class="table-ca" style="vertical-align: top;"><p class="taba">Perfect relationship</p></td>
</tr>
</tbody>
</table>
<p class="indent">In standard ANSI SQL and PostgreSQL, we calculate the Pearson correlation coefficient using <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span>. It’s one of several <em>binary aggregate functions</em> in SQL and is so named because these functions accept two inputs. In binary aggregate functions, the input <span class="codeitalic">Y</span> is the <em>dependent variable</em> whose variation depends on the value of another variable, and <span class="codeitalic">X</span> is the <em>independent variable</em> whose value doesn’t depend on another variable.</p>
<div class="note">
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>
<p class="notep"><em>Even though SQL specifies the <span class="literal">Y</span> and <span class="literal">X</span> inputs for the <span class="literal">corr()</span> function, correlation calculations don’t distinguish between dependent and independent variables. Switching the order of inputs in <span class="literal">corr()</span> produces the same result. However, for convenience and readability, these examples order the input variables according to dependent and independent.</em></p>
</div>
<p class="indent">We’ll use the <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> function to discover the relationship between education level and income. Enter the code in <a href="ch10.xhtml#ch10list2">Listing 10-2</a> to use <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> with the <span class="literal">median_hh_income</span> and <span class="literal">pct_bachelors_higher</span> variables as inputs:</p>
<p class="programs">SELECT corr(median_hh_income, pct_bachelors_higher)<br/>    AS bachelors_income_r<br/>FROM acs_2011_2015_stats;</p>
<p class="listing" id="ch10list2"><em>Listing 10-2: Using <span class="literal">corr(Y, X)</span> to measure the relationship between education and income</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_159"/>Run the query; your result should be an <em>r</em> value of just above .68 given as the floating-point <span class="literal">double precision</span> data type:</p>
<p class="programs">bachelors_income_r<br/>------------------<br/>0.682185675451399</p>
<p class="indent">This positive <em>r</em> value indicates that as a county’s educational attainment increases, household income tends to increase. The relationship isn’t perfect, but the <em>r</em> value shows the relationship is fairly strong. We can visualize this pattern by plotting the variables on a scatterplot using Excel, as shown in <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>. Each data point represents one U.S. county; the data point’s position on the x-axis shows the percentage of the population ages 25 and older that have a bachelor’s degree or higher. The data point’s position on the y-axis represents the county’s median household income.</p>
<div class="image"><a id="ch10fig1"/><img alt="image" src="../images/f0159-01.jpg"/></div>
<p class="figcap"><em>Figure 10-1: A scatterplot showing the relationship between education and income</em></p>
<p class="indent">Notice that although most of the data points are grouped together in the bottom-left corner of the graph, they do generally slope upward from left to right. Also, the points spread out rather than strictly follow a straight line. If they were in a straight line sloping up from left to right, the <em>r</em> value would be 1, indicating a perfect positive linear relationship.</p>
<h4 class="h4" id="lev160"><em>Checking Additional Correlations</em></h4>
<p class="noindent">Now let’s calculate the correlation coefficients for the remaining variable pairs using the code in <a href="ch10.xhtml#ch10list3">Listing 10-3</a>:</p>
<p class="programs">SELECT<br/>  <span class="ent">➊</span> round(<br/>      corr(median_hh_income, pct_bachelors_higher)::numeric, 2<br/>      ) AS bachelors_income_r,<br/><span epub:type="pagebreak" id="page_160"/>    round(<br/>      corr(pct_travel_60_min, median_hh_income)::numeric, 2<br/>      ) AS income_travel_r,<br/>    round(<br/>      corr(pct_travel_60_min, pct_bachelors_higher)::numeric, 2<br/>      ) AS bachelors_travel_r<br/>FROM acs_2011_2015_stats;</p>
<p class="listing" id="ch10list3"><em>Listing 10-3: Using <span class="literal">corr(Y, X)</span> on additional variables</em></p>
<p class="indent">This time we’ll make the output more readable by rounding off the decimal values. We’ll do this by wrapping the <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> function inside SQL’s <span class="literal">round()</span> function <span class="ent">➊</span>, which takes two inputs: the <span class="literal">numeric</span> value to be rounded and an <span class="literal">integer</span> value indicating the number of decimal places to round the first value. If the second parameter is omitted, the value is rounded to the nearest whole integer. Because <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> returns a floating-point value by default, we’ll change it to the <span class="literal">numeric</span> type using the <span class="literal">::</span> notation you learned in <a href="ch03.xhtml#ch03">Chapter 3</a>. Here’s the output:</p>
<p class="programs">bachelors_income_r    income_travel_r    bachelors_travel_r<br/>------------------    ---------------    ------------------<br/>              0.68               0.05                 -0.14</p>
<p class="indent">The <span class="literal">bachelors_income_r</span> value is <span class="literal">0.68</span>, which is the same as our first run but rounded to two decimal places. Compared to <span class="literal">bachelors_income_r</span>, the other two correlations are weak.</p>
<p class="indent">The <span class="literal">income_travel_r</span> value shows that the correlation between income and the percentage of those who commute more than an hour to work is practically zero. This indicates that a county’s median household income bears little connection to how long it takes people to get to work.</p>
<p class="indent">The <span class="literal">bachelors_travel_r</span> value shows that the correlation of bachelor’s degrees and commuting is also low at <span class="literal">-0.14</span>. The negative value indicates an inverse relationship: as education increases, the percentage of the population that travels more than an hour to work decreases. Although this is interesting, a correlation coefficient that is this close to zero indicates a weak relationship.</p>
<p class="indent">When testing for correlation, we need to note some caveats. The first is that even a strong correlation does not imply causality. We can’t say that a change in one variable causes a change in the other, only that the changes move together. The second is that correlations should be subject to testing to determine whether they’re statistically significant. Those tests are beyond the scope of this book but worth studying on your own.</p>
<p class="indent">Nevertheless, the SQL <span class="literal">corr(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> function is a handy tool for quickly checking correlations between variables.</p>
<h4 class="h4" id="lev161"><em>Predicting Values with Regression Analysis</em></h4>
<p class="noindent">Researchers not only want to understand relationships between variables; they also want to predict values using available data. For example, let’s say 30 percent of a county’s population has a bachelor’s degree or higher. Given <span epub:type="pagebreak" id="page_161"/>the trend in our data, what would we expect that county’s median household income to be? Likewise, for each percent increase in education, how much increase, on average, would we expect in income?</p>
<p class="indent">We can answer both questions using <em>linear regression</em>. Simply put, the regression method finds the best linear equation, or straight line, that describes the relationship between an independent variable (such as education) and a dependent variable (such as income). Standard ANSI SQL and PostgreSQL include functions that perform linear regression.</p>
<p class="indent"><a href="ch10.xhtml#ch10fig2">Figure 10-2</a> shows our previous scatterplot with a regression line added.</p>
<div class="image"><a id="ch10fig2"/><img alt="image" src="../images/f0161-01.jpg"/></div>
<p class="figcap"><em>Figure 10-2: Scatterplot with least squares regression line showing the relationship between education and income</em></p>
<p class="indent">The straight line running through the middle of all the data points is called the <em>least squares regression line</em>, which approximates the “best fit” for a straight line that best describes the relationship between the variables. The equation for the regression line is like the <em>slope-intercept</em> formula you might remember from high school math but written using differently named variables: <em>Y</em> = <em>bX</em> + <em>a</em>. Here are the formula’s components:</p>
<p class="hang"><strong><em>Y</em></strong> is the predicted value, which is also the value on the y-axis, or dependent variable.</p>
<p class="hang"><strong><em>b</em></strong> is the slope of the line, which can be positive or negative. It measures how many units the y-axis value will increase or decrease for each unit of the x-axis value.</p>
<p class="hang"><strong><em>X</em></strong> represents a value on the x-axis, or independent variable.</p>
<p class="hang"><strong><em>a</em></strong> is the y-intercept, the value at which the line crosses the y-axis when the <em>X</em> value is zero.</p>
<p class="indentt">Let’s apply this formula using SQL. Earlier, we questioned what the expected median household income in a county would be if the percentage of people with a bachelor’s degree or higher in that county was 30 percent. <span epub:type="pagebreak" id="page_162"/>In our scatterplot, the percentage with bachelor’s degrees falls along the x-axis, represented by <em>X</em> in the calculation. Let’s plug that value into the regression line formula in place of <em>X</em>:</p>
<p class="center"><em>Y</em> = <em>b</em>(30) + <em>a</em></p>
<p class="indent">To calculate <em>Y</em>, which represents the predicted median household income, we need the line’s slope, <em>b</em>, and the y-intercept, <em>a</em>. To get these values, we’ll use the SQL functions <span class="literal">regr_slope(</span><span class="codeitalic">Y</span><span class="literal">,</span> <span class="codeitalic">X</span><span class="literal">)</span> and <span class="literal">regr_intercept(</span><span class="codeitalic">Y</span><span class="literal">,</span> <span class="codeitalic">X</span><span class="literal">)</span>, as shown in <a href="ch10.xhtml#ch10list4">Listing 10-4</a>:</p>
<p class="programs">SELECT<br/>    round(<br/>        regr_slope(median_hh_income, pct_bachelors_higher)::numeric, 2<br/>        ) AS slope,<br/>    round(<br/>        regr_intercept(median_hh_income, pct_bachelors_higher)::numeric, 2<br/>        ) AS y_intercept<br/>FROM acs_2011_2015_stats;</p>
<p class="listing" id="ch10list4"><em>Listing 10-4: Regression slope and intercept functions</em></p>
<p class="indent">Using the <span class="literal">median_hh_income</span> and <span class="literal">pct_bachelors_higher</span> variables as inputs for both functions, we’ll set the resulting value of the <span class="literal">regr_slope(</span><span class="codeitalic">Y</span><span class="literal">,</span> <span class="codeitalic">X</span><span class="literal">)</span> function as <span class="literal">slope</span> and the output for the <span class="literal">regr_intercept(</span><span class="codeitalic">Y</span><span class="literal">,</span> <span class="codeitalic">X</span><span class="literal">)</span> function as <span class="literal">y_intercept</span>.</p>
<p class="indent">Run the query; the result should show the following:</p>
<p class="programs">slope     y_intercept<br/>------    -----------<br/>926.95       27901.15</p>
<p class="indent">The <span class="literal">slope</span> value shows that for every one-unit increase in bachelor’s degree percentage, we can expect a county’s median household income will increase by 926.95. Slope always refers to change per one unit of <em>X</em>. The <span class="literal">y_intercept</span> value shows that when the regression line crosses the y-axis, where the percentage with bachelor’s degrees is at 0, the y-axis value is 27901.15. Now let’s plug both values into the equation to get the Y value:</p>
<p class="center"><em>Y</em> = 926.95(30) + 27901.15</p>
<p class="center"><em>Y</em> = 55709.65</p>
<p class="indent">Based on our calculation, in a county in which 30 percent of people age 25 and older have a bachelor’s degree or higher, we can expect a median household income in that county to be about $55,710. Of course, our data includes counties whose median income falls above and below that predicted value, but we expect this to be the case because our data points in the scatterplot don’t line up perfectly along the regression line. Recall that the correlation coefficient we calculated was 0.68, indicating a strong but not perfect relationship between education and income. Other factors probably contributed to variations in income as well.</p>
<h4 class="h4" id="lev162"><span epub:type="pagebreak" id="page_163"/><em>Finding the Effect of an Independent Variable with r-squared</em></h4>
<p class="noindent">Earlier in the chapter, we calculated the correlation coefficient, <em>r</em>, to determine the direction and strength of the relationship between two variables. We can also calculate the extent that the variation in the <em>x</em> (independent) variable explains the variation in the <em>y</em> (dependent) variable by squaring the <em>r</em> value to find the <em>coefficient of determination</em>, better known as <em>r-squared</em>. An <em>r</em>-squared value is between zero and one and indicates the percentage of the variation that is explained by the independent variable. For example, if <em>r</em>-squared equals .1, we would say that the independent variable explains 10 percent of the variation in the dependent variable, or not much at all.</p>
<p class="indent">To find <em>r</em>-squared, we use the <span class="literal">regr_r2(</span><span class="codeitalic">Y,</span> <span class="codeitalic">X</span><span class="literal">)</span> function in SQL. Let’s apply it to our education and income variables using the code in <a href="ch10.xhtml#ch10list5">Listing 10-5</a>:</p>
<p class="programs">SELECT round(<br/>        regr_r2(median_hh_income, pct_bachelors_higher)::numeric, 3<br/>        ) AS r_squared<br/>FROM acs_2011_2015_stats;</p>
<p class="listing" id="ch10list5"><em>Listing 10-5: Calculating the coefficient of determination, or</em> r-<em>squared</em></p>
<p class="indent">This time we’ll round off the output to the nearest thousandth place and set the result to <span class="literal">r_squared</span>. The query should return the following result:</p>
<p class="programs">r_squared<br/>---------<br/>    0.465</p>
<p class="indent">The <em>r</em>-squared value of <span class="literal">0.465</span> indicates that about 47 percent of the variation in median household income in a county can be explained by the percentage of people with a bachelor’s degree or higher in that county. What explains the other 53 percent of the variation in household income? Any number of factors could explain the rest of the variation, and statisticians will typically test numerous combinations of variables to determine what they are.</p>
<p class="indent">But before you use these numbers in a headline or presentation, it’s worth revisiting the following points:</p>
<ol>
<li class="noindent"><p class="list">Correlation doesn’t prove causality. For verification, do a Google search on “correlation and causality.” Many variables correlate well but have no meaning. (See <em><a href="http://www.tylervigen.com/spurious-correlations">http://www.tylervigen.com/spurious-correlations</a></em> for examples of correlations that don’t prove causality, including the correlation between divorce rate in Maine and margarine consumption.) Statisticians usually perform <em>significance testing</em> on the results to make sure values are not simply the result of randomness.</p></li>
<li class="noindent"><p class="list">Statisticians also apply additional tests to data before accepting the results of a regression analysis, including whether the variables follow the standard bell curve distribution and meet other criteria for a valid result.</p></li>
</ol>
<p class="indent"><span epub:type="pagebreak" id="page_164"/>Given these factors, SQL’s statistics functions are useful as a preliminary survey of your data before doing more rigorous analysis. If your work involves statistics, a full study on performing regression is worthwhile.</p>
<h3 class="h3" id="lev163">Creating Rankings with SQL</h3>
<p class="noindent">Rankings make the news often. You’ll see them used anywhere from weekend box office charts to a sports team’s league standings. You’ve already learned how to order query results based on values in a column, but SQL lets you go further and create numbered rankings. Rankings are useful for data analysis in several ways, such as tracking changes over time if you have several years’ worth of data. You can also simply use a ranking as a fact on its own in a report. Let’s explore how to create rankings using SQL.</p>
<h4 class="h4" id="lev164"><em>Ranking with rank() and dense_rank()</em></h4>
<p class="noindent">Standard ANSI SQL includes several ranking functions, but we’ll just focus on two: <span class="literal">rank()</span> and <span class="literal">dense_rank()</span>. Both are <em>window functions</em>, which perform calculations across sets of rows we specify using the <span class="literal">OVER</span> clause. Unlike aggregate functions, which group rows while calculating results, window functions present results for each row in the table.</p>
<p class="indent">The difference between <span class="literal">rank()</span> and <span class="literal">dense_rank()</span> is the way they handle the next rank value after a tie: <span class="literal">rank()</span> includes a gap in the rank order, but <span class="literal">dense_rank()</span> does not. This concept is easier to understand in action, so let’s look at an example. Consider a Wall Street analyst who covers the highly competitive widget manufacturing market. The analyst wants to rank companies by their annual output. The SQL statements in <a href="ch10.xhtml#ch10list6">Listing 10-6</a> create and fill a table with this data and then rank the companies by widget output:</p>
<p class="programs">CREATE TABLE widget_companies (<br/>    id bigserial,<br/>    company varchar(30) NOT NULL,<br/>    widget_output integer NOT NULL<br/>);<br/><br/>INSERT INTO widget_companies (company, widget_output)<br/>VALUES<br/>    ('Morse Widgets', 125000),<br/>    ('Springfield Widget Masters', 143000),<br/>    ('Best Widgets', 196000),<br/>    ('Acme Inc.', 133000),<br/>    ('District Widget Inc.', 201000),<br/>    ('Clarke Amalgamated', 620000),<br/>    ('Stavesacre Industries', 244000),<br/>    ('Bowers Widget Emporium', 201000);<br/><br/>SELECT<br/>    company,<br/>    widget_output,<br/>  <span class="ent">➊</span> rank() OVER (ORDER BY widget_output DESC),<br/><span epub:type="pagebreak" id="page_165"/>  <span class="ent">➋</span> dense_rank() OVER (ORDER BY widget_output DESC)<br/>FROM widget_companies;</p>
<p class="listing" id="ch10list6"><em>Listing 10-6: Using the <span class="literal">rank()</span> and <span class="literal">dense_rank()</span> window functions</em></p>
<p class="indent">Notice the syntax in the <span class="literal">SELECT</span> statement that includes <span class="literal">rank()</span> <span class="ent">➊</span> and <span class="literal">dense_rank()</span> <span class="ent">➋</span>. After the function names, we use the <span class="literal">OVER</span> clause and in parentheses place an expression that specifies the “window” of rows the function should operate on. In this case, we want both functions to work on all rows of the <span class="literal">widget_output</span> column, sorted in descending order. Here’s the output:</p>
<p class="programs">company                       widget_output    rank    dense_rank<br/>--------------------------    -------------    ----    ----------<br/>Clarke Amalgamated                   620000       1             1<br/>Stavesacre Industries                244000       2             2<br/>Bowers Widget Emporium               201000       3             3<br/>District Widget Inc.                 201000       3             3<br/>Best Widgets                         196000       5             4<br/>Springfield Widget Masters           143000       6             5<br/>Acme Inc.                            133000       7             6<br/>Morse Widgets                        125000       8             7</p>
<p class="indent">The columns produced by the <span class="literal">rank()</span> and <span class="literal">dense_rank()</span> functions show each company’s ranking based on the <span class="literal">widget_output</span> value from highest to lowest, with Clarke Amalgamated at number one. To see how <span class="literal">rank()</span> and <span class="literal">dense_rank()</span> differ, check the fifth row listing, Best Widgets.</p>
<p class="indent">With <span class="literal">rank()</span>, Best Widgets is the fifth highest ranking company, showing there are four companies with more output and there is no company ranking in fourth place, because <span class="literal">rank()</span> allows a gap in the order when a tie occurs. In contrast, <span class="literal">dense_rank()</span>, which doesn’t allow a gap in the rank order, reflects the fact that Best Widgets has the fourth highest output number regardless of how many companies produced more. Therefore, Best Widgets ranks in fourth place using <span class="literal">dense_rank()</span>.</p>
<p class="indent">Both ways of handling ties have merit, but in practice <span class="literal">rank()</span> is used most often. It’s also what I recommend using, because it more accurately reflects the total number of companies ranked, shown by the fact that Best Widgets has four companies ahead of it in total output, not three.</p>
<p class="indent">Let’s look at a more complex ranking example.</p>
<h4 class="h4" id="lev165"><em>Ranking Within Subgroups with PARTITION BY</em></h4>
<p class="noindent">The ranking we just did was a simple overall ranking based on widget output. But sometimes you’ll want to produce ranks within groups of rows in a table. For example, you might want to rank government employees by salary within each department or rank movies by box office earnings within each genre.</p>
<p class="indent">To use window functions in this way, we’ll add <span class="literal">PARTITION BY</span> to the <span class="literal">OVER</span> clause. A <span class="literal">PARTITION BY</span> clause divides table rows according to values in a column we specify.</p>
<p class="indent"><span epub:type="pagebreak" id="page_166"/>Here’s an example using made-up data about grocery stores. Enter the code in <a href="ch10.xhtml#ch10list7">Listing 10-7</a> to fill a table called <span class="literal">store_sales</span>:</p>
<p class="programs">CREATE TABLE store_sales (<br/>    store varchar(30),<br/>    category varchar(30) NOT NULL,<br/>    unit_sales bigint NOT NULL,<br/>    CONSTRAINT store_category_key PRIMARY KEY (store, category)<br/>);<br/><br/>INSERT INTO store_sales (store, category, unit_sales)<br/>VALUES<br/>    ('Broders', 'Cereal', 1104),<br/>    ('Wallace', 'Ice Cream', 1863),<br/>    ('Broders', 'Ice Cream', 2517),<br/>    ('Cramers', 'Ice Cream', 2112),<br/>    ('Broders', 'Beer', 641),<br/>    ('Cramers', 'Cereal', 1003),<br/>    ('Cramers', 'Beer', 640),<br/>    ('Wallace', 'Cereal', 980),<br/>    ('Wallace', 'Beer', 988);<br/><br/>SELECT<br/>    category,<br/>    store,<br/>    unit_sales,<br/>  <span class="ent">➊</span> rank() OVER (PARTITION BY category ORDER BY unit_sales DESC)<br/>FROM store_sales;</p>
<p class="listing" id="ch10list7"><em>Listing 10-7: Applying <span class="literal">rank()</span> within groups using <span class="literal">PARTITION BY</span></em></p>
<p class="indent">In the table, each row includes a store’s product category and sales for that category. The final <span class="literal">SELECT</span> statement creates a result set showing how each store’s sales ranks within each category. The new element is the addition of <span class="literal">PARTITION BY</span> in the <span class="literal">OVER</span> clause <span class="ent">➊</span>. In effect, the clause tells the program to create rankings one category at a time, using the store’s unit sales in descending order. Here’s the output:</p>
<p class="programs">category     store      unit_sales    rank<br/>---------    -------    ----------    ----<br/>Beer         Wallace           988       1<br/>Beer         Broders           641       2<br/>Beer         Cramers           640       3<br/>Cereal       Broders          1104       1<br/>Cereal       Cramers          1003       2<br/>Cereal       Wallace           980       3<br/>Ice Cream    Broders          2517       1<br/>Ice Cream    Cramers          2112       2<br/>Ice Cream    Wallace          1863       3</p>
<p class="indent">Notice that category names are ordered and grouped in the <span class="literal">category</span> column as a result of <span class="literal">PARTITION BY</span> in the <span class="literal">OVER</span> clause. Rows for each category are ordered by category unit sales with the <span class="literal">rank</span> column displaying the ranking.</p>
<p class="indent"><span epub:type="pagebreak" id="page_167"/>Using this table, we can see at a glance how each store ranks in a food category. For instance, Broders tops sales for cereal and ice cream, but Wallace wins in the beer category. You can apply this concept to many other scenarios: for example, for each auto manufacturer, finding the vehicle with the most consumer complaints; figuring out which month had the most rainfall in each of the last 20 years; finding the team with the most wins against left-handed pitchers; and so on.</p>
<p class="indent">SQL offers additional window functions. Check the official PostgreSQL documentation at <em><a href="https://www.postgresql.org/docs/current/static/tutorial-window.html">https://www.postgresql.org/docs/current/static/tutorial-window.html</a></em> for an overview of window functions, and check <em><a href="https://www.postgresql.org/docs/current/static/functions-window.html">https://www.postgresql.org/docs/current/static/functions-window.html</a></em> for a listing of window functions.</p>
<h3 class="h3" id="lev166">Calculating Rates for Meaningful Comparisons</h3>
<p class="noindent">As helpful and interesting as they are, rankings based on raw counts aren’t always meaningful; in fact, they can actually be misleading. Consider this example of crime statistics: according to the U.S. Federal Bureau of Investigation (FBI), in 2015, New York City reported about 130,000 property crimes, which included burglary, larceny, motor vehicle thefts, and arson. Meanwhile, Chicago reported about 80,000 property crimes the same year.</p>
<p class="indent">So, you’re more likely to find trouble in New York City, right? Not necessarily. In 2015, New York City had more than 8 million residents, whereas Chicago had 2.7 million. Given that context, just comparing the total numbers of property crimes in the two cities isn’t very meaningful.</p>
<p class="indent">A more accurate way to compare these numbers is to turn them into rates. Analysts often calculate a rate per 1,000 people, or some multiple of that number, for apples-to-apples comparisons. For the property crimes in this example, the math is simple: divide the number of offenses by the population and then multiply that quotient by 1,000. For example, if a city has 80 vehicle thefts and a population of 15,000, you can calculate the rate of vehicle thefts per 1,000 people as follows:</p>
<p class="center">(80 / 15,000) × 1,000 = 5.3 vehicle thefts per thousand residents</p>
<p class="indent">This is easy math with SQL, so let’s try it using select city-level data I compiled from the FBI’s <em>2015 Crime in the United States</em> report available at <em><a href="https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/home">https://ucr.fbi.gov/crime-in-the-u.s/2015/crime-in-the-u.s.-2015/home</a></em>. <a href="ch10.xhtml#ch10list8">Listing 10-8</a> contains the code to create and fill a table. Remember to point the script to the location in which you’ve saved the CSV file, which you can download at <em><a href="https://www.nostarch.com/practicalSQL/">https://www.nostarch.com/practicalSQL/</a></em>.</p>
<p class="programs">CREATE TABLE fbi_crime_data_2015 (<br/>    st varchar(20),<br/>    city varchar(50),<br/>    population integer,<br/>    violent_crime integer,<br/>    property_crime integer,<br/>    burglary integer,<br/><span epub:type="pagebreak" id="page_168"/>    larceny_theft integer,<br/>    motor_vehicle_theft integer,<br/>    CONSTRAINT st_city_key PRIMARY KEY (st, city)<br/>);<br/><br/>COPY fbi_crime_data_2015<br/>FROM '<em>C:\YourDirectory\</em>fbi_crime_data_2015.csv'<br/>WITH (FORMAT CSV, HEADER, DELIMITER ',');<br/><br/>SELECT * FROM fbi_crime_data_2015<br/>ORDER BY population DESC;</p>
<p class="listing" id="ch10list8"><em>Listing 10-8: Creating and filling a 2015 FBI crime data table</em></p>
<p class="indent">The <span class="literal">fbi_crime_data_2015</span> table includes the state, city name, and population for that city. Next is the number of crimes reported by police in categories, including violent crime, vehicle thefts, and property crime. To calculate property crimes per 1,000 people in cities with more than 500,000 people and order them, we’ll use the code in <a href="ch10.xhtml#ch10list9">Listing 10-9</a>:</p>
<p class="programs">SELECT<br/>    city,<br/>    st,<br/>    population,<br/>    property_crime,<br/>    round(<br/>      <span class="ent">➊</span> (property_crime::numeric / population) * 1000, 1<br/>        ) AS pc_per_1000<br/>FROM fbi_crime_data_2015<br/>WHERE population &gt;= 500000<br/>ORDER BY (property_crime::numeric / population) DESC;</p>
<p class="listing" id="ch10list9"><em>Listing 10-9: Finding property crime rates per thousand in cities with 500,000 or more people</em></p>
<p class="indent">In <a href="ch05.xhtml#ch05">Chapter 5</a>, you learned that when dividing an integer by an integer, one of the values must be a <span class="literal">numeric</span> or <span class="literal">decimal</span> for the result to include decimal places. We do that in the rate calculation <span class="ent">➊</span> with PostgreSQL’s double-colon shorthand. Because we don’t need to see many decimal places, we wrap the statement in the <span class="literal">round()</span> function to round off the output to the nearest tenth. Then we give the calculated column an alias of <span class="literal">pc_per_1000</span> for easy reference. Here’s a portion of the result set:</p>
<div class="image"><img alt="image" src="../images/prog_page_168.jpg"/></div>
<p class="indent"><span epub:type="pagebreak" id="page_169"/>Tucson, Arizona, has the highest rate of property crimes, followed by San Francisco, California. At the bottom is New York City, with a rate that’s one-fourth of Tucson’s. If we had compared the cities based solely on the raw numbers of property crimes, we’d have a far different result than the one we derived by calculating the rate per thousand.</p>
<p class="indent">I’d be remiss not to point out that the FBI website at <em><a href="https://ucr.fbi.gov/ucr-statistics-their-proper-use/">https://ucr.fbi.gov/ucr-statistics-their-proper-use/</a></em> discourages creating rankings from its crime data, stating that doing so creates “misleading perceptions which adversely affect geographic entities and their residents.” They point out that variations in crimes and crime rates across the country are often due to a number of factors ranging from population density to economic conditions and even the climate. Also, the FBI’s crime data has well-documented short­comings, including incomplete reporting by police agencies.</p>
<p class="indent">That said, asking why a locality has higher or lower crime rates than others is still worth pursuing, and rates do provide some measure of comparison despite certain limitations.</p>
<h3 class="h3" id="lev167">Wrapping Up</h3>
<p class="noindent">That wraps up our exploration of statistical functions in SQL, rankings, and rates. Now your SQL analysis toolkit includes ways to find relationships among variables using statistics functions, create rankings from ordered data, and properly compare raw numbers by turning them into rates. That toolkit is starting to look impressive!</p>
<p class="indent">Next, we’ll dive deeper into date and time data, using SQL functions to extract the information we need.</p>
<div class="sidebar" id="ch10sb1">
<p class="sidebart"><strong>TRY IT YOURSELF</strong></p>
<p class="spara">Test your new skills with the following questions:</p>
<ol>
<li class="noindent"><p class="list">In <a href="ch10.xhtml#ch10list2">Listing 10-2</a>, the correlation coefficient, or <em>r</em> value, of the variables <span class="literal">pct_bachelors</span><span class="literal">_higher</span> and <span class="literal">median_hh_income</span> was about .68. Write a query using the same data set to show the correlation between <span class="literal">pct_masters</span><span class="literal">_higher</span> and <span class="literal">median_hh_income</span>. Is the <em>r</em> value higher or lower? What might explain the difference?</p></li>
<li class="noindent"><p class="list">In the FBI crime data, which cities with a population of 500,000 or more have the highest rates of motor vehicle thefts (column <span class="literal">motor_vehicle_theft</span>)? Which have the highest violent crime rates (column <span class="literal">violent_crime</span>)?</p></li>
<li class="noindent"><p class="list">As a bonus challenge, revisit the libraries data in the table <span class="literal">pls_fy2014_pupld14a</span> in <a href="ch08.xhtml#ch08">Chapter 8</a>. Rank library agencies based on the rate of visits per 1,000 population (column <span class="literal">popu_lsa</span>), and limit the query to agencies serving 250,000 people or more.<span epub:type="pagebreak" id="page_170"/></p></li>
</ol>
</div>
</body>
</html>