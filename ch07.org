* 7.Table Design That Works For You

Obsession with detail can be a good thing. When you're running out the door, it's reassuring to know your keys will be hanging on the hook where you /always/ leave them. The same holds true for database design. When you need to excavate a nugget of information from dozens of tables and millions of rows, you'll appreciate a dose of that same detail obsession. When you organize data into a finely tuned, smartly named set of tables, the analysis experience becomes more manageable.

In this chapter, I'll build on [[file:ch06.xhtml#ch06][Chapter 6]] by introducing /best practices/ for organizing and tuning SQL databases, whether they're yours or ones you inherit for analysis. You already know how to create basic tables and add columns with the appropriate data type and a primary key. Now, we'll dig deeper into table design by exploring naming rules and conventions, ways to maintain the integrity of your data, and how to add indexes to tables to speed up queries.

** Naming Tables, Columns, and Other Identifiers


Developers tend to follow different SQL style patterns when naming tables, columns, and other objects (called /identifiers/). Some prefer to use /camel case/, as in berrySmoothie, where words are strung together and the first letter of each word is capitalized except for the first word. /Pascal case/, as in BerrySmoothie, follows a similar pattern but capitalizes the first letter of the first word too. With /snake case/, as in berry_smoothie, all the words are lowercase and separated by underscores. So far, I've been using snake case in most of the examples, such as in the table us_counties_2010.

You'll find passionate supporters of each naming convention, and some preferences are tied to individual database applications or programming languages. For example, Microsoft recommends Pascal case for its SQL Server users. Whichever convention you prefer, it's most important to choose a style and apply it consistently. Be sure to check whether your organization has a style guide or offer to collaborate on one, and then follow it religiously.

Mixing styles or following none generally leads to a mess. It will be difficult to know which table is the most current, which is the backup, or the difference between two similarly named tables. For example, imagine connecting to a database and finding the following collection of tables:

Customers
customers
custBackup
customer_analysis
customer_test2
customer_testMarch2012
customeranalysis

In addition, working without a consistent naming scheme makes it problematic for others to dive into your data and makes it challenging for you to pick up where you left off.

Let's explore considerations related to naming identifiers and suggestions for best practices.

*** Using Quotes Around Identifiers to Enable Mixed Case/


Standard ANSI SQL and many database-specific variants of SQL treat identifiers as case-insensitive unless you provide a delimiter around them---typically double quotes. Consider these two hypothetical CREATE TABLE statements for PostgreSQL:

CREATE TABLE customers (
    customer_id serial,
    /--snip--/
);

CREATE TABLE Customers (
    customer_id serial,
    /--snip--/
);

When you execute these statements in order, the first CREATE TABLE command creates a table called customers. But rather than creating a second table called Customers, the second statement will throw an error: relation "customers" already exists. Because you didn't quote the identifier, PostgreSQL treats customers and Customers as the same identifier, disregarding the case. If you want to preserve the uppercase letter and create a separate table named Customers, you must surround the identifier with quotes, like this:

CREATE TABLE "Customers" (
    customer_id serial,
    /--snip--/
);

Now, PostgreSQL retains the uppercase C and creates Customers as well as customers. Later, to query Customers rather than customers, you'll have to quote its name in the SELECT statement:

SELECT * FROM "Customers";

Of course, you wouldn't want two tables with such similar names because of the high risk of a mix-up. This example simply illustrates the behavior of SQL in

*** Pitfalls with Quoting Identifiers/


Using quotation marks also permits characters not otherwise allowed in an identifier, including spaces. But be aware of the negatives of using this method: for example, you might want to throw quotes around "trees planted" and use that as a column name in a reforestation database, but then all users will have to provide quotes on every subsequent reference to that column. Omit the quotes and the database will respond with an error, identifying trees and planted as separate columns missing a comma between them. A more readable and reliable option is to use snake case, as in trees_planted.

Another downside to quoting is that it lets you use SQL /reserved keywords/, such as TABLE, WHERE, or SELECT, as an identifier. Reserved keywords are words SQL designates as having special meaning in the language. Most database developers frown on using reserved keywords as identifiers. At a minimum it's confusing, and at worst neglecting or forgetting to quote that keyword later will result in an error because the database will interpret the word as a command instead of an identifier.

*NOTE*

/For PostgreSQL, you can find a list of keywords documented at/ [[https://www.postgresql.org/docs/current/static/sql-keywords-appendix.html]]. In addition, many code editors and database tools, including pgAdmin, will automatically highlight keywords in a particular color.

*** Guidelines for Naming Identifiers/


Given the extra burden of quoting and its potential problems, it's best to keep your identifier names simple, unquoted, and consistent. Here are my recommendations:

- *Use snake case.* Snake case is readable and reliable, as shown in the earlier trees_planted example. It's used throughout the official PostgreSQL documentation and helps make multiword names easy to understand: video_on_demand makes more sense at a glance than videoondemand.
- *Make names easy to understand and avoid cryptic abbreviations.* If you're building a database related to travel, arrival_time is a better reminder of the content as a column name than arv_tm.
- *For table names, use plurals.* Tables hold rows, and each row represents one instance of an entity. So, use plural names for tables, such as teachers, vehicles, or departments.
- *Mind the length.* The maximum number of characters allowed for an identifier name varies by database application: the SQL standard is 128 characters, but PostgreSQL limits you to 63, and the Oracle system maximum is 30. If you're writing code that may get reused in another database system, lean toward shorter identifier names.
- *When making copies of tables, use names that will help you manage them later.* One method is to append a YYYY_MM_DD date to the table name when you create it, such as tire_sizes_2017_10_20. An additional benefit is that the table names will sort in date order.

** Controlling Column Values with Constraints


A column's data type already broadly defines the kind of data it will accept: integers versus characters, for example. But SQL provides several additional constraints that let us further specify acceptable values for a column based on rules and logical tests. With constraints, we can avoid the “garbage in, garbage out” phenomenon, which is what happens when poor-quality data result in inaccurate or incomplete analysis. Constraints help maintain the quality of the data and ensure the integrity of the relationships among tables.

In [[file:ch06.xhtml#ch06][Chapter 6]], you learned about /primary/ and /foreign keys/, which are two of the most commonly used constraints. Let's review them as well as the following additional constraint types:

CHECK Evaluates whether the data falls within values we specify

UNIQUE Ensures that values in a column or group of columns are unique in each row in the table

NOT NULL Prevents NULL values in a column

We can add constraints in two ways: as a /column constraint/ or as a /table constraint/. A column constraint only applies to that column. It's declared with the column name and data type in the CREATE TABLE statement, and it gets checked whenever a change is made to the column. With a table constraint, we can supply criteria that apply to one or more columns. We declare it in the CREATE TABLE statement immediately after defining all the table columns, and it gets checked whenever a change is made to a row in the table.

Let's explore these constraints, their syntax, and their usefulness in table design.

 /Primary Keys: Natural vs. Surrogate/


In [[file:ch06.xhtml#ch06][Chapter 6]], you learned about giving a table a /primary key/: a column or collection of columns whose values uniquely identify each row in a table. A primary key is a constraint, and it imposes two rules on the column or columns that make up the key:

1. Each column in the key must have a unique value for each row.

2. No column in the key can have missing values.

Primary keys also provide a means of relating tables to each other and maintaining /referential integrity/, which is ensuring that rows in related tables have matching values when we expect them to. The simple primary key example in [[file:ch06.xhtml#lev82][“Relating Tables with Key Columns”]] on [[file:ch06.xhtml#page_74][page 74]] had a single ID field that used an integer inserted by us, the user. However, as with most areas of SQL, you can implement primary keys in several ways. Often, the data will suggest the best path. But first we must assess whether to use a /natural key/ or a /surrogate key/ as the primary key.

** Using Existing Columns for Natural Keys


You implement a natural key by using one or more of the table's existing columns rather than creating a column and filling it with artificial values to act as keys. If a column's values obey the primary key constraint---unique for every row and never empty---it can be used as a natural key. A value in the column can change as long as the new value doesn't cause a violation of the constraint.

An example of a natural key is a driver's license identification number issued by a local Department of Motor Vehicles. Within a governmental jurisdiction, such as a state in the United States, we'd reasonably expect that all drivers would receive a unique ID on their licenses. But if we were compiling a national driver's license database, we might not be able to make that assumption; several states could independently issue the same ID code. In that case, the driver_id column may not have unique values and cannot be used as the natural key unless it's combined with one or more additional columns. Regardless, as you build tables, you'll encounter many values suitable for natural keys: a part number, a serial number, or a book's ISBN are all good examples.

** Introducing Columns for Surrogate Keys


Instead of relying on existing data, a surrogate key typically consists of a single column that you fill with artificial values. This might be a sequential number auto-generated by the database; for example, using a serial data type (covered in [[file:ch03.xhtml#lev34][“Auto-Incrementing Integers”]] on [[file:ch03.xhtml#page_27][page 27]]). Some developers like to use a /Universally Unique Identifier (UUID)/, which is a code comprised of 32 hexadecimal digits that identifies computer hardware or software. Here's an example:

2911d8a8-6dea-4a46-af23-d64175a08237

** Pros and Cons of Key Types


As with most SQL debates, there are arguments for using either type of primary key. Reasons cited for using natural keys often include the following:

1) The data already exists in the table, and you don't need to add a column to create a key.

2) Because the natural key data has meaning, it can reduce the need to join tables when searching.

   Alternatively, advocates of surrogate keys highlight these points in favor:

3) Because a surrogate key doesn't have any meaning in itself and its values are independent of the data in the table, if your data changes later, you're not limited by the key structure.

4) Natural keys tend to consume more storage than the integers typically used for surrogate keys.

A well-designed table should have one or more columns that can serve as a natural key. An example is a product table with a unique product code. But in a table of employees, it might be difficult to find any single column, or even multiple columns, that would be unique on a row-by-row basis to serve as a primary key. In that case, you can create a surrogate key, but you probably should reconsider the table structure.

** Primary Key Syntax


In “JOIN Types” on [[file:ch06.xhtml#page_78][page 78]], you created primary keys on the schools_left and schools_right tables to try out JOIN types. In fact, these were surrogate keys: in both tables, you created columns called id to use as the key and used the keywords CONSTRAINT key_name PRIMARY KEY to declare them as primary keys. Let's work through several more primary key examples.

In [[file:ch07.xhtml#ch07list1][Listing 7-1]], we declare a primary key using the column constraint and table constraint methods on a table similar to the driver's license example mentioned earlier. Because we expect the driver's license IDs to always be unique, we'll use that column as a natural key.

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-1: Declaring a single-column natural key as primary key

-- As a column constraint
CREATE TABLE natural_key_example (
    license_id varchar(10) CONSTRAINT license_key PRIMARY KEY,
    first_name varchar(50),
    last_name varchar(50)
);

-- Drop the table before trying again
DROP TABLE natural_key_example;

-- As a table constraint
CREATE TABLE natural_key_example (
    license_id varchar(10),
    first_name varchar(50),
    last_name varchar(50),
    CONSTRAINT license_key PRIMARY KEY (license_id)
);
#+end_src

#+RESULTS:
| CREATE TABLE |
|--------------|
| DROP TABLE   |
| CREATE TABLE |

Listing 7-1: Declaring a single-column natural key as a primary key/

We first use the column constraint syntax to declare license_id as the primary key by adding the CONSTRAINT keyword ➊ followed by a name for the key and then the keywords PRIMARY KEY. An advantage of using this syntax is that it's easy to understand at a glance which column is designated as the primary key. Note that in the column constraint syntax you can omit the CONSTRAINT keyword and name for the key, and simply use PRIMARY KEY.

Next, we delete the table from the database by using the DROP TABLE command ➋ to prepare for the table constraint example.

To add the same primary key using the table constraint syntax, we declare the CONSTRAINT after listing the final column ➌ with the column we want to use as the key in parentheses. In this example, we end up with the same column for the primary key as we did with the column constraint syntax. However, you must use the table constraint syntax when you want to create a primary key using more than one column. In that case, you would list the columns in parentheses, separated by commas. We'll explore that in a moment.

First, let's look at how having a primary key protects you from ruining the integrity of your data. [[file:ch07.xhtml#ch07list2][Listing 7-2]] contains two INSERT statements:

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-2: Example of a primary key violation
INSERT INTO natural_key_example (license_id, first_name, last_name)
VALUES ('T229901', 'Lynn', 'Malero');

INSERT INTO natural_key_example (license_id, first_name, last_name)
VALUES ('T229901', 'Sam', 'Tracy');
#+end_src

#+RESULTS:
| INSERT 0 1 |
|------------|

When you execute the first INSERT statement on its own, the server loads a row into the natural_key_example table without any issue. When you attempt to execute the second, the server replies with an error:

ERROR: duplicate key value violates unique constraint "license_key"
DETAIL: Key (license_id)=(T229901) already exists.

Before adding the row, the server checked whether a license_id of T229901 was already present in the table. Because it was, and because a primary key by definition must be unique for each row, the server rejected the operation. The rules of the fictional DMV state that no two drivers can have the same license ID, so checking for and rejecting duplicate data is one way for the database to enforce that rule.

** Creating a Composite Primary Key


If we want to create a natural key but a single column in the table isn't sufficient for meeting the primary key requirements for uniqueness, we may be able to create a suitable key from a combination of columns, which is called a /composite primary key/.

As a hypothetical example, let's use a table that tracks student school attendance. The combination of a student ID column and a date column would give us unique data for each row, tracking whether or not the student was in school each day during a school year. To create a composite primary key from two or more columns, you must declare it using the table constraint syntax mentioned earlier. [[file:ch07.xhtml#ch07list3][Listing 7-3]] creates an example table for the student attendance scenario. The school database would record each student_id only once per school_day, creating a unique value for the row. A present column of data type boolean indicates whether the student was there on that day.

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-3: Declaring a composite primary key as a natural key
CREATE TABLE natural_key_composite_example (
    student_id varchar(10),
    school_day date,
    present boolean,
    CONSTRAINT student_key PRIMARY KEY (student_id, school_day)
);
#+end_src

#+RESULTS:
| CREATE TABLE |
|--------------|

/Listing 7-3: Declaring a composite primary key as a natural key/

The syntax in [[file:ch07.xhtml#ch07list3][Listing 7-3]] follows the same table constraint format for adding a primary key for one column, but we pass two (or more) columns as arguments rather than one. Again, we can simulate a key violation by attempting to insert a row where the combination of values in the two key columns---student_id and school_day---is not unique to the table. Run the code in [[file:ch07.xhtml#ch07list4][Listing 7-4]]:

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-4: Example of a composite primary key violation

INSERT INTO natural_key_composite_example (student_id, school_day, present)
VALUES(775, '1/22/2017', 'Y');

INSERT INTO natural_key_composite_example (student_id, school_day, present)
VALUES(775, '1/23/2017', 'Y');

INSERT INTO natural_key_composite_example (student_id, school_day, present)
VALUES(775, '1/23/2017', 'N');
#+end_src

#+RESULTS:
|   |

The first two INSERT statements execute fine because there's no duplication of values in the combination of key columns. But the third statement causes an error because the student_id and school_day values it contains match a combination that already exists in the table:

ERROR: duplicate key value violates unique constraint "student_key"
DETAIL: Key (student_id, school_day)=(775, 2017-01-23) already exists.

You can create composite keys with more than two columns. The specific database you're using imposes the limit to the number of columns you can use.

** Creating an Auto-Incrementing Surrogate Key


If a table you're creating has no columns suitable for a natural primary key, you may have a data integrity problem; in that case, it's best to reconsider how you're structuring the database. If you're inheriting data for analysis or feel strongly about using surrogate keys, you can create a column and fill it with unique values. Earlier, I mentioned that some developers use UUIDs for this; others rely on software to generate a unique code. For our purposes, an easy way to create a surrogate primary key is with an auto-incrementing integer using one of the serial data types discussed in [[file:ch03.xhtml#lev34][“Auto-Incrementing Integers”]] on [[file:ch03.xhtml#page_27][page 27]].

Recall the three serial types: smallserial, serial, and bigserial. They correspond to the integer types smallint, integer, and bigint in terms of the range of values they handle and the amount of disk storage they consume. For a primary key, it may be tempting to try to save disk space by using serial, which handles numbers as large as 2,147,483,647. But many a database developer has received a late-night call from a user frantic to know why their application is broken, only to discover that the database is trying to generate a number one greater than the data type's maximum. For this reason, with PostgreSQL, it's generally wise to use bigserial, which accepts numbers as high as 9.2 /quintillion/. You can set it and forget it, as shown in the first column defined in [[file:ch07.xhtml#ch07list5][Listing 7-5]]:
#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-5: Declaring a bigserial column as a surrogate key

CREATE TABLE surrogate_key_example (
    order_number bigserial,
    product_name varchar(50),
    order_date date,
    CONSTRAINT order_key PRIMARY KEY (order_number)
);

INSERT INTO surrogate_key_example (product_name, order_date)
VALUES ('Beachball Polish', '2015-03-17'),
       ('Wrinkle De-Atomizer', '2017-05-22'),
       ('Flux Capacitor', '1985-10-26');

SELECT * FROM surrogate_key_example;
#+end_src

#+RESULTS:
| CREATE TABLE |                     |            |
|--------------+---------------------+------------|
|   INSERT 0 3 |                     |            |
| order_number | product_name        | order_date |
|            1 | Beachball Polish    | 2015-03-17 |
|            2 | Wrinkle De-Atomizer | 2017-05-22 |
|            3 | Flux Capacitor      | 1985-10-26 |

/Listing 7-5: Declaring a bigserial column as a surrogate key/

[[file:ch07.xhtml#ch07list5][Listing 7-5]] shows how to declare the bigserial ➊ data type for an order_number column and set the column as the primary key ➋. When you insert data into the table ➌, you can omit the order_number column. With order_number set to bigserial, the database will create a new value for that column on each insert. The new value will be one greater than the largest already created for the column.

Run SELECT * FROM surrogate_key_example; to see how the column fills in automatically:

order_number    product_name           order_date
------------    -------------------    ----------
           1    Beachball Polish       2015-03-17
           2    Wrinkle De-Atomizer    2017-05-22
           3    Flux Capacitor         1985-10-26

The database will add one to order_number each time a new row is inserted. But it won't fill any gaps in the sequence created after rows are deleted.

*** Foreign Keys


With the /foreign key/ constraint, SQL very helpfully provides a way to ensure data in related tables doesn't end up unrelated, or orphaned. A foreign key is one or more columns in a table that match the primary key of another table. But a foreign key also imposes a constraint: values entered must already exist in the primary key or other unique key of the table it references. If not, the value is rejected. This constraint ensures that we don't end up with rows in one table that have no relation to rows in the other tables we can join them to.

To illustrate, [[file:ch07.xhtml#ch07list6][Listing 7-6]] shows two tables from a hypothetical database tracking motor vehicle activity:

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-6: A foreign key example

CREATE TABLE licenses (
    license_id varchar(10),
    first_name varchar(50),
    last_name varchar(50),
    CONSTRAINT licenses_key PRIMARY KEY (license_id)
);

CREATE TABLE registrations (
    registration_id varchar(10),
    registration_date date,
    license_id varchar(10) REFERENCES licenses (license_id),
    CONSTRAINT registration_key PRIMARY KEY (registration_id, license_id)
);

INSERT INTO licenses (license_id, first_name, last_name)
VALUES ('T229901', 'Lynn', 'Malero');

INSERT INTO registrations (registration_id, registration_date, license_id)
VALUES ('A203391', '3/17/2017', 'T229901');

INSERT INTO registrations (registration_id, registration_date, license_id)
VALUES ('A75772', '3/17/2017', 'T000001');
#+end_src

#+RESULTS:
| CREATE TABLE |
|--------------|
| CREATE TABLE |
| INSERT 0 1   |

The first table, licenses, is similar to the natural_key_example table we made earlier and uses a driver's unique license_id ➊ as a natural primary key. The second table, registrations, is for tracking vehicle registrations. A single license ID might be connected to multiple vehicle registrations, because each licensed driver can register multiple vehicles over a number of years. Also, a single vehicle could be registered to multiple license holders, establishing, as you learned in [[file:ch06.xhtml#ch06][Chapter 6]], a many-to-many relationship.

Here's how that relationship is expressed via SQL: in the registrations table, we designate the column license_id as a foreign key by adding the REFERENCES keyword, followed by the table name and column for it to reference ➋.

Now, when we insert a row into registrations, the database will test whether the value inserted into license_id already exists in the license_id primary key column of the licenses table. If it doesn't, the database returns an error, which is important. If any rows in registrations didn't correspond to a row in licenses, we'd have no way to write a query to find the person who registered the vehicle.

To see this constraint in action, create the two tables and execute the INSERT statements one at a time. The first adds a row to licenses ➌ that includes the value T229901 for the license_id. The second adds a row to registrations ➍ where the foreign key contains the same value. So far, so good, because the value exists in both tables. But we encounter an error with the third insert, which tries to add a row to registrations ➎ with a value for license_id that's not in licenses:

ERROR: insert or update on table "registrations" violates foreign key
constraint "registrations_license_id_fkey"
DETAIL: Key (license_id)=(T000001) is not present in table "licenses".

The resulting error is good because it shows the database is keeping the data clean. But it also indicates a few practical implications: first, it affects the order we insert data. We cannot add data to a table that contains a foreign key before the other table referenced by the key has the related records, or we'll get an error. In this example, we'd have to create a driver's license record before inserting a related registration record (if you think about it, that's what your local department of motor vehicles probably does).

Second, the reverse applies when we delete data. To maintain referential integrity, the foreign key constraint prevents us from deleting a row from licenses before removing any related rows in registrations, because doing so would leave an orphaned record. We would have to delete the related row in registrations first, and then delete the row in licenses. However, ANSI SQL provides a way to handle this order of operations automatically using the ON DELETE CASCADE keywords, which I'll discuss next.

*** Automatically Deleting Related Records with CASCADE/


To delete a row in licenses and have that action automatically delete any related rows in registrations, we can specify that behavior by adding ON DELETE CASCADE when defining the foreign key constraint.

When we create the registrations table, the keywords would go at the end of the definition of the license_id column, like this:

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
CREATE TABLE registrations (
    registration_id varchar(10),
    registration_date date,
    license_id varchar(10) REFERENCES licenses (license_id) ON DELETE CASCADE,
    CONSTRAINT registration_key PRIMARY KEY (registration_id, license_id)
);
#+end_src

Now, deleting a row in licenses should also delete all related rows in registrations. This allows us to delete a driver's license without first having to manually remove any registrations to it. It also maintains data integrity by ensuring deleting a license doesn't leave orphaned rows in registrations.

*** The CHECK Constraint


A CHECK constraint evaluates whether data added to a column meets the expected criteria, which we specify with a logical test. If the criteria aren't met, the database returns an error. The CHECK constraint is extremely valuable because it can prevent columns from getting loaded with nonsensical data. For example, a new employee's birthdate probably shouldn't be more than 120 years in the past, so you can set a cap on birthdates. Or, in most schools I know, Z isn't a valid letter grade for a course (although my barely passing algebra grade felt like it), so we might insert constraints that only accept the values A--F.

As with primary keys, we can implement a CHECK constraint as a column constraint or a table constraint. For a column constraint, declare it in the CREATE TABLE statement after the column name and data type: CHECK (logical expression). As a table constraint, use the syntax CONSTRAINT constraint_name CHECK (logical expression) after all columns are defined.

[[file:ch07.xhtml#ch07list7][Listing 7-7]] shows a CHECK constraint applied to two columns in a table we might use to track the user role and salary of employees within an organization. It uses the table constraint syntax for the primary key and the CHECK constraint.

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis
-- Listing 7-7: CHECK constraint examples

CREATE TABLE check_constraint_example (
    user_id bigserial,
    user_role varchar(50),
    salary integer,
    CONSTRAINT user_id_key PRIMARY KEY (user_id),
    CONSTRAINT check_role_in_list CHECK (user_role IN('Admin', 'Staff')),
    CONSTRAINT check_salary_not_zero CHECK (salary > 0)
);

-- Both of these will fail:
INSERT INTO check_constraint_example (user_role)
VALUES ('admin');

INSERT INTO check_constraint_example (salary)
VALUES (0);
#+end_src

#+RESULTS:
| CREATE TABLE |
|--------------|


We create the table and set the user_id column as an auto-incrementing surrogate primary key. The first CHECK ➊ tests whether values entered into the user_role column match one of two predefined strings, Admin or Staff, by using the SQL IN operator. The second CHECK tests whether values entered in the salary column are greater than 0, because no one should be earning a negative amount ➋. Both tests are another example of a /Boolean expression/, a statement that evaluates as either true or false. If a value tested by the constraint evaluates as true, the check passes.

*NOTE*

/Developers may debate whether check logic belongs in the database, in the application in front of the database, such as a human resources system, or both. One advantage of checks in the database is that the database will maintain data integrity in the case of changes to the application, even if a new system gets built or users are given alternate ways to add data./

When values are inserted or updated, the database checks them against the constraint. If the values in either column violate the constraint---or, for that matter, if the primary key constraint is violated---the database will reject the change.

If we use the table constraint syntax, we also can combine more than one test in a single CHECK statement. Say we have a table related to student achievement. We could add the following:

CONSTRAINT grad_check CHECK (credits >= 120 AND tuition = 'Paid')

Notice that we combine two logical tests by enclosing them in parentheses and connecting them with AND. Here, both Boolean expressions must evaluate as true for the entire check to pass. You can also test values across columns, as in the following example where we want to make sure an item's sale price is a discount on the original, assuming we have columns for both values:

CONSTRAINT sale_check CHECK (sale_price < retail_price)

Inside the parentheses, the logical expression checks that the sale price is less than the retail price.

*** The UNIQUE Constraint


We can also ensure that a column has a unique value in each row by using the UNIQUE constraint. If ensuring unique values sounds similar to the purpose of a primary key, it is. But UNIQUE has one important difference. In a primary key, no values can be NULL, but a UNIQUE constraint permits multiple NULL values in a column.

To show the usefulness of UNIQUE, look at the code in [[file:ch07.xhtml#ch07list8][Listing 7-8]], which is a table for tracking contact info:

#+begin_src sql :engine postgresql :dbuser org  :dbpassword 1618 :database analysis

CREATE TABLE unique_constraint_example (
    contact_id bigserial CONSTRAINT contact_id_key PRIMARY KEY,
    first_name varchar(50),
    last_name varchar(50),
    email varchar(200),
    CONSTRAINT email_unique UNIQUE (email)
);

INSERT INTO unique_constraint_example (first_name, last_name, email)
VALUES ('Samantha', 'Lee', 'slee@example.org');

INSERT INTO unique_constraint_example (first_name, last_name, email)
VALUES ('Betty', 'Diaz', 'bdiaz@example.org');

INSERT INTO unique_constraint_example (first_name, last_name, email)
VALUES ('Sasha', 'Lee', 'slee@example.org');
#+end_src


In this table, contact_id serves as a surrogate primary key, uniquely identifying each row. But we also have an email column, the main point of contact with each person. We'd expect this column to contain only unique email addresses, but those addresses might change over time. So, we use UNIQUE ➊ to ensure that any time we add or update a contact's email we're not providing one that already exists. If we do try to insert an email that already exists ➋, the database will return an error:

ERROR: duplicate key value violates unique constraint "email_unique"
DETAIL: Key (email)=(slee@example.org) already exists.

Again, the error shows the database is working for us.

 /The NOT NULL Constraint/


In [[file:ch06.xhtml#ch06][Chapter 6]], you learned about NULL, a special value in SQL that represents a condition where no data is present in a row in a column or the value is unknown. You've also learned that NULL values are not allowed in a primary key, because primary keys need to uniquely identify each row in a table. But there will be other columns besides primary keys where you don't want to allow empty values. For example, in a table listing each student in a school, it would be necessary for columns containing first and last names to be filled for each row. To require a value in a column, SQL provides the NOT NULL constraint, which simply prevents a column from accepting empty values.

[[file:ch07.xhtml#ch07list9][Listing 7-9]] demonstrates the NOT NULL syntax:

CREATE TABLE not_null_example (
    student_id bigserial,
    first_name varchar(50) NOT NULL,
    last_name varchar(50) NOT NULL,
    CONSTRAINT student_id_key PRIMARY KEY (student_id)
);

/Listing 7-9: A NOT NULL constraint example/

Here, we declare NOT NULL for the first_name and last_name columns because it's likely we'd require those pieces of information in a table tracking student information. If we attempt an INSERT on the table and don't include values for those columns, the database will notify us of the violation.

*** Removing Constraints or Adding Them Later/


So far, we've been placing constraints on tables at the time of creation. You can also remove a constraint or later add one to an existing table using ALTER TABLE, the SQL command that makes changes to tables and columns. We'll work with ALTER TABLE more in [[file:ch09.xhtml#ch09][Chapter 9]], but for now we'll review the syntax for adding and removing constraints.

To remove a primary key, foreign key, or a UNIQUE constraint, you would write an ALTER TABLE statement in this format:

ALTER TABLE /table_name/ DROP CONSTRAINT /constraint_name/;

To drop a NOT NULL constraint, the statement operates on the column, so you must use the additional ALTER COLUMN keywords, like so:

ALTER TABLE /table_name/ ALTER COLUMN /column_name/ DROP NOT NULL;

Let's use these statements to modify the not_null_example table you just made, as shown in [[file:ch07.xhtml#ch07list10][Listing 7-10]]:

ALTER TABLE not_null_example DROP CONSTRAINT student_id_key;
ALTER TABLE not_null_example ADD CONSTRAINT student_id_key PRIMARY KEY (student_id);
ALTER TABLE not_null_example ALTER COLUMN first_name DROP NOT NULL;
ALTER TABLE not_null_example ALTER COLUMN first_name SET NOT NULL;

/Listing 7-10: Dropping and adding a primary key and a NOT NULL constraint/

Execute the statements one at a time to make changes to the table. Each time, you can view the changes to the table definition in pgAdmin by clicking the table name once, and then clicking the *SQL* tab above the query window. With the first ALTER TABLE statement, we use DROP CONSTRAINT to remove the primary key named student_id_key. We then add the primary key back using ADD CONSTRAINT. We'd use that same syntax to add a constraint to any existing table.

*NOTE*

/You can only add a constraint to an existing table if the data in the target column obeys the limits of the constraint. For example, you can't place a primary key constraint on a column that has duplicate or empty values./

In the third statement, ALTER COLUMN and DROP NOT NULL remove the NOT NULL constraint from the first_name column. Finally, SET NOT NULL adds the constraint.

** Speeding Up Queries with Indexes


In the same way that a book's index helps you find information more quickly, you can speed up queries by adding an /index/ to one or more columns. The database uses the index as a shortcut rather than scanning each row to find data. That's admittedly a simplistic picture of what, in SQL databases, is a nontrivial topic. I could write several chapters on SQL indexes and tuning databases for performance, but instead I'll offer general guidance on using indexes and a PostgreSQL-specific example that demonstrates their benefits.

 /B-Tree: PostgreSQL's Default Index/


While following along in this book, you've already created several indexes, perhaps without knowing. Each time you add a primary key or UNIQUE constraint to a table, PostgreSQL (as well as most database systems) places an index on the column. Indexes are stored separately from the table data, but they're accessed automatically when you run a query and are updated every time a row is added or removed from the table.

In PostgreSQL, the default index type is the /B-Tree index/. It's created automatically on the columns designated for the primary key or a UNIQUE constraint, and it's also the type created by default when you execute a CREATE INDEX statement. B-Tree, short for /balanced tree/, is so named because the structure organizes the data in a way that when you search for a value, it looks from the top of the tree down through branches until it locates the data you want. (Of course, the process is a lot more complicated than that. A good start on understanding more about the B-Tree is the B-Tree Wikipedia entry.) A B-Tree index is useful for data that can be ordered and searched using equality and range operators, such as <, <=, =, >=, >, and BETWEEN.

PostgreSQL incorporates additional index types, including the /Generalized Inverted Index (GIN)/ and the /Generalized Search Tree (GiST)/. Each has distinct uses, and I'll incorporate them in later chapters on full text search and queries using geometry types.

For now, let's see a B-Tree index speed a simple search query. For this exercise, we'll use a large data set comprising more than 900,000 New York City street addresses, compiled by the OpenAddresses project at /[[https://openaddresses.io/]]/. The file with the data, /city_of_new_york.csv/, is available for you to download along with all the resources for this book from /[[https://www.nostarch.com/practicalSQL/]]/.

After you've downloaded the file, use the code in [[file:ch07.xhtml#ch07list11][Listing 7-11]] to create a new_york_addresses table and import the address data. You're a pro at this by now, although the import will take longer than the tiny data sets you've loaded so far. The final, loaded table is 126MB, and on one of my systems, it took nearly a minute for the COPY command to complete.

CREATE TABLE new_york_addresses (
    longitude numeric(9,6),
    latitude numeric(9,6),
    street_number varchar(10),
    street varchar(32),
    unit varchar(7),
    postcode varchar(5),
    id integer CONSTRAINT new_york_key PRIMARY KEY
);

COPY new_york_addresses
FROM '/C:YourDirectory/city_of_new_york.csv'
WITH (FORMAT CSV, HEADER);

/Listing 7-11: Importing New York City address data/

When the data loads, run a quick SELECT query to visually check that you have 940,374 rows and seven columns. A common use for this data might be to search for matches in the street column, so we'll use that example for exploring index performance.

** Benchmarking Query Performance with EXPLAIN

We'll measure how well an index can improve query speed by checking the performance before and after adding one. To do this, we'll use PostgreSQL's EXPLAIN command, which is specific to PostgreSQL and not part of standard SQL. The EXPLAIN command provides output that lists the /query plan/ for a specific database query. This might include how the database plans to scan the table, whether or not it will use indexes, and so on. If we add the ANALYZE keyword, EXPLAIN will carry out the query and show the actual execution time, which is what we want for the current exercise.

** Recording Some Control Execution Times


Run each of the three queries in [[file:ch07.xhtml#ch07list12][Listing 7-12]] one at a time. We're using typical SELECT queries with a WHERE clause but with the keywords EXPLAIN ANALYZE included at the beginning. Instead of showing the query results, these keywords tell the database to execute the query and display statistics about the query process and how long it took to execute.

EXPLAIN ANALYZE SELECT * FROM new_york_addresses
WHERE street = 'BROADWAY';

EXPLAIN ANALYZE SELECT * FROM new_york_addresses
WHERE street = '52 STREET';

EXPLAIN ANALYZE SELECT * FROM new_york_addresses
WHERE street = 'ZWICKY AVENUE';

/Listing 7-12: Benchmark queries for index performance/

On my system, the first query returns these stats:

➊ Seq Scan on new_york_addresses  (cost=0.00..20730.68 rows=3730 width=46)
  (actual time=0.055..289.426 rows=3336 loops=1)
     Filter: ((street)::text = 'BROADWAY'::text)
     Rows Removed by Filter: 937038
   Planning time: 0.617 ms
➋ Execution time: 289.838 ms

Not all the output is relevant here, so I won't decode it all, but two lines are pertinent. The first indicates that to find any rows where street = 'BROADWAY', the database will conduct a sequential scan ➊ of the table. That's a synonym for a full table scan: each row will be examined, and the database will remove any row that doesn't match BROADWAY. The execution time (on my computer about 290 milliseconds) ➋ is how long this will take. Your time will depend on factors including your computer hardware.

Run each query in [[file:ch07.xhtml#ch07list12][Listing 7-12]] and record the execution time for each.

** Adding the Index


Now, let's see how adding an index changes the query's search method and how fast it works. [[file:ch07.xhtml#ch07list13][Listing 7-13]] shows the SQL statement for creating the index with PostgreSQL:

CREATE INDEX street_idx ON new_york_addresses (street);

/Listing 7-13: Creating a B-Tree index on the new_york_addresses table/

Notice that it's similar to the commands for creating constraints we've covered in the chapter already. (Other database systems have their own variants and options for creating indexes, and there is no ANSI standard.) We give the CREATE INDEX keywords followed by a name we choose for the index, in this case street_idx. Then ON is added, followed by the target table and column.

Execute the CREATE INDEX statement, and PostgreSQL will scan the values in the street column and build the index from them. We only need to create the index once. When the task finishes, rerun each of the three queries in [[file:ch07.xhtml#ch07list12][Listing 7-12]] and record the execution times reported by EXPLAIN ANALYZE. For example:

  Bitmap Heap Scan on new_york_addresses  (cost=65.80..5962.17 rows=2758
  width=46) (actual time=1.792..9.816 rows=3336 loops=1)
    Recheck Cond: ((street)::text = 'BROADWAY'::text)
    Heap Blocks: exact=2157
➊   ->  Bitmap Index Scan on street_idx  (cost=0.00..65.11 rows=2758 width=0)
         (actual time=1.253..1.253 rows=3336 loops=1)
           Index Cond: ((street)::text = 'BROADWAY'::text)
  Planning time: 0.163 ms
➋ Execution time: 5.887 ms

Do you notice a change? First, instead of a sequential scan, the EXPLAIN ANALYZE statistics for each query show that the database is now using an index scan on street_idx ➊ instead of visiting each row. Also, the query speed is now markedly faster ➋. [[file:ch07.xhtml#ch07tab1][Table 7-1]] shows the execution times (rounded) from my computer before and after adding the index.

*Table 7-1:* Measuring Index Performance

*Query Filter*

*Before Index*

*After Index*

WHERE street = 'BROADWAY'

290 ms

6 ms

WHERE street = '52 STREET'

271 ms

6 ms

WHERE street = 'ZWICKY AVENUE'

306 ms

1 ms

The execution times are much, much better, effectively a quarter second faster or more per query. Is a quarter second that impressive? Well, whether you're seeking answers in data using repeated querying or creating a database system for thousands of users, the time savings adds up.

If you ever need to remove an index from a table---perhaps if you're testing the performance of several index types---use the DROP INDEX command followed by the name of the index to remove.

 /Considerations When Using Indexes/


You've seen that indexes have significant performance benefits, so does that mean you should add an index to every column in a table? Not so fast! Indexes are valuable, but they're not always needed. In addition, they do enlarge the database and impose a maintenance cost on writing data. Here are a few tips for judging when to uses indexes:

- Consult the documentation for the database manager you're using to learn about the kinds of indexes available and which to use on particular data types. PostgreSQL, for example, has five more index types in addition to B-Tree. One, called GiST, is particularly suited to the geometry data types I'll discuss later in the book. Full text search, which you'll learn in [[file:ch13.xhtml#ch13][Chapter 13]], also benefits from indexing.
- Consider adding indexes to any columns you'll use in table joins. Primary keys are indexed by default in PostgreSQL, but foreign key columns in related tables are not and are a good target for indexes.
- Add indexes to columns that will frequently end up in a query WHERE clause. As you've seen, search performance is significantly improved via indexes.
- Use EXPLAIN ANALYZE to test performance under a variety of configurations if you're unsure. Optimization is a process!

** Wrapping Up


With the tools you've added to your toolbox in this chapter, you're ready to ensure that the databases you build or inherit are best suited for your collection and exploration of data. Your queries will run faster, you can exclude unwanted values, and your database objects will have consistent organization. That's a boon for you and for others who share your data.

This chapter concludes the first part of the book, which focused on giving you the essentials to dig into SQL databases. I'll continue building on these foundations as we explore more complex queries and strategies for data analysis. In the next chapter, we'll use SQL aggregate functions to assess the quality of a data set and get usable information from it.


*TRY IT YOURSELF*

Are you ready to test yourself on the concepts covered in this chapter? Consider the following two tables from a database you're making to keep track of your vinyl LP collection. Start by reviewing these CREATE TABLE statements:

CREATE TABLE albums (
    album_id bigserial,
    album_catalog_code varchar(100),
    album_title text,
    album_artist text,
    album_release_date date,
    album_genre varchar(40),
    album_description text
);

CREATE TABLE songs (
    song_id bigserial,
    song_title text,
    song_artist text,
    album_id bigint
);

The albums table includes information specific to the overall collection of songs on the disc. The songs table catalogs each track on the album. Each song has a title and its own artist column, because each song might feature its own collection of artists.

Use the tables to answer these questions:

1. Modify these CREATE TABLE statements to include primary and foreign keys plus additional constraints on both tables. Explain why you made your choices.

2. Instead of using album_id as a surrogate key for your primary key, are there any columns in albums that could be useful as a natural key? What would you have to know to decide?

3. To speed up queries, which columns are good candidates for indexes?


 are indexed by default in PostgreSQL, but foreign key columns in related tables are not and are a good target for indexes.
- Add indexes to columns that will frequently end up in a query WHERE clause. As you've seen, search performance is significantly improved via indexes.
- Use EXPLAIN ANALYZE to test performance under a variety of configurations if you're unsure. Optimization is a process!

**** Wrapping Up
    :PROPERTIES:
    :CUSTOM_ID: lev119
    :CLASS: h3
    :END:

With the tools you've added to your toolbox in this chapter, you're ready to ensure that the databases you build or inherit are best suited for your collection and exploration of data. Your queries will run faster, you can exclude unwanted values, and your database objects will have consistent organization. That's a boon for you and for others who share your data.

This chapter concludes the first part of the book, which focused on giving you the essentials to dig into SQL databases. I'll continue building on these foundations as we explore more complex queries and strategies for data analysis. In the next chapter, we'll use SQL aggregate functions to assess the quality of a data set and get usable information from it.

<<ch07sb1>>
*TRY IT YOURSELF*

Are you ready to test yourself on the concepts covered in this chapter? Consider the following two tables from a database you're making to keep track of your vinyl LP collection. Start by reviewing these CREATE TABLE statements:

CREATE TABLE albums (\\
    album\_id bigserial,\\
    album\_catalog\_code varchar(100),\\
    album\_title text,\\
    album\_artist text,\\
    album\_release\_date date,\\
    album\_genre varchar(40),\\
    album\_description text\\
);\\
\\
CREATE TABLE songs (\\
    song\_id bigserial,\\
    song\_title text,\\
    song\_artist text,\\
    album\_id bigint\\
);

The albums table includes information specific to the overall collection of songs on the disc. The songs table catalogs each track on the album. Each song has a title and its own artist column, because each song might feature its own collection of artists.

Use the tables to answer these questions:

1. Modify these CREATE TABLE statements to include primary and foreign keys plus additional constraints on both tables. Explain why you made your choices.

2. Instead of using album\_id as a surrogate key for your primary key, are there any columns in albums that could be useful as a natural key? What would you have to know to decide?

3. To speed up queries, which columns are good candidates for indexes?


