* 12 Advanced Query Techniques

Sometimes data analysis requires advanced SQL techniques that go beyond a table join or basic SELECT query. For example, to find the story in your data, you might need to write a query that uses the results of other queries as inputs. Or you might need to reclassify numerical values into categories before counting them. Like other programming languages, SQL provides a collection of functions and options essential for solving more complex problems, and that is what we'll explore in this chapter.

For the exercises, I'll introduce a data set of temperatures recorded in select U.S. cities and we'll revisit data sets you've created in previous chapters. The code for the exercises is available, along with all the book's resources, at /[[https://www.nostarch.com/practicalSQL/]]/. You'll continue to use the analysis database you've already built. Let's get started.

** Using Subqueries


A /subquery/ is nested inside another query. Typically, it's used for a calculation or logical test that provides a value or set of data to be passed into the main portion of the query. Its syntax is not unusual: we just enclose the subquery in parentheses and use it where needed. For example, we can write a subquery that returns multiple rows and treat the results as a table in the FROM clause of the main query. Or we can create a /scalar subquery/ that returns a single value and use it as part of an /expression/ to filter rows via WHERE, IN, and HAVING clauses. These are the most common uses of subqueries.

You first encountered a subquery in [[file:ch09.xhtml#ch09][Chapter 9]] in the ANSI SQL standard syntax for a table UPDATE, which is shown again here. Both the data for the update and the condition that specifies which rows to update are generated by subqueries that look for values that match the columns in table and table_b:

  UPDATE /table/
➊ SET /column/ = (SELECT /column/
                FROM /table_b/
                WHERE /table.column/ = /table_b.column/)
➋ WHERE EXISTS (SELECT /column/
                FROM /table_b/
                WHERE /table.column/ = /table_b.column/);

This example query has two subqueries that use the same syntax. We use the SELECT statement inside parentheses ➊ as the first subquery in the SET clause, which generates values for the update. Similarly, we use a second subquery in the WHERE EXISTS clause, again with a SELECT statement ➋ to filter the rows we want to update. Both subqueries are /correlated subqueries/ and are so named because they depend on a value or table name from the main query that surrounds them. In this case, both subqueries depend on /table/ from the main UPDATE statement. An /uncorrelated subquery/ has no reference to objects in the main query.

It's easier to understand these concepts by working with actual data, so let's look at some examples. We'll revisit two data sets from earlier chapters: the Decennial 2010 Census table us_counties_2010 you created in [[file:ch04.xhtml#ch04][Chapter 4]] and the meat_poultry_egg_inspect table in [[file:ch09.xhtml#ch09][Chapter 9]].

 /Filtering with Subqueries in a WHERE Clause/


You know that a WHERE clause lets you filter query results based on criteria you provide, using an expression such as WHERE quantity > 1000. But this requires that you already know the value to use for comparison. What if you don't? That's one way a subquery comes in handy: it lets you write a query that generates one or more values to use as part of an expression in a WHERE clause.

** Generating Values for a Query Expression


Say you wanted to write a query to show which U.S. counties are at or above the 90th percentile, or top 10 percent, for population. Rather than writing two separate queries---one to calculate the 90th percentile and the other to filter by counties---you can do both at once using a subquery in a WHERE clause, as shown in [[file:ch12.xhtml#ch12list1][Listing 12-1]]:

  SELECT geo_name,
         state_us_abbreviation,
         p0010001
  FROM us_counties_2010
➊ WHERE p0010001 >= (
      SELECT percentile_cont(.9) WITHIN GROUP (ORDER BY p0010001)
      FROM us_counties_2010
      )
  ORDER BY p0010001 DESC;

/Listing 12-1: Using a subquery in a WHERE clause/

This query is standard in terms of what we've done so far except that the WHERE clause ➊, which filters by the total population column p0010001, doesn't include a value like it normally would. Instead, after the >= comparison operators, we provide a second query in parentheses. This second query uses the percentile_cont() function in [[file:ch05.xhtml#ch05][Chapter 5]] to generate one value: the 90th percentile cut-off point in the p0010001 column, which will then be used in the main query.

*NOTE*

/Using percentile_cont() to filter with a subquery works only if you pass in a single input, as shown. If you pass in an array, as in [[file:ch05.xhtml#ch05list12][Listing 5-12]] on [[file:ch05.xhtml#page_68][page 68]], percentile_cont() returns an array, and the query will fail to evaluate the >= against an array type./

If you run the subquery separately by highlighting it in pgAdmin, you should see the results of the subquery, a value of 197444.6. But you won't see that number when you run the entire query in [[file:ch12.xhtml#ch12list1][Listing 12-1]], because the result of that subquery is passed directly to the WHERE clause to use in filtering the results.

The entire query should return 315 rows, or about 10 percent of the 3,143 rows in us_counties_2010.

geo_name              state_us_abbreviation    p0010001
------------------    ---------------------    --------
Los Angeles County    CA                        9818605
Cook County           IL                        5194675
Harris County         TX                        4092459
Maricopa County       AZ                        3817117
San Diego County      CA                        3095313
/--snip--/
Elkhart County        IN                         197559
Sangamon County       IL                         197465

The result includes all counties with a population greater than or equal to 197444.6, the value the subquery generated.

** Using a Subquery to Identify Rows to Delete


Adding a subquery to a WHERE clause can be useful in query statements other than SELECT. For example, we can use a similar subquery in a DELETE statement to specify what to remove from a table. Imagine you have a table with 100 million rows that, because of its size, takes a long time to query. If you just want to work on a subset of the data (such as a particular state), you can make a copy of the table and delete what you don't need from it.

[[file:ch12.xhtml#ch12list2][Listing 12-2]] shows an example of this approach. It makes a copy of the census table using the method you learned in [[file:ch09.xhtml#ch09][Chapter 9]] and then deletes everything from that backup except the 315 counties in the top 10 percent of population:

CREATE TABLE us_counties_2010_top10 AS
SELECT * FROM us_counties_2010;

DELETE FROM us_counties_2010_top10
WHERE p0010001 < (
    SELECT percentile_cont(.9) WITHIN GROUP (ORDER BY p0010001)
    FROM us_counties_2010_top10
    );

/Listing 12-2: Using a subquery in a WHERE clause with DELETE/

Run the code in [[file:ch12.xhtml#ch12list2][Listing 12-2]], and then execute SELECT count(*) FROM us_counties_2010_top10; to count the remaining rows in the table. The result should be 315 rows, which is the original 3,143 minus the 2,828 the subquery deleted.

 /Creating Derived Tables with Subqueries/


If your subquery returns rows and columns of data, you can convert that data to a table by placing it in a FROM clause, the result of which is known as a /derived table/. A derived table behaves just like any other table, so you can query it or join it to other tables, even other derived tables. This approach is helpful when a single query can't perform all the operations you need.

Let's look at a simple example. In [[file:ch05.xhtml#ch05][Chapter 5]], you learned the difference between average and median values. I explained that a median can often better indicate a data set's central value because a few very large or small values (or outliers) can skew an average. For that reason, I often recommend comparing the average and median. If they're close, the data probably falls in a /normal distribution/ (the familiar bell curve), and the average is a good representation of the central value. If the average and median are far apart, some outliers might be having an effect or the distribution is skewed, not normal.

Finding the average and median population of U.S. counties as well as the difference between them is a two-step process. We need to calculate the average and the median, and then we need to subtract the two. We can do both operations in one fell swoop with a subquery in the FROM clause, as shown in [[file:ch12.xhtml#ch12list3][Listing 12-3]].

  SELECT round(calcs.average, 0) AS average,
         calcs.median,
         round(calcs.average - calcs.median, 0) AS median_average_diff
  FROM (
     ➊ SELECT avg(p0010001) AS average,
              percentile_cont(.5)
                  WITHIN GROUP (ORDER BY p0010001)::numeric(10,1) AS median
       FROM us_counties_2010
       )
➋ AS calcs;

/Listing 12-3: Subquery as a derived table in a FROM clause/

The subquery ➊ is straightforward. We use the avg() and percentile_cont() functions to find the average and median of the census table's p0010001 total population column and name each column with an alias. Then we name the subquery with an alias ➋ of calcs so we can reference it as a table in the main query.

Subtracting the median from the average, both of which are returned by the subquery, is done in the main query; then the main query rounds the result and labels it with the alias median_average_diff. Run the query, and the result should be the following:

average    median     median_average_diff
-------    -------    -------------------
  98233    25857.0                  72376

The difference between the median and average, 72,736, is nearly three times the size of the median. That helps show that a relatively small number of high-population counties push the average county size over 98,000, whereas the median of all counties is much less at 25,857.

 /Joining Derived Tables/


Because derived tables behave like regular tables, you can join them. Joining derived tables lets you perform multiple preprocessing steps before arriving at the result. For example, say we wanted to determine which states have the most meat, egg, and poultry processing plants per million population; before we can calculate that rate, we need to know the number of plants in each state and the population of each state.

We start by counting producers by state using the meat_poultry_egg_inspect table in [[file:ch09.xhtml#ch09][Chapter 9]]. Then we can use the us_counties_2010 table to count population by state by summing and grouping county values. [[file:ch12.xhtml#ch12list4][Listing 12-4]] shows how to write subqueries for both tasks and join them to calculate the overall rate.

  SELECT census.state_us_abbreviation AS st,
         census.st_population,
         plants.plant_count,
       ➊ round((plants.plant_count/census.st_population::numeric(10,1))*1000000, 1)
             AS plants_per_million
  FROM
      (
        ➋ SELECT st,
                 count(*) AS plant_count
          FROM meat_poultry_egg_inspect
          GROUP BY st
      )
      AS plants
  JOIN
      (
        ➌ SELECT state_us_abbreviation,
                 sum(p0010001) AS st_population
          FROM us_counties_2010
          GROUP BY state_us_abbreviation
      )
      AS census
➍ ON plants.st = census.state_us_abbreviation
  ORDER BY plants_per_million DESC;

/Listing 12-4: Joining two derived tables/

You learned how to calculate rates in [[file:ch10.xhtml#ch10][Chapter 10]], so the math and syntax in the main query for finding plants_per_million ➊ should be familiar. We divide the number of plants by the population, and then multiply that quotient by 1 million. For the inputs, we use the values generated from derived tables using subqueries.

The first subquery ➋ finds the number of plants in each state using the count() aggregate function and then groups them by state. We label this subquery with the plants alias for reference in the main part of the query. The second subquery ➌ finds the total population by state by using sum() on the p0010001 total population column and then groups those by state_us_abbreviation. We alias this derived table as census.

Next, we join the derived tables ➍ by linking the st column in plants to the state_us_abbreviation column in census. We then list the results in descending order based on the calculated rates. Here's a sample output of 51 rows showing the highest and lowest rates:

st    st_population    plant_count    plants_per_million
--    -------------    -----------    ------------------
NE          1826341            110                  60.2
IA          3046355            149                  48.9
VT           625741             27                  43.1
HI          1360301             47                  34.6
ND           672591             22                  32.7
/--snip--/
SC          4625364             55                  11.9
LA          4533372             49                  10.8
AZ          6392017             37                   5.8
DC           601723              2                   3.3
WY           563626              1                   1.8

The results line up with what we might expect. The top states are well-known meat producers. For example, Nebraska is one of the nation's top cattle exporters, and Iowa leads the United States in pork production. Washington, D.C., and Wyoming at the bottom of the list are among those states with the fewest plants per million.

*NOTE*

/Your results will differ slightly if you didn't add missing state values to the meat_poultry_egg_inspect table as noted in [[file:ch09.xhtml#lev146][“Updating Rows Where Values Are Missing”]] on [[file:ch09.xhtml#page_141][page 141]]./

 /Generating Columns with Subqueries/


You can also generate new columns of data with subqueries by placing a subquery in the column list after SELECT. Typically, you would use a single value from an aggregate. For example, the query in [[file:ch12.xhtml#ch12list5][Listing 12-5]] selects the geo_name and total population column p0010001 from us_counties_2010, and then adds a subquery to add the median of all counties to each row in the new column us_median:

SELECT geo_name,
       state_us_abbreviation AS st,
       p0010001 AS total_pop,
       (SELECT percentile_cont(.5) WITHIN GROUP (ORDER BY p0010001)
        FROM us_counties_2010) AS us_median
FROM us_counties_2010;

/Listing 12-5: Adding a subquery to a column list/

The first rows of the result set should look like this:

geo_name          st    total_pop    us_median
--------------    --    ---------    ---------
Autauga County    AL        54571        25857
Baldwin County    AL       182265        25857
Barbour County    AL        27457        25857
Bibb County       AL        22915        25857
Blount County     AL        57322        25857
/--snip--/

On its own, that repeating us_median value isn't very helpful because it's the same each time. It would be more interesting and useful to generate values that indicate how much each county's population deviates from the median value. Let's look at how we can use the same subquery technique to do that. [[file:ch12.xhtml#ch12list6][Listing 12-6]] builds on [[file:ch12.xhtml#ch12list5][Listing 12-5]] by adding a subquery expression after SELECT that calculates the difference between the population and the median for each county:

  SELECT geo_name,
         state_us_abbreviation AS st,
         p0010001 AS total_pop,
         (SELECT percentile_cont(.5) WITHIN GROUP (ORDER BY p0010001)
         FROM us_counties_2010) AS us_median,
     ➊ p0010001 - (SELECT percentile_cont(.5) WITHIN GROUP (ORDER BY p0010001)
                   FROM us_counties_2010) AS diff_from_median
  FROM us_counties_2010
➋ WHERE (p0010001 - (SELECT percentile_cont(.5) WITHIN GROUP (ORDER BY p0010001)
                     FROM us_counties_2010))
         BETWEEN -1000 AND 1000;

/Listing 12-6: Using a subquery expression in a calculation/

The added subquery ➊ is part of a column definition that subtracts the subquery's result from p0010001, the total population. It puts that new data in a column with an alias of diff_from_median. To make this query even more useful, we can narrow the results further to show only counties whose population falls within 1,000 of the median. This would help us identify which counties in America have close to the median county population. To do this, we repeat the subquery expression in the WHERE clause ➋ and filter results using the BETWEEN -1000 AND 1000 expression.

The outcome should reveal 71 counties with a population relatively close to the U.S. median. Here are the first five rows of the results:

[[../images/prog_page_198.jpg]]

Bear in mind that subqueries add to overall query execution time; therefore, if we were working with millions of rows, we could simplify [[file:ch12.xhtml#ch12list6][Listing 12-6]] by eliminating the subquery that displays the column us_median. I've left it in this example for your reference.

 /Subquery Expressions/


You can also use subqueries to filter rows by evaluating whether a condition evaluates as true or false. For this, we can use several standard ANSI SQL /subquery expressions/, which are a combination of a keyword with a subquery and are generally used in WHERE clauses to filter rows based on the existence of values in another table.

The PostgreSQL documentation at /[[https://www.postgresql.org/docs/current/static/functions-subquery.html]]/ lists available subquery expressions, but here we'll examine the syntax for just two of them.

** Generating Values for the IN Operator


The subquery expression IN (subquery) is like the IN comparison operator in [[file:ch02.xhtml#ch02][Chapter 2]] except we use a subquery to provide the list of values to check against rather than having to manually provide one. In the following example, we use a subquery to generate id values from a retirees table, and then use that list for the IN operator in the WHERE clause. The NOT IN expression does the opposite to find employees whose id value does /not/ appear in retirees.

SELECT first_name, last_name
FROM employees
WHERE id IN (
    SELECT id
    FROM retirees);

We would expect the output to show the names of employees who have id values that match those in retirees.

*NOTE*

/The presence of NULL values in a subquery result set will cause a query with a NOT IN expression to return no rows. If your data contains NULL values, use the WHERE NOT EXISTS expression described in the next section./

** Checking Whether Values Exist


Another subquery expression, EXISTS (/subquery/), is a true/false test. It returns a value of true if the subquery in parentheses returns at least one row. If it returns no rows, EXISTS evaluates to false. In the following example, the query returns all names from an employees table as long as the subquery finds at least one value in id in a retirees table.

SELECT first_name, last_name
FROM employees
WHERE EXISTS (
    SELECT id
    FROM retirees);

Rather than return all names from employees, we instead could mimic the behavior of IN and limit names to where the subquery after EXISTS finds at least one corresponding id value in retirees. The following is a correlated subquery because the table named in the main query is referenced in the subquery.

SELECT first_name, last_name
FROM employees
WHERE EXISTS (
    SELECT id
    FROM retirees
    WHERE id = employees.id);

This approach is particularly helpful if you need to join on more than one column, which you can't do with the IN expression.

You can also use the NOT keyword with EXISTS. For example, to find employees with no corresponding record in retirees, you would run this query:

SELECT first_name, last_name
FROM employees
WHERE NOT EXISTS (
    SELECT id
    FROM retirees
    WHERE id = employees.id);

The technique of using NOT with EXISTS is helpful for assessing whether a data set is complete.

** Common Table Expressions


Earlier in this chapter, you learned how to create derived tables by placing subqueries in a FROM clause. A second approach to creating temporary tables for querying uses the /Common Table Expression (CTE)/, a relatively recent addition to standard SQL that's informally called a “WITH clause.” Using a CTE, you can define one or more tables up front with subqueries. Then you can query the table results as often as needed in a main query that follows.

[[file:ch12.xhtml#ch12list7][Listing 12-7]] shows a simple CTE called large_counties based on our census data, followed by a query of that table. The code determines how many counties in each state have 100,000 people or more. Let's walk through the example.

➊ WITH
      large_counties (geo_name, st, p0010001)
  AS
      (
        ➋ SELECT geo_name, state_us_abbreviation, p0010001
          FROM us_counties_2010
          WHERE p0010001 >= 100000
      )
➌ SELECT st, count(*)
  FROM large_counties
  GROUP BY st
  ORDER BY count(*) DESC;

/Listing 12-7: Using a simple CTE to find large counties/

The WITH ... AS block ➊ defines the CTE's temporary table large_counties. After WITH, we name the table and list its column names in parentheses. Unlike column definitions in a CREATE TABLE statement, we don't need to provide data types, because the temporary table inherits those from the subquery ➋, which is enclosed in parentheses after AS. The subquery must return the same number of columns as defined in the temporary table, but the column names don't need to match. Also, the column list is optional if you're not renaming columns, although including the list is still a good idea for clarity even if you don't rename columns.

The main query ➌ counts and groups the rows in large_counties by st, and then orders by the count in descending order. The top five rows of the results should look like this:

st    count
--    -----
TX       39
CA       35
FL       33
PA       31
OH       28
/--snip--/

As you can see, Texas, California, and Florida are among the states with the highest number of counties with a population of 100,000 or more.

You could find the same results using a SELECT query instead of a CTE, as shown here:

SELECT state_us_abbreviation, count(*)
FROM us_counties_2010
WHERE p0010001 >= 100000
GROUP BY state_us_abbreviation
ORDER BY count(*) DESC;

So why use a CTE? One reason is that by using a CTE, you can pre-stage subsets of data to feed into a larger query for more complex analysis. Also, you can reuse each table defined in a CTE in multiple places in the main query, which means you don't have to repeat the SELECT query each time. Another commonly cited advantage is that the code is more readable than if you performed the same operation with subqueries.

[[file:ch12.xhtml#ch12list8][Listing 12-8]] uses a CTE to rewrite the join of derived tables in [[file:ch12.xhtml#ch12list4][Listing 12-4]] (finding the states that have the most meat, egg, and poultry processing plants per million population) into a more readable format:

  WITH
    ➊ counties (st, population) AS
      (SELECT state_us_abbreviation, sum(population_count_100_percent)
       FROM us_counties_2010
       GROUP BY state_us_abbreviation),

    ➋ plants (st, plants) AS
      (SELECT st, count(*) AS plants
       FROM meat_poultry_egg_inspect
       GROUP BY st)

  SELECT counties.st,
         population,
         plants,
         round((plants/population::numeric(10,1)) * 1000000, 1) AS per_million
➌ FROM counties JOIN plants
  ON counties.st = plants.st
  ORDER BY per_million DESC;

/Listing 12-8: Using CTEs in a table join/

Following the WITH keyword, we define two tables using subqueries. The first subquery, counties ➊, returns the population of each state. The second, plants ➋, returns the number of plants per state. With those tables defined, we join them ➌ on the st column in each table and calculate the rate per million. The results are identical to the joined derived tables in [[file:ch12.xhtml#ch12list4][Listing 12-4]], but [[file:ch12.xhtml#ch12list8][Listing 12-8]] is easier to read.

As another example, you can use a CTE to simplify queries with redundant code. For example, in [[file:ch12.xhtml#ch12list6][Listing 12-6]], we used a subquery with the percentile_cont() function in three different locations to find median county population. In [[file:ch12.xhtml#ch12list9][Listing 12-9]], we can write that subquery just once as a CTE:

➊ WITH us_median AS
      (SELECT percentile_cont(.5)
       WITHIN GROUP (ORDER BY p0010001) AS us_median_pop
       FROM us_counties_2010)

  SELECT geo_name,
         state_us_abbreviation AS st,
         p0010001 AS total_pop,
       ➋ us_median_pop,
       ➌ p0010001 - us_median_pop AS diff_from_median
➍ FROM us_counties_2010 CROSS JOIN us_median
➎ WHERE (p0010001 - us_median_pop)
         BETWEEN -1000 AND 1000;

/Listing 12-9: Using CTEs to minimize redundant code/

After the WITH keyword, we define us_median ➊ as the result of the same subquery used in [[file:ch12.xhtml#ch12list6][Listing 12-6]], which finds the median population using percentile_cont(). Then we reference the us_median_pop column on its own ➋, as part of a calculated column ➌, and in a WHERE clause ➎. To make the value available to every row in the us_counties_2010 table during SELECT, we use the CROSS JOIN query ➍ you learned in [[file:ch06.xhtml#ch06][Chapter 6]].

This query provides identical results to those in [[file:ch12.xhtml#ch12list6][Listing 12-6]], but we only had to write the subquery once to find the median. Not only does this save time, but it also lets you revise the query more easily. For example, to find counties whose population is close to the 90th percentile, you can substitute .9 for .5 as input to percentile_cont() in just one place.

** Cross Tabulations


/Cross tabulations/ provide a simple way to summarize and compare variables by displaying them in a table layout, or matrix. In a matrix, rows represent one variable, columns represent another variable, and each cell where a row and column intersects holds a value, such as a count or percentage.

You'll often see cross tabulations, also called /pivot tables/ or /crosstabs/, used to report summaries of survey results or to compare sets of variables. A frequent example happens during every election when candidates' votes are tallied by geography:

candidate    ward 1    ward 2    ward 3
---------    ------    ------    ------
Dirk            602     1,799     2,112
Pratt           599     1,398     1,616
Lerxst          911       902     1,114

In this case, the candidates' names are one variable, the wards (or city districts) are another variable, and the cells at the intersection of the two hold the vote totals for that candidate in that ward. Let's look at how to generate cross tabulations.

 /Installing the crosstab() Function/


Standard ANSI SQL doesn't have a crosstab function, but PostgreSQL does as part of a /module/ you can install easily. Modules include PostgreSQL extras that aren't part of the core application; they include functions related to security, text search, and more. You can find a list of PostgreSQL modules at /[[https://www.postgresql.org/docs/current/static/contrib.html]]/.

PostgreSQL's crosstab() function is part of the tablefunc module. To install tablefunc in the pgAdmin Query Tool, execute this command:

CREATE EXTENSION tablefunc;

PostgreSQL should return the message CREATE EXTENSION when it's done installing. (If you're working with another database management system, check the documentation to see whether it offers a similar functionality. For example, Microsoft SQL Server has the PIVOT command.)

Next, we'll create a basic crosstab so you can learn the syntax, and then we'll handle a more complex case.

 /Tabulating Survey Results/


Let's say your company needs a fun employee activity, so you coordinate an ice cream social at your three offices in the city. The trouble is, people are particular about ice cream flavors. To choose flavors people will like, you decide to conduct a survey.

The CSV file /ice_cream_survey.csv/ contains 200 responses to your survey. You can download this file, along with all the book's resources, at /[[https://www.nostarch.com/practicalSQL/]]/. Each row includes a response_id, office, and flavor. You'll need to count how many people chose each flavor at each office and present the results in a readable way to your colleagues.

In your analysis database, use the code in [[file:ch12.xhtml#ch12list10][Listing 12-10]] to create a table and load the data. Make sure you change the file path to the location on your computer where you saved the CSV file.

CREATE TABLE ice_cream_survey (
    response_id integer PRIMARY KEY,
    office varchar(20),
    flavor varchar(20)
);

COPY ice_cream_survey
FROM '/C:YourDirectory/ice_cream_survey.csv'
WITH (FORMAT CSV, HEADER);

/Listing 12-10: Creating and filling the ice_cream_survey table/

If you want to inspect the data, run the following to view the first five rows:

SELECT *
FROM ice_cream_survey
LIMIT 5;

The data should look like this:

response_id    office      flavor
-----------    --------    ----------
          1    Uptown      Chocolate
          2    Midtown     Chocolate
          3    Downtown    Strawberry
          4    Uptown      Chocolate
          5    Midtown     Chocolate

It looks like chocolate is in the lead! But let's confirm this choice by using the code in [[file:ch12.xhtml#ch12list11][Listing 12-11]] to generate a crosstab from the table:

  SELECT *
➊ FROM crosstab('SELECT ➋office,
                        ➌flavor,
                        ➍count(*)
                  FROM ice_cream_survey
                  GROUP BY office, flavor
                  ORDER BY office',

              ➎ 'SELECT flavor
                  FROM ice_cream_survey
                  GROUP BY flavor
                  ORDER BY flavor')

➏ AS (office varchar(20),
      chocolate bigint,
      strawberry bigint,
      vanilla bigint);

/Listing 12-11: Generating the ice cream survey crosstab/

The query begins with a SELECT * statement that selects everything from the contents of the crosstab() function ➊. We place two subqueries inside the crosstab() function. The first subquery generates the data for the crosstab and has three required columns. The first column, office ➋, supplies the row names for the crosstab, and the second column, flavor ➌, supplies the category columns. The third column supplies the values for each cell where row and column intersect in the table. In this case, we want the intersecting cells to show a count() ➍ of each flavor selected at each office. This first subquery on its own creates a simple aggregated list.

The second subquery ➎ produces the set of category names for the columns. The crosstab() function requires that the second subquery return only one column, so here we use SELECT to retrieve the flavor column, and we use GROUP BY to return that column's unique values.

Then we specify the names and data types of the crosstab's output columns following the AS keyword ➏. The list must match the row and column names in the order the subqueries generate them. For example, because the second subquery that supplies the category columns orders the flavors alphabetically, the output column list does as well.

When we run the code, our data displays in a clean, readable crosstab:

office      chocolate    strawberry    vanilla
--------    ---------    ----------    -------
Downtown           23            32         19
Midtown            41                       23
Uptown             22            17         23

It's easy to see at a glance that the Midtown office favors chocolate but has no interest in strawberry, which is represented by a NULL value showing that strawberry received no votes. But strawberry is the top choice Downtown, and the Uptown office is more evenly split among the three flavors.

 /Tabulating City Temperature Readings/


Let's create another crosstab, but this time we'll use real data. The /temperature_readings.csv/ file, also available with all the book's resources at /[[https://www.nostarch.com/practicalSQL/]]/, contains a year's worth of daily temperature readings from three observation stations around the United States: Chicago, Seattle, and Waikiki, a neighborhood on the south shore of the city of Honolulu. The data come from the U.S. National Oceanic and Atmospheric Administration (NOAA) at /[[https://www.ncdc.noaa.gov/cdo-web/datatools/findstation/]]/.

Each row in the CSV file contains four values: the station name, the date, the day's maximum temperature, and the day's minimum temperature. All temperatures are in Fahrenheit. For each month in each city, we want to calculate the median high temperature so we can compare climates. [[file:ch12.xhtml#ch12list12][Listing 12-12]] contains the code to create the temperature_readings table and import the CSV file:

CREATE TABLE temperature_readings (
    reading_id bigserial,
    station_name varchar(50),
    observation_date date,
    max_temp integer,
    min_temp integer
);

COPY temperature_readings
     (station_name, observation_date, max_temp, min_temp)
FROM '/C:YourDirectory/temperature_readings.csv'
WITH (FORMAT CSV, HEADER);

/Listing 12-12: Creating and filling a temperature_readings table/

The table contains the four columns from the CSV file along with an added reading_id of type bigserial that we use as a surrogate primary key. If you perform a quick count on the table, you should have 1,077 rows. Now, let's see what cross tabulating the data does using [[file:ch12.xhtml#ch12list13][Listing 12-13]]:

SELECT *
FROM crosstab('SELECT
               ➊ station_name,
               ➋ date_part(''month'', observation_date),
               ➌ percentile_cont(.5)
                      WITHIN GROUP (ORDER BY max_temp)
               FROM temperature_readings
               GROUP BY station_name,
                        date_part(''month'', observation_date)
               ORDER BY station_name',

              'SELECT month
               FROM ➍generate_series(1,12) month')

AS (station varchar(50),
    jan numeric(3,0),
    feb numeric(3,0),
    mar numeric(3,0),
    apr numeric(3,0),
    may numeric(3,0),
    jun numeric(3,0),
    jul numeric(3,0),
    aug numeric(3,0),
    sep numeric(3,0),
    oct numeric(3,0),
    nov numeric(3,0),
    dec numeric(3,0)
);

/Listing 12-13: Generating the temperature readings crosstab/

The structure of the crosstab is the same as in [[file:ch12.xhtml#ch12list11][Listing 12-11]]. The first subquery inside the crosstab() function generates the data for the crosstab, calculating the median maximum temperature for each month. It supplies the three required columns. The first column, station_name ➊, names the rows. The second column uses the date_part() function ➋ you learned in [[file:ch11.xhtml#ch11][Chapter 11]] to extract the month from observation_date, which provides the crosstab columns. Then we use percentile_cont(.5) ➌ to find the 50th percentile, or the median, of the max_temp. We group by station name and month so we have a median max_temp for each month at each station.

As in [[file:ch12.xhtml#ch12list11][Listing 12-11]], the second subquery produces the set of category names for the columns. I'm using a function called generate_series() ➍ in a manner noted in the official PostgreSQL documentation to create a list of numbers from 1 to 12 that match the month numbers date_part() extracts from observation_date.

Following AS, we provide the names and data types for the crosstab's output columns. Each is a numeric type, matching the output of the percentile function. The following output is practically poetry:

[[../images/prog_page_207.jpg]]

We've transformed a raw set of daily readings into a compact table showing the median maximum temperature each month for each station. You can see at a glance that the temperature in Waikiki is consistently balmy, whereas Chicago's median high temperatures vary from just above freezing to downright pleasant. Seattle falls between the two.

Crosstabs do take time to set up, but viewing data sets in a matrix often makes comparisons easier than viewing the same data in a vertical list. Keep in mind that the crosstab() function is CPU-intensive, so tread carefully when querying sets that have millions or billions of rows.

** Reclassifying Values with CASE


The ANSI Standard SQL CASE statement is a /conditional expression/, meaning it lets you add some “if this, then . . .” logic to a query. You can use CASE in multiple ways, but for data analysis, it's handy for reclassifying values into categories. You can create categories based on ranges in your data and classify values according to those categories.

The CASE syntax follows this pattern:

➊ CASE WHEN /condition/ THEN /result/
     ➋ WHEN /another_condition/ THEN /result/
     ➌ ELSE /result/
➍ END

We give the CASE keyword ➊, and then provide at least one WHEN /condition/ THEN /result/ clause, where /condition/ is any expression the database can evaluate as true or false, such as county = 'Dutchess County' or date > '1995-08-09'. If the condition is true, the CASE statement returns the /result/ and stops checking any further conditions. The result can be any valid data type. If the condition is false, the database moves on to evaluate the next condition.

To evaluate more conditions, we can add optional WHEN ... THEN clauses ➋. We can also provide an optional ELSE clause ➌ to return a result in case no condition evaluates as true. Without an ELSE clause, the statement would return a NULL when no conditions are true. The statement finishes with an END keyword ➍.

[[file:ch12.xhtml#ch12list14][Listing 12-14]] shows how to use the CASE statement to reclassify the temperature readings data into descriptive groups (named according to my own bias against cold weather):

SELECT max_temp,
       CASE WHEN max_temp >= 90 THEN 'Hot'
            WHEN max_temp BETWEEN 70 AND 89 THEN 'Warm'
            WHEN max_temp BETWEEN 50 AND 69 THEN 'Pleasant'
            WHEN max_temp BETWEEN 33 AND 49 THEN 'Cold'
            WHEN max_temp BETWEEN 20 AND 32 THEN 'Freezing'
            ELSE 'Inhumane'
        END AS temperature_group
FROM temperature_readings;

/Listing 12-14: Reclassifying temperature data with CASE/

We create five ranges for the max_temp column in temperature_readings, which we define using comparison operators. The CASE statement evaluates each value to find whether any of the five expressions are true. If so, the statement outputs the appropriate text. Note that the ranges account for all possible values in the column, leaving no gaps. If none of the statements is true, then the ELSE clause assigns the value to the category Inhumane. The way I've structured the ranges, this happens only when max_temp is below 20 degrees. Alternatively, we could replace ELSE with a WHEN clause that looks for temperatures less than or equal to 19 degrees by using max_temp <= 19.

Run the code; the first five rows of output should look like this:

max_temp    temperature_group
--------    -----------------
      31    Freezing
      34    Cold
      32    Freezing
      32    Freezing
      34    Cold
      /--snip--/

Now that we've collapsed the data set into six categories, let's use those categories to compare climate among the three cities in the table.

** Using CASE in a Common Table Expression


The operation we performed with CASE on the temperature data in the previous section is a good example of a preprocessing step you would use in a CTE. Now that we've grouped the temperatures in categories, let's count the groups by city in a CTE to see how many days of the year fall into each temperature category.

[[file:ch12.xhtml#ch12list15][Listing 12-15]] shows the code for reclassifying the daily maximum temperatures recast to generate a temps_collapsed CTE and then use it for an analysis:

➊ WITH temps_collapsed (station_name, max_temperature_group) AS
      (SELECT station_name,
             CASE WHEN max_temp >= 90 THEN 'Hot'
                  WHEN max_temp BETWEEN 70 AND 89 THEN 'Warm'
                  WHEN max_temp BETWEEN 50 AND 69 THEN 'Pleasant'
                  WHEN max_temp BETWEEN 33 AND 49 THEN 'Cold'
                  WHEN max_temp BETWEEN 20 AND 32 THEN 'Freezing'
                  ELSE 'Inhumane'
              END
       FROM temperature_readings)

➋ SELECT station_name, max_temperature_group, count(*)
  FROM temps_collapsed
  GROUP BY station_name, max_temperature_group
  ORDER BY station_name, count(*) DESC;

/Listing 12-15: Using CASE in a CTE/

This code reclassifies the temperatures, and then counts and groups by station name to find general climate classifications of each city. The WITH keyword defines the CTE of temps_collapsed ➊, which has two columns: station_name and max_temperature_group. We then run a SELECT query on the CTE ➋, performing straightforward count(*) and GROUP BY operations on both columns. The results should look like this:

station_name                      max_temperature_group    count
------------------------------    ---------------------    -----
CHICAGO NORTHERLY ISLAND IL US    Warm                       133
CHICAGO NORTHERLY ISLAND IL US    Cold                        92
CHICAGO NORTHERLY ISLAND IL US    Pleasant                    91
CHICAGO NORTHERLY ISLAND IL US    Freezing                    30
CHICAGO NORTHERLY ISLAND IL US    Inhumane                     8
CHICAGO NORTHERLY ISLAND IL US    Hot                          8
SEATTLE BOEING FIELD WA US        Pleasant                   198
SEATTLE BOEING FIELD WA US        Warm                        98
SEATTLE BOEING FIELD WA US        Cold                        50
SEATTLE BOEING FIELD WA US        Hot                          3
WAIKIKI 717.2 HI US               Warm                       361
WAIKIKI 717.2 HI US               Hot                          5

Using this classification scheme, the amazingly consistent Waikiki weather, with Warm maximum temperatures 361 days of the year, confirms its appeal as a vacation destination. From a temperature standpoint, Seattle looks good too, with nearly 300 days of high temps categorized as Pleasant or Warm (although this belies Seattle's legendary rainfall). Chicago, with 30 days of Freezing max temps and 8 days Inhumane, probably isn't for me.

** Wrapping Up


In this chapter, you learned to make queries work harder for you. You can now add subqueries in multiple locations to provide finer control over filtering or preprocessing data before analyzing it in a main query. You also can visualize data in a matrix using cross tabulations and reclassify data into groups; both techniques give you more ways to find and tell stories using your data. Great work!

Throughout the next chapters, we'll dive into SQL techniques that are more specific to PostgreSQL. We'll begin by working with and searching text and strings.


*TRY IT YOURSELF*

Here are two tasks to help you become more familiar with the concepts introduced in the chapter:

1. Revise the code in [[file:ch12.xhtml#ch12list15][Listing 12-15]] to dig deeper into the nuances of Waikiki's high temperatures. Limit the temps_collapsed table to the Waikiki maximum daily temperature observations. Then use the WHEN clauses in the CASE statement to reclassify the temperatures into seven groups that would result in the following text output:

   '90 or more'
   '88-89'
   '86-87'
   '84-85'
   '82-83'
   '80-81'
   '79 or less'

   In which of those groups does Waikiki's daily maximum temperature fall most often?

2. Revise the ice cream survey crosstab in [[file:ch12.xhtml#ch12list11][Listing 12-11]] to flip the table. In other words, make flavor the rows and office the columns. Which elements of the query do you need to change? Are the counts different?


                        3\\
WAIKIKI 717.2 HI US               Warm                       361\\
WAIKIKI 717.2 HI US               Hot                          5

Using this classification scheme, the amazingly consistent Waikiki weather, with Warm maximum temperatures 361 days of the year, confirms its appeal as a vacation destination. From a temperature standpoint, Seattle looks good too, with nearly 300 days of high temps categorized as Pleasant or Warm (although this belies Seattle's legendary rainfall). Chicago, with 30 days of Freezing max temps and 8 days Inhumane, probably isn't for me.

**** Wrapping Up
    :PROPERTIES:
    :CUSTOM_ID: lev218
    :CLASS: h3
    :END:

In this chapter, you learned to make queries work harder for you. You can now add subqueries in multiple locations to provide finer control over filtering or preprocessing data before analyzing it in a main query. You also can visualize data in a matrix using cross tabulations and reclassify data into groups; both techniques give you more ways to find and tell stories using your data. Great work!

Throughout the next chapters, we'll dive into SQL techniques that are more specific to PostgreSQL. We'll begin by working with and searching text and strings.

<<ch12sb1>>
*TRY IT YOURSELF*

Here are two tasks to help you become more familiar with the concepts introduced in the chapter:

1. Revise the code in [[file:ch12.xhtml#ch12list15][Listing 12-15]] to dig deeper into the nuances of Waikiki's high temperatures. Limit the temps\_collapsed table to the Waikiki maximum daily temperature observations. Then use the WHEN clauses in the CASE statement to reclassify the temperatures into seven groups that would result in the following text output:

   '90 or more'\\
   '88-89'\\
   '86-87'\\
   '84-85'\\
   '82-83'\\
   '80-81'\\
   '79 or less'

   In which of those groups does Waikiki's daily maximum temperature fall most often?

2. Revise the ice cream survey crosstab in [[file:ch12.xhtml#ch12list11][Listing 12-11]] to flip the table. In other words, make flavor the rows and office the columns. Which elements of the query do you need to change? Are the counts different?


