* 11.Working With Dates And Times

Columns filled with dates and times can indicate /when/ events happened or /how long/ they took, and that can lead to interesting lines of inquiry. What patterns exist in the moments on a timeline? Which events were shortest or longest? What relationships exist between a particular activity and the time of day or season in which it occurred?

In this chapter, we'll explore these kinds of questions using SQL data types for dates and times and their related functions. We'll start with a closer look at data types and functions related to dates and times. Then we'll explore a data set that contains information on trips by New York City taxicabs to look for patterns and try to discover what, if any, story the data tells. We'll also explore time zones using Amtrak data to calculate the duration of train trips across the United States.

** Data Types and Functions for Dates and Times


[[file:ch03.xhtml#ch03][Chapter 3]] explored primary SQL data types, but to review, here are the four data types related to dates and times:

date Records only the date. PostgreSQL accepts several date formats. For example, valid formats for adding the 21st day of September 2018 are September 21, 2018 or 9/21/2018. I recommend using YYYY-MM-DD (or 2018-09-21), which is the ISO 8601 international standard format and also the default PostgreSQL date output. Using the ISO format helps avoid confusion when sharing data internationally.

time Records only the time. Adding with time zone makes the column time zone aware. The ISO 8601 format is HH:MM:SS, where HH represents the hour, MM the minutes, and SS the seconds. You can add an optional time zone designator. For example, 2:24 PM in San Francisco during standard time in fall and winter would be 14:24 PST.

*timestamp* Records the date and time. You can add with time zone to make the column time zone aware. The format timestamp with time zone is part of the SQL standard, but with PostgreSQL, you can use the shorthand timestamptz, which combines the date and time formats plus a time zone designator at the end: YYYY-MM-DD HH:MM:SS TZ. You can specify time zones in three different formats: its UTC offset, an area/location designator, or a standard abbreviation.

interval Holds a value that represents a unit of time expressed in the format quantity unit. It doesn't record the start or end of a period, only its duration. Examples include 12 days or 8 hours.

The first three data types, date, time, and timestamp, are known as /datetime types/ whose values are called /datetimes/. The interval value is an /interval type/ whose values are /intervals/. All four data types can track the system clock and the nuances of the calendar. For example, date and timestamp recognize that June has 30 days. Therefore, June 31 is an invalid datetime value that causes the database to throw an error. Likewise, the date February 29 is valid only in a leap year, such as 2020.

** Manipulating Dates and Times


We can use SQL functions to perform calculations on dates and times or extract components from them. For example, we can retrieve the day of the week from a timestamp or extract just the month from a date. ANSI SQL outlines a handful of functions to do this, but many database managers (including MySQL and Microsoft SQL Server) deviate from the standard to implement their own date and time data types, syntax, and function names. If you're using a database other than PostgreSQL, check its documentation.

Let's review how to manipulate dates and times using PostgreSQL functions.

 /Extracting the Components of a timestamp Value/


It's not unusual to need just one piece of a date or time value for analysis, particularly when you're aggregating results by month, year, or even minute. We can extract these components using the PostgreSQL date_part() function. Its format looks like this:

date_part(/text/, /value/)

The function takes two inputs. The first is a string in text format that represents the part of the date or time to extract, such as hour, minute, or week. The second is the date, time, or timestamp value. To see the date_part() function in action, we'll execute it multiple times on the same value using the code in [[file:ch11.xhtml#ch11list1][Listing 11-1]]. In the listing, we format the string as a timestamp with time zone using the PostgreSQL-specific shorthand timestamptz. We also assign a column name to each with AS.

SELECT
    date_part('year', '2019-12-01 18:37:12 EST'::timestamptz) AS "year",
    date_part('month', '2019-12-01 18:37:12 EST'::timestamptz) AS "month",
    date_part('day', '2019-12-01 18:37:12 EST'::timestamptz) AS "day",
    date_part('hour', '2019-12-01 18:37:12 EST'::timestamptz) AS "hour",
    date_part('minute', '2019-12-01 18:37:12 EST'::timestamptz) AS "minute",
    date_part('seconds', '2019-12-01 18:37:12 EST'::timestamptz) AS "seconds",
    date_part('timezone_hour', '2019-12-01 18:37:12 EST'::timestamptz) AS "tz",
    date_part('week', '2019-12-01 18:37:12 EST'::timestamptz) AS "week",
    date_part('quarter', '2019-12-01 18:37:12 EST'::timestamptz) AS "quarter",
    date_part('epoch', '2019-12-01 18:37:12 EST'::timestamptz) AS "epoch";

/Listing 11-1: Extracting components of a timestamp value using date_part()/

Each column statement in this SELECT query first uses a string to name the component we want to extract: year, month, day, and so on. The second input uses the string 2019-12-01 18:37:12 EST cast as a timestamp with time zone with the PostgreSQL double-colon syntax and the timestamptz shorthand. In December, the United States is observing standard time, which is why we can designate the Eastern time zone using the Eastern Standard Time (EST) designation.

Here's the output as shown on my computer, which is located in the U.S. Eastern time zone. (The database converts the values to reflect your PostgreSQL time zone setting, so your output might be different; for example, if it's set to the U.S. Pacific time zone, the hour will show as 15):

[[../images/prog_page_173.jpg]]

Each column contains a single value that represents 6:37:12 PM on December 1, 2019, in the U.S. Eastern time zone. Even though you designated the time zone using EST in the string, PostgreSQL reports back the /UTC offset/ of that time zone, which is the number of hours plus or minus from UTC. UTC refers to Coordinated Universal Time, a world time standard, as well as the value of UTC +/−00:00, the time zone that covers the United Kingdom and Western Africa. Here, the UTC offset is -5 (because EST is five hours behind UTC).

*NOTE*

/You can derive the UTC offset from the time zone but not vice versa. Each UTC offset can refer to multiple named time zones plus standard and daylight saving time variants./

The first seven values are easy to recognize from the original timestamp, but the last three are calculated values that deserve an explanation.

The week column shows that December 1, 2019, falls in the 48th week of the year. This number is determined by ISO 8601 standards, which start each week on a Monday. That means a week at the end of a year can extend from December into January of the following year.

The quarter column shows that our test date is part of the fourth quarter of the year. The epoch column shows a measurement, which is used in computer systems and programming languages, that represents the number of seconds elapsed before or after 12 AM, January 1, 1970, at UTC 0. A positive value designates a time since that point; a negative value designates a time before it. In this example, 1,575,243,432 seconds elapsed between January 1, 1970, and the timestamp. Epoch is useful if you need to compare two timestamps mathematically on an absolute scale.

PostgreSQL also supports the SQL-standard extract() function, which parses datetimes in the same way as the date_part() function. I've featured date_part() here instead for two reasons. First, its name helpfully reminds us what it does. Second, extract() isn't widely supported by database managers. Most notably, it's absent in Microsoft's SQL Server. Nevertheless, if you need to use extract(), the syntax takes this form:

extract(/text/ from /value/)

To replicate the first date_part() example in [[file:ch11.xhtml#ch11list1][Listing 11-1]] where we pull the year from the timestamp, we'd set up the function like this:

extract('year' from '2019-12-01 18:37:12 EST'::timestamptz)

PostgreSQL provides additional components you can extract or calculate from dates and times. For the full list of functions, see the documentation at /[[https://www.postgresql.org/docs/current/static/functions-datetime.html]]/.

 /Creating Datetime Values from timestamp Components/


It's not unusual to come across a data set in which the year, month, and day exist in separate columns, and you might want to create a datetime value from these components. To perform calculations on a date, it's helpful to combine and format those pieces correctly into one column.

You can use the following PostgreSQL functions to make datetime objects:

*make_date(/year/, /month/, /day/)* Returns a value of type date

*make_time(/hour/, /minute/, /seconds/)* Returns a value of type time without time zone

*make_timestamptz(/year/, /month/, /day/, /hour/, /minute/, /second/, /time zone/)* Returns a timestamp with time zone

The variables for these three functions take integer types as input, with two exceptions: seconds are of the type double precision because you can supply fractions of seconds, and time zones must be specified with a text string that names the time zone.

[[file:ch11.xhtml#ch11list2][Listing 11-2]] shows examples of the three functions in action using components of February 22, 2018, for the date, and 6:04:30.3 PM in Lisbon, Portugal for the time:

SELECT make_date(2018, 2, 22);
SELECT make_time(18, 4, 30.3);
SELECT make_timestamptz(2018, 2, 22, 18, 4, 30.3, 'Europe/Lisbon');

/Listing 11-2: Three functions for making datetimes from components/

When I run each query in order, the output on my computer in the U.S. Eastern time zone is as follows. Again, yours may differ depending on your time zone setting:

2018-02-22
18:04:30.3
2018-02-22 13:04:30.3-05

Notice that the timestamp in the third line shows 13:04:30.3, which is Eastern Standard Time and is five hours behind (-05) the time input to the function: 18:04:30.3. In our discussion on time zone--enabled columns in [[file:ch03.xhtml#lev41][“Dates and Times”]] on [[file:ch03.xhtml#page_32][page 32]], I noted that PostgreSQL displays times relative to the client's time zone or the time zone set in the database session. This output reflects the appropriate time because my location is five hours behind Lisbon. We'll explore working with time zones in more detail, and you'll learn to adjust its display in [[file:ch11.xhtml#lev173][“Working with Time Zones”]] on [[file:ch11.xhtml#page_177][page 177]].

 /Retrieving the Current Date and Time/


If you need to record the current date or time as part of a query---when updating a row, for example---standard SQL provides functions for that too. The following functions record the time as of the start of the query:

current_date Returns the date.

current_time Returns the current time with time zone.

current_timestamp Returns the current timestamp with time zone. A shorthand PostgreSQL-specific version is now().

localtime Returns the current time without time zone.

localtimestamp Returns the current timestamp without time zone.

Because these functions record the time at the start of the query (or a collection of queries grouped under a /transaction/, which I covered in [[file:ch09.xhtml#ch09][Chapter 9]]), they'll provide that same time throughout the execution of a query regardless of how long the query runs. So, if your query updates 100,000 rows and takes 15 seconds to run, any timestamp recorded at the start of the query will be applied to each row, and so each row will receive the same timestamp.

If, instead, you want the date and time to reflect how the clock changes during the execution of the query, you can use the PostgreSQL-specific clock_timestamp() function to record the current time as it elapses. That way, if you're updating 100,000 rows and inserting a timestamp each time, each row gets the time the row updated rather than the time at the start of the query. Note that clock_timestamp() can slow large queries and may be subject to system limitations.

[[file:ch11.xhtml#ch11list3][Listing 11-3]] shows current_timestamp and clock_timestamp() in action when inserting a row in a table:

CREATE TABLE current_time_example (
    time_id bigserial,
  ➊ current_timestamp_col timestamp with time zone,
  ➋ clock_timestamp_col timestamp with time zone
);

INSERT INTO current_time_example (current_timestamp_col, clock_timestamp_col)
  ➌ (SELECT current_timestamp,
            clock_timestamp()
     FROM generate_series(1,1000));

SELECT * FROM current_time_example;

/Listing 11-3: Comparing current_timestamp and clock_timestamp() during row insert/

The code creates a table that includes two timestamp columns with a time zone. The first holds the result of the current_timestamp function ➊, which records the time at the start of the INSERT statement that adds 1,000 rows to the table. To do that, we use the generate_series() function, which returns a set of integers starting with 1 and ending with 1,000. The second column holds the result of the clock_timestamp() function ➋, which records the time of insertion of each row. You call both functions as part of the INSERT statement ➌. Run the query, and the result from the final SELECT statement should show that the time in the current_timestamp_col is the same for all rows, whereas the time in clock_timestamp_col increases with each row inserted.

** Working with Time Zones


Time zone data lets the dates and times in your database reflect the location around the globe where those dates and times apply and their UTC offset. A timestamp of 1 PM is only useful, for example, if you know whether the value refers to local time in Asia, Eastern Europe, one of the 12 time zones of Antarctica, or anywhere else on the globe.

Of course, very often you'll receive data sets that contain no time zone data in their datetime columns. This isn't always a deal breaker in terms of whether or not you should continue to use the data. If you know that every event in the data happened in the same location, having the time zone in the timestamp is less critical, and it's relatively easy to modify all the timestamps of your data to reflect that single time zone.

Let's look at some strategies for working with time zones in your data.

 /Finding Your Time Zone Setting/


When working with time zones in SQL, you first need know the time zone setting for your database server. If you installed PostgreSQL on your own computer, the default will be your local time zone. If you're connecting to a PostgreSQL database elsewhere, perhaps on a network or a cloud provider such as Amazon Web Services, the time zone setting may be different than your own. To help avoid confusion, database administrators often set a shared server's time zone to UTC.

To find out the default time zone of your PostgreSQL server, use the SHOW command with timezone, as shown in [[file:ch11.xhtml#ch11list4][Listing 11-4]]:

SHOW timezone;

/Listing 11-4: Showing your PostgreSQL server's default time zone/

Entering [[file:ch11.xhtml#ch11list4][Listing 11-4]] into pgAdmin and running it on my computer returns US/Eastern, one of several location names that falls into the Eastern time zone, which encompasses eastern Canada and the United States, the Caribbean, and parts of Mexico.

*NOTE*

/You can use SHOW ALL; to see the settings of every parameter on your PostgreSQL server./

You can also use the two commands in [[file:ch11.xhtml#ch11list5][Listing 11-5]] to list all time zone names, abbreviations, and their UTC offsets:

SELECT * FROM pg_timezone_abbrevs;
SELECT * FROM pg_timezone_names;

/Listing 11-5: Showing time zone abbreviations and names/

You can easily filter either of these SELECT statements with a WHERE clause to look up specific location names or time zones:

SELECT * FROM pg_timezone_names
WHERE name LIKE 'Europe%';

This code should return a table listing that includes the time zone name, abbreviation, UTC offset, and a boolean column is_dst that notes whether the time zone is currently observing daylight saving time:

name                abbrev    utc_offset    is_dst
----------------    ------    ----------    ------
Europe/Amsterdam    CEST      02:00:00      t
Europe/Andorra      CEST      02:00:00      t
Europe/Astrakhan    +04       04:00:00      f
Europe/Athens       EEST      03:00:00      t
Europe/Belfast      BST       01:00:00      t
/--snip--/

This is a faster way of looking up time zones than using Wikipedia. Now let's look at how to set the time zone to a particular value.

 /Setting the Time Zone/


When you installed PostgreSQL, the server's default time zone was set as a parameter in /postgresql.conf/, a file that contains dozens of values read by PostgreSQL each time it starts. The location of /postgresql.conf/ in your file system varies depending on your operating system and sometimes on the way you installed PostgreSQL. To make permanent changes to /postgresql.conf/, you need to edit the file and restart the server, which might be impossible if you're not the owner of the machine. Changes to configurations might also have unintended consequences for other users or applications.

I'll cover working with /postgresql.conf/ in more depth in [[file:ch17.xhtml#ch17][Chapter 17]]. However, for now you can easily set the pgAdmin client's time zone on a per-session basis, and the change should last as long as you're connected to the server. This solution is handy when you want to specify how you view a particular table or handle timestamps in a query.

To set and change the pgAdmin client's time zone, we use the command SET timezone TO, as shown in [[file:ch11.xhtml#ch11list6][Listing 11-6]]:

➊ SET timezone TO 'US/Pacific';

➋ CREATE TABLE time_zone_test (
      test_date timestamp with time zone
  );
➌ INSERT INTO time_zone_test VALUES ('2020-01-01 4:00');

➍ SELECT test_date
  FROM time_zone_test;

➎ SET timezone TO 'US/Eastern';

➏ SELECT test_date
  FROM time_zone_test;

➐ SELECT test_date AT TIME ZONE 'Asia/Seoul'
  FROM time_zone_test;

/Listing 11-6: Setting the time zone for a client session/

First, we set the time zone to US/Pacific ➊, which designates the Pacific time zone that covers western Canada and the United States along with Baja California in Mexico. Second, we create a one-column table ➋ with a data type of timestamp with time zone and insert a single row to display a test result. Notice that the value inserted, 2020-01-01 4:00, is a timestamp with no time zone ➌. You'll encounter timestamps with no time zone quite often, particularly when you acquire data sets restricted to a specific location.

When executed, the first SELECT statement ➍ returns 2020-01-01 4:00 as a timestamp that now contains time zone data:

test_date
----------------------
2020-01-01 04:00:00-08

Recall from our discussion on data types in [[file:ch03.xhtml#ch03][Chapter 3]] that the -08 at the end of this timestamp is the UTC offset. In this case, the -08 shows that the Pacific time zone is eight hours behind UTC. Because we initially set the pgAdmin client's time zone to US/Pacific for this session, any value we now enter into a column that is time zone aware will be in Pacific time and coded accordingly. However, it's worth noting that on the server, the timestamp with time zone data type always stores data as UTC internally; the time zone setting governs how it's displayed.

Now comes some fun. We change the time zone for this session to the Eastern time zone using the SET command ➎ and the US/Eastern designation. Then, when we execute the SELECT statement ➏ again, the result should be as follows:

test_date
----------------------
2020-01-01 07:00:00-05

In this example, two components of the timestamp have changed: the time is now 07:00, and the UTC offset is -05 because we're viewing the timestamp from the perspective of the Eastern time zone: 4 AM Pacific is 7 AM Eastern. The original Pacific time value remains unaltered in the table, and the database converts it to show the time in whatever time zone we set at ➎.

Even more convenient is that we can view a timestamp through the lens of any time zone without changing the session setting. The final SELECT statement uses the AT TIME ZONE keywords ➐ to display the timestamp in our session as Korea standard time (KST) by specifying Asia/Seoul:

timezone
-------------------
2020-01-01 21:00:00

Now we know that the database value of 4 AM in US/Pacific on January 1, 2020, is equivalent to 9 PM that same day in Asia/Seoul. Again, this syntax changes the output data type, but the data on the server remains unchanged. If the original value is a timestamp with time zone, the output removes the time zone. If the original value has no time zone, the output is timestamp with time zone.

The ability of databases to track time zones is extremely important for accurate calculations of intervals, as you'll see next.

** Calculations with Dates and Times


We can perform simple arithmetic on datetime and interval types the same way we can on numbers. Addition, subtraction, multiplication, and division are all possible in PostgreSQL using the math operators +, -, *, and /. For example, you can subtract one date from another date to get an integer that represents the difference in days between the two dates. The following code returns an integer of 3:

SELECT '9/30/1929'::date - '9/27/1929'::date;

The result indicates that these two dates are exactly three days apart.

Likewise, you can use the following code to add a time interval to a date to return a new date:

SELECT '9/30/1929'::date + '5 years'::interval;

This code adds five years to the date 9/30/1929 to return a timestamp value of 9/30/1934.

You can find more examples of math functions you can use with dates and times in the PostgreSQL documentation at /[[https://www.postgresql.org/docs/current/static/functions-datetime.html]]/. Let's explore some more practical examples using actual transportation data.

 /Finding Patterns in New York City Taxi Data/


When I visit New York City, I usually take at least one ride in one of the 13,500 iconic yellow cars that ferry hundreds of thousands of people across the city's five boroughs each day. The New York City Taxi and Limousine Commission releases data on monthly yellow taxi trips plus other for-hire vehicles. We'll use this large, rich data set to put date functions to practical use.

The /yellow_tripdata_2016_06_01.csv/ file available from the book's resources (at /[[https://www.nostarch.com/practicalSQL/]]/) holds one day of yellow taxi trip records from June 1, 2016. Save the file to your computer and execute the code in [[file:ch11.xhtml#ch11list7][Listing 11-7]] to build the nyc_yellow_taxi_trips_2016_06_01 table. Remember to change the file path in the COPY command to the location where you've saved the file and adjust the path format to reflect whether you're using Windows, macOS, or Linux.

➊ CREATE TABLE nyc_yellow_taxi_trips_2016_06_01 (
      trip_id bigserial PRIMARY KEY,
      vendor_id varchar(1) NOT NULL,
      tpep_pickup_datetime timestamp with time zone NOT NULL,
      tpep_dropoff_datetime timestamp with time zone NOT NULL,
      passenger_count integer NOT NULL,
      trip_distance numeric(8,2) NOT NULL,
      pickup_longitude numeric(18,15) NOT NULL,
      pickup_latitude numeric(18,15) NOT NULL,
      rate_code_id varchar(2) NOT NULL,
      store_and_fwd_flag varchar(1) NOT NULL,
      dropoff_longitude numeric(18,15) NOT NULL,
      dropoff_latitude numeric(18,15) NOT NULL,
      payment_type varchar(1) NOT NULL,
      fare_amount numeric(9,2) NOT NULL,
      extra numeric(9,2) NOT NULL,
      mta_tax numeric(5,2) NOT NULL,
      tip_amount numeric(9,2) NOT NULL,
      tolls_amount numeric(9,2) NOT NULL,
      improvement_surcharge numeric(9,2) NOT NULL,
      total_amount numeric(9,2) NOT NULL
  );

➋ COPY nyc_yellow_taxi_trips_2016_06_01 (
      vendor_id,
      tpep_pickup_datetime,
      tpep_dropoff_datetime,
      passenger_count,
      trip_distance,
      pickup_longitude,
      pickup_latitude,
      rate_code_id,
      store_and_fwd_flag,
      dropoff_longitude,
      dropoff_latitude,
      payment_type,
      fare_amount,
      extra,
      mta_tax,
      tip_amount,
      tolls_amount,
      improvement_surcharge,
      total_amount
   )
  FROM '/C:YourDirectory/yellow_tripdata_2016_06_01.csv'
  WITH (FORMAT CSV, HEADER, DELIMITER ',');

➌ CREATE INDEX tpep_pickup_idx
  ON nyc_yellow_taxi_trips_2016_06_01 (tpep_pickup_datetime);

/Listing 11-7: Creating a table and importing NYC yellow taxi data/

The code in [[file:ch11.xhtml#ch11list7][Listing 11-7]] builds the table ➊, imports the rows ➋, and creates an index ➌. In the COPY statement, we provide the names of columns because the input CSV file doesn't include the trip_id column that exists in the target table. That column is of type bigserial, which you've learned is an auto-incrementing integer and will fill automatically. After your import is complete, you should have 368,774 rows, one for each yellow cab ride on June 1, 2016. You can check the number of rows in your table with a count using the following code:

SELECT count(*) FROM nyc_yellow_taxi_trips_2016_06_01;

Each row includes data on the number of passengers, the location of pickup and drop-off in latitude and longitude, and the fare and tips in U.S. dollars. The data dictionary that describes all columns and codes is available at /[[http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf]]/. For these exercises, we're most interested in the timestamp columns tpep_pickup_datetime and tpep_dropoff_datetime, which represent the start and end times of the ride. (The Technology Passenger Enhancements Project [TPEP] is a program that in part includes automated collection of data about taxi rides.)

The values in both timestamp columns include the time zone provided by the Taxi and Limousine Commission. In all rows of the CSV file, the time zone included with the timestamp is shown as -4, which is the summertime UTC offset for the Eastern time zone when New York City and the rest of the U.S. East Coast observe daylight saving time. If you're not or your PostgreSQL server isn't located in Eastern time, I suggest setting your time zone using the following code so your results will match mine:

SET timezone TO 'US/Eastern';

Now let's explore the patterns we can identify in the data related to these times.

** The Busiest Time of Day


One question you might ask after viewing this data set is when taxis provide the most rides. Is it morning or evening rush hour, or is there another time---at least, on this day---when rides spiked? You can determine the answer with a simple aggregation query that uses date_part().

[[file:ch11.xhtml#ch11list8][Listing 11-8]] contains the query to count rides by hour using the pickup time as the input:

SELECT
  ➊ date_part('hour', tpep_pickup_datetime) AS trip_hour,
  ➋ count(*)
FROM nyc_yellow_taxi_trips_2016_06_01
GROUP BY trip_hour
ORDER BY trip_hour;

/Listing 11-8: Counting taxi trips by hour/

In the query's first column ➊, date_part() extracts the hour from tpep_pickup_datetime so we can group the number of rides by hour. Then we aggregate the number of rides in the second column via the count() function ➋. The rest of the query follows the standard patterns for grouping and ordering the results, which should return 24 rows, one for each hour of the day:

trip_hour    count
---------    -----
        0     8182
        1     5003
        2     3070
        3     2275
        4     2229
        5     3925
        6    10825
        7    18287
        8    21062
        9    18975
       10    17367
       11    17383
       12    18031
       13    17998
       14    19125
       15    18053
       16    15069
       17    18513
       18    22689
       19    23190
       20    23098
       21    24106
       22    22554
       23    17765

Eyeballing the numbers, it's apparent that on June 1, 2016, New York City taxis had the most passengers between 6 PM and 10 PM, possibly reflecting commutes home plus the plethora of city activities on a summer evening. But to see the overall pattern, it's best to visualize the data. Let's do this next.

** Exporting to CSV for Visualization in Excel


Charting data with a tool such as Microsoft Excel makes it easier to understand patterns, so I often export query results to a CSV file and work up a quick chart. [[file:ch11.xhtml#ch11list9][Listing 11-9]] uses the query from the preceding example within a COPY ... TO statement, similar to [[file:ch04.xhtml#ch04list9][Listing 4-9]] on [[file:ch04.xhtml#page_52][page 52]]:

COPY
    (SELECT
        date_part('hour', tpep_pickup_datetime) AS trip_hour,
        count(*)
    FROM nyc_yellow_taxi_trips_2016_06_01
    GROUP BY trip_hour
    ORDER BY trip_hour
    )
TO '/C:YourDirectory/hourly_pickups_2016_06_01.csv'
WITH (FORMAT CSV, HEADER, DELIMITER ',');

/Listing 11-9: Exporting taxi pickups per hour to a CSV file/

When I load the data into Excel and build a line graph, the day's pattern becomes more obvious and thought-provoking, as shown in [[file:ch11.xhtml#ch11fig1][Figure 11-1]].

[[../images/f0184-01.jpg]]

/Figure 11-1: NYC yellow taxi pickups by hour/

Rides bottomed out in the wee hours of the morning before rising sharply between 5 AM and 8 AM. Volume remained relatively steady throughout the day and increased again for evening rush hour after 5 PM. But there was a dip between 3 PM and 4 PM---why?

To answer that question, we would need to dig deeper to analyze data that spanned several days or even several months to see whether our data from June 1, 2016, is typical. We could use the date_part() function to compare trip volume on weekdays versus weekends by extracting the day of the week. To be even more ambitious, we could check weather reports and compare trips on rainy days versus sunny days. There are many different ways to slice a data set to derive conclusions.

** When Do Trips Take the Longest?


Let's investigate another interesting question: at which hour did taxi trips take the longest? One way to find an answer is to calculate the median trip time for each hour. The median is the middle value in an ordered set of values; it's often more accurate than an average for making comparisons because a few very small or very large values in the set won't skew the results as they would with the average.

In [[file:ch05.xhtml#ch05][Chapter 5]], we used the percentile_cont() function to find medians. We use it again in [[file:ch11.xhtml#ch11list10][Listing 11-10]] to calculate median trip times:

SELECT
  ➊ date_part('hour', tpep_pickup_datetime) AS trip_hour,
  ➋ percentile_cont(.5)
      ➌ WITHIN GROUP (ORDER BY
            tpep_dropoff_datetime - tpep_pickup_datetime) AS median_trip
FROM nyc_yellow_taxi_trips_2016_06_01
GROUP BY trip_hour
ORDER BY trip_hour;

/Listing 11-10: Calculating median trip time by hour/

We're aggregating data by the hour portion of the timestamp column tpep_pickup_datetime again, which we extract using date_part() ➊. For the input to the percentile_cont() function ➋, we subtract the pickup time from the drop-off time in the WITHIN GROUP clause ➌. The results show that the 1 PM hour has the highest median trip time of 15 minutes:

date_part    median_trip
---------    -----------
        0    00:10:04
        1    00:09:27
        2    00:08:59
        3    00:09:57
        4    00:10:06
        5    00:07:37
        6    00:07:54
        7    00:10:23
        8    00:12:28
        9    00:13:11
       10    00:13:46
       11    00:14:20
       12    00:14:49
       13    00:15:00
       14    00:14:35
       15    00:14:43
       16    00:14:42
       17    00:14:15
       18    00:13:19
       19    00:12:25
       20    00:11:46
       21    00:11:54
       22    00:11:37
       23    00:11:14

As we would expect, trip times are shortest in the early morning hours. This result makes sense because less traffic in the early morning means passengers are more likely to get to their destinations faster.

Now that we've explored ways to extract portions of the timestamp for analysis, let's dig deeper into analysis that involves intervals.

 /Finding Patterns in Amtrak Data/


Amtrak, the nationwide rail service in America, offers several packaged trips across the United States. The All American, for example, is a train that departs from Chicago and stops in New York, New Orleans, Los Angeles, San Francisco, and Denver before returning to Chicago. Using data from the Amtrak website (/[[http://www.amtrak.com/]]/), we'll build a table that shows information for each segment of the trip. The trip spans four time zones, so we'll need to track the time zones each time we enter an arrival or departure time. Then we'll calculate the duration of the journey at each segment and figure out the length of the entire trip.

** Calculating the Duration of Train Trips


Let's create a table that divides The All American train route into six segments. [[file:ch11.xhtml#ch11list11][Listing 11-11]] contains SQL to create and fill a table with the departure and arrival time for each leg of the journey:

SET timezone TO 'US/Central';➊

CREATE TABLE train_rides (
    trip_id bigserial PRIMARY KEY,
    segment varchar(50) NOT NULL,
    departure timestamp with time zone NOT NULL,➋
    arrival timestamp with time zone NOT NULL
);

INSERT INTO train_rides (segment, departure, arrival)➌
VALUES
    ('Chicago to New York', '2017-11-13 21:30 CST', '2017-11-14 18:23 EST'),
    ('New York to New Orleans', '2017-11-15 14:15 EST', '2017-11-16 19:32 CST'),
    ('New Orleans to Los Angeles', '2017-11-17 13:45 CST', '2017-11-18 9:00 PST'),
    ('Los Angeles to San Francisco', '2017-11-19 10:10 PST', '2017-11-19 21:24 PST'),
    ('San Francisco to Denver', '2017-11-20 9:10 PST', '2017-11-21 18:38 MST'),
    ('Denver to Chicago', '2017-11-22 19:10 MST', '2017-11-23 14:50 CST');

SELECT * FROM train_rides;

/Listing 11-11: Creating a table to hold train trip data/

First, we set the session to the Central time zone, the value for Chicago, using the US/Central designator ➊. We'll use Central time as our reference when viewing the timestamps of the data we enter so that regardless of your and my machine's default time zones, we'll share the same view of the data.

Next, we use the standard CREATE TABLE statement. Note that columns for departures and arrival times are set to timestamp with time zone ➋. Finally, we insert rows that represent the six legs of the trip ➌. Each timestamp input reflects the time zone of the departure and arrival city. Specifying the city's time zone is the key to getting an accurate calculation of trip duration and accounting for time zone changes. It also accounts for annual changes to and from daylight saving time if they were to occur during the time span you're examining.

The final SELECT statement should return the contents of the table like this:

[[../images/prog_page_187.jpg]]

All timestamps should now carry a UTC offset of -06, which is equivalent to the Central time zone in the United States during the month of November, after the nation had switched to standard time. Regardless of the time zone we supplied on insert, our view of the data is now in Central time, and the times are adjusted accordingly if they're in another time zone.

Now that we've created segments corresponding to each leg of the trip, we'll use [[file:ch11.xhtml#ch11list12][Listing 11-12]] to calculate the duration of each segment:

SELECT segment,
     ➊ to_char(departure, 'YYYY-MM-DD HH12:MI a.m. TZ') AS departure,
     ➋ arrival - departure AS segment_time
FROM train_rides;

/Listing 11-12: Calculating the length of each trip segment/

This query lists the trip segment, the departure time, and the duration of the segment journey. Before we look at the calculation, notice the additional code around the departure column ➊. These are PostgreSQL-specific formatting functions that specify how to format different components of the timestamp. In this case, the to_char() function turns the departure timestamp column into a string of characters formatted as YYYY-MM-DD HH12:MI a.m. TZ. The YYYY-MM-DD portion specifies the ISO format for the date, and the HH12:MI a.m. portion presents the time in hours and minutes. The HH12 portion specifies the use of a 12-hour clock rather than 24-hour military time. The a.m. portion specifies that we want to show morning or night times using lowercase characters separated by periods, and the TZ portion denotes the time zone.

For a complete list of formatting functions, check out the PostgreSQL documentation at /[[https://www.postgresql.org/docs/current/static/functions-formatting.html]]/.

Last, we subtract departure from arrival to determine the segment_time ➋. When you run the query, the output should look like this:

[[../images/prog_page_187a.jpg]]

Subtracting one timestamp from another produces an interval data type, which was introduced in [[file:ch03.xhtml#ch03][Chapter 3]]. As long as the value is less than 24 hours, PostgreSQL presents the interval in the HH:MM:SS format. For values greater than 24 hours, it returns the format 1 day 08:28:00, as shown in the San Francisco to Denver segment.

In each calculation, PostgreSQL accounts for the changes in time zones so we don't inadvertently add or lose hours when subtracting. If we used a timestamp without time zone data type, we would end up with an incorrect trip length if a segment spanned multiple time zones.

** Calculating Cumulative Trip Time


As it turns out, San Francisco to Denver is the longest leg of the All American train trip. But how long does the entire trip take? To answer this question, we'll revisit window functions, which you learned about in [[file:ch10.xhtml#lev164][“Ranking with rank() and dense_rank()”]] on [[file:ch10.xhtml#page_164][page 164]].

Our prior query produced an interval, which we labeled segment_time. It would seem like the natural next step would be to write a query to add those values, creating a cumulative interval after each segment. And indeed, we can use sum() as a window function, combined with the OVER clause mentioned in [[file:ch10.xhtml#ch10][Chapter 10]], to create running totals. But when we do, the resulting values are odd. To see what I mean, run the code in [[file:ch11.xhtml#ch11list13][Listing 11-13]]:

SELECT segment,
       arrival - departure AS segment_time,
       sum(arrival - departure) OVER (ORDER BY trip_id) AS cume_time
FROM train_rides;

/Listing 11-13: Calculating cumulative intervals using OVER/

In the third column, we sum the intervals generated when we subtract departure from arrival. The resulting running total in the cume_time column is accurate but formatted in an unhelpful way:

segment                         segment_time      cume_time
----------------------------    --------------    ---------------
Chicago to New York             19:53:00          19:53:00
New York to New Orleans         1 day 06:17:00    1 day 26:10:00
New Orleans to Los Angeles      21:15:00          1 day 47:25:00
Los Angeles to San Francisco    11:14:00          1 day 58:39:00
San Francisco to Denver         1 day 08:28:00    2 days 67:07:00
Denver to Chicago               18:40:00          2 days 85:47:00

PostgreSQL creates one sum for the day portion of the interval and another for the hours and minutes. So, instead of a more understandable cumulative time of 5 days 13:47:00, the database reports 2 days 85:47:00. Both results amount to the same length of time, but 2 days 85:47:00 is harder to decipher. This is an unfortunate limitation of summing the database intervals using this syntax.

As a workaround, we'll use the code in [[file:ch11.xhtml#ch11list14][Listing 11-14]]:

SELECT segment,
       arrival - departure AS segment_time,
       sum(date_part➊('epoch', (arrival - departure)))
           OVER (ORDER BY trip_id) * interval '1 second'➋ AS cume_time
FROM train_rides;

/Listing 11-14: Better formatting for cumulative trip time/

Recall from earlier in this chapter that epoch is the number of seconds that have elapsed since midnight on January 1, 1970, which makes it useful for calculating duration. In [[file:ch11.xhtml#ch11list14][Listing 11-14]], we use date_part() ➊ with the epoch setting to extract the number of seconds elapsed between the arrival and departure intervals. Then we multiply each sum with an interval of 1 second ➋ to convert those seconds to an interval value. The output is clearer using this method:

segment                         segment_time      cume_time
----------------------------    --------------    ---------
Chicago to New York             19:53:00          19:53:00
New York to New Orleans         1 day 06:17:00    50:10:00
New Orleans to Los Angeles      21:15:00          71:25:00
Los Angeles to San Francisco    11:14:00          82:39:00
San Francisco to Denver         1 day 08:28:00    115:07:00
Denver to Chicago               18:40:00          133:47:00

The final cume_time, now in HH:MM:SS format, adds all the segments to return the total trip length of 133 hours and 47 minutes. That's a long time to spend on a train, but I'm sure the scenery is well worth the ride.

** Wrapping Up


Handling times and dates in SQL databases adds an intriguing dimension to your analysis, letting you answer questions about when an event occurred along with other temporal concerns in your data. With a solid grasp of time and date formats, time zones, and functions to dissect the components of a timestamp, you can analyze just about any data set you come across.

Next, we'll look at advanced query techniques that help answer more complex questions.


*TRY IT YOURSELF*

Try these exercises to test your skills on dates and times.

1. Using the New York City taxi data, calculate the length of each ride using the pickup and drop-off timestamps. Sort the query results from the longest ride to the shortest. Do you notice anything about the longest or shortest trips that you might want to ask city officials about?

2. Using the AT TIME ZONE keywords, write a query that displays the date and time for London, Johannesburg, Moscow, and Melbourne the moment January 1, 2100, arrives in New York City.

3. As a bonus challenge, use the statistics functions in [[file:ch10.xhtml#ch10][Chapter 10]] to calculate the correlation coefficient and /r/-squared values using trip time and the total_amount column in the New York City taxi data, which represents the total amount charged to passengers. Do the same with the trip_distance and total_amount columns. Limit the query to rides that last three hours or less.


1:15:00          71:25:00\\
Los Angeles to San Francisco    11:14:00          82:39:00\\
San Francisco to Denver         1 day 08:28:00    115:07:00\\
Denver to Chicago               18:40:00          133:47:00

The final cume\_time, now in HH:MM:SS format, adds all the segments to return the total trip length of 133 hours and 47 minutes. That's a long time to spend on a train, but I'm sure the scenery is well worth the ride.

**** Wrapping Up
    :PROPERTIES:
    :CUSTOM_ID: lev184
    :CLASS: h3
    :END:

Handling times and dates in SQL databases adds an intriguing dimension to your analysis, letting you answer questions about when an event occurred along with other temporal concerns in your data. With a solid grasp of time and date formats, time zones, and functions to dissect the components of a timestamp, you can analyze just about any data set you come across.

Next, we'll look at advanced query techniques that help answer more complex questions.

<<ch11sb1>>
*TRY IT YOURSELF*

Try these exercises to test your skills on dates and times.

1. Using the New York City taxi data, calculate the length of each ride using the pickup and drop-off timestamps. Sort the query results from the longest ride to the shortest. Do you notice anything about the longest or shortest trips that you might want to ask city officials about?

2. Using the AT TIME ZONE keywords, write a query that displays the date and time for London, Johannesburg, Moscow, and Melbourne the moment January 1, 2100, arrives in New York City.

3. As a bonus challenge, use the statistics functions in [[file:ch10.xhtml#ch10][Chapter 10]] to calculate the correlation coefficient and /r/-squared values using trip time and the total\_amount column in the New York City taxi data, which represents the total amount charged to passengers. Do the same with the trip\_distance and total\_amount columns. Limit the query to rides that last three hours or less.


