* 13 Mining Text To Find Meaningful Data


[[../images/common01.jpg]]

Although it might not be obvious at first glance, you can extract data and even quantify data from text in speeches, reports, press releases, and other documents. Even though most text exists as /unstructured/ or /semi-structured data/, which is not organized in rows and columns, as in a table, you can use SQL to derive meaning from it.

One way to do this is to transform the text into /structured data/. You search for and extract elements such as dates or codes from the text, load them into a table, and analyze them. Another way to find meaning from textual data is to use advanced text analysis features, such as PostgreSQL's full text search. Using these techniques, ordinary text can reveal facts or trends that might otherwise remain hidden.

In this chapter, you'll learn how to use SQL to analyze and transform text. You'll start with simple text wrangling using string formatting and pattern matching before moving on to more advanced analysis functions. We'll use two data sets as examples: a small collection of crime reports from a sheriff's department near Washington, D.C., and a set of State of the Union addresses delivered by former U.S. presidents.

** Formatting Text Using String Functions


Whether you're looking for data in text or simply want to change how it looks in a report, you first need to transform it into a format you can use. PostgreSQL has more than 50 built-in string functions that handle routine but necessary tasks, such as capitalizing letters, combining strings, and removing unwanted spaces. Some are part of the ANSI SQL standard, and others are specific to PostgreSQL. You'll find a complete list of string functions at /[[https://www.postgresql.org/docs/current/static/functions-string.html]]/, but in this section we'll examine several that you'll likely use most often.

You can use these functions inside a variety of queries. Let's try one now using a simple query that places a function after SELECT and runs it in the pgAdmin Query Tool, like this: SELECT upper('hello');. Examples of each function plus code for all the listings in this chapter are available at /[[https://www.nostarch.com/practicalSQL/]]/.

 /Case Formatting/


The capitalization functions format the text's case. The upper(string) function capitalizes all alphabetical characters of a string passed to it. Nonalphabet characters, such as numbers, remain unchanged. For example, upper('Neal7') returns NEAL7. The lower(string) function lowercases all alphabetical characters while keeping nonalphabet characters unchanged. For example, lower('Randy') returns randy.

The initcap(string) function capitalizes the first letter of each word. For example, initcap('at the end of the day') returns At The End Of The Day. This function is handy for formatting titles of books or movies, but because it doesn't recognize acronyms, it's not always the perfect solution. For example, initcap('Practical SQL') would return Practical Sql, because it doesn't recognize SQL as an acronym.

The upper() and lower() functions are ANSI SQL standard commands, but initcap() is PostgreSQL-specific. These three functions give you enough options to rework a column of text into the case you prefer. Note that capitalization does not work with all locales or languages.

 /Character Information/


Several functions return data about the string rather than transforming it. These functions are helpful on their own or combined with other functions. For example, the char_length(string) function returns the number of characters in a string, including any spaces. For example, char_length(' Pat ') returns a value of 5, because the three letters in Pat and the spaces on either end total five characters. You can also use the non-ANSI SQL function length(string) to count strings, which has a variant that lets you count the length of binary strings.

*NOTE*

/The length() function can return a different value than char_length() when used with multibyte encodings, such as character sets covering the Chinese, Japanese, or Korean languages./

The position(substring in string) function returns the location of the substring characters in the string. For example, position(', ' in 'Tan, Bella') returns 4, because the comma and space characters (, ) specified in the substring passed as the first parameter start at the fourth index position in the main string Tan, Bella.

Both char_length() and position() are in the ANSI SQL standard.

 /Removing Characters/


The trim(characters from string) function removes unwanted characters from strings. To declare one or more characters to remove, add them to the function followed by the keyword from and the main string you want to change. Options to remove leading characters (at the front of the string), trailing characters (at the end of the string), or both make this function super flexible.

For example, trim('s' from 'socks') removes all s characters and returns ock. To remove only the s at the end of the string, add the trailing keyword before the character to trim: trim(trailing 's' from 'socks') returns sock.

If you don't specify any characters to remove, trim() removes any spaces in the string by default. For example, trim(' Pat ') returns Pat without the leading or trailing spaces. To confirm the length of the trimmed string, we can nest trim() inside char_length() like this:

SELECT char_length(trim(' Pat '));

This query should return 3, the number of letters in Pat, which is the result of trim(' Pat ').

The ltrim(string, characters) and rtrim(string, characters) functions are PostgreSQL-specific variations of the trim() function. They remove characters from the left or right ends of a string. For example, rtrim('socks', 's') returns sock by removing only the s on the right end of the string.

 /Extracting and Replacing Characters/


The left(string, number) and right(string, number) functions, both ANSI SQL standard, extract and return selected characters from a string. For example, to get just the 703 area code from the phone number 703-555-1212, use left('703-555-1212', 3) to specify that you want the first three characters of the string starting from the left. Likewise, right('703-555-1212', 8) returns eight characters from the right: 555-1212.

To substitute characters in a string, use the replace(string, from, to) function. To change bat to cat, for example, you would use replace('bat', 'b', 'c') to specify that you want to replace the b in bat with a c.

Now that you know basic functions for manipulating strings, let's look at how to match more complex patterns in text and turn those patterns into data we can analyze.

** Matching Text Patterns with Regular Expressions


/Regular expressions/ (or /regex/) are a type of notational language that describes text patterns. If you have a string with a noticeable pattern (say, four digits followed by a hyphen and then two more digits), you can write a regular expression that describes the pattern. You can then use the notation in a WHERE clause to filter rows by the pattern or use regular expression functions to extract and wrangle text that contains the same pattern.

Regular expressions can seem inscrutable to beginning programmers; they take practice to comprehend because they use single-character symbols that aren't intuitive. Getting an expression to match a pattern can involve trial and error, and each programming language has subtle differences in the way it handles regular expressions. Still, learning regular expressions is a good investment because you gain superpower-like abilities to search text using many programming languages, text editors, and other applications.

In this section, I'll provide enough regular expression basics to work through the exercises. To learn more, I recommend interactive online code testers, such as /[[https://regexr.com/]]/ or /[[http://www.regexpal.com/]]/, which have notation references.

 /Regular Expression Notation/


Matching letters and numbers using regular expression notation is straightforward because letters and numbers (and certain symbols) are literals that indicate the same characters. For example, Al matches the first two characters in Alicia.

For more complex patterns, you'll use combinations of the regular expression elements in [[file:ch13.xhtml#ch13tab1][Table 13-1]].

*Table 13-1:* Regular Expression Notation Basics

*Expression*

*Description*

.

A dot is a wildcard that finds any character except a newline.

[FGz]

Any character in the square brackets. Here, F, G, or z.

[a-z]

A range of characters. Here, lowercase a to z.

[^a-z]

The caret negates the match. Here, not lowercase a to z.

w

Any word character or underscore. Same as [A-Za-z0-9_].

d

Any digit.

s

A space.

t

Tab character.

n

Newline character.

r

Carriage return character.

^

Match at the start of a string.

$

Match at the end of a string.

?

Get the preceding match zero or one time.

*

Get the preceding match zero or more times.

+

Get the preceding match one or more times.

{/m/}

Get the preceding match exactly /m/ times.

{/m/,/n/}

Get the preceding match between /m/ and /n/ times.

/a/|/b/

The pipe denotes alternation. Find either /a/ or /b/.

( )

Create and report a capture group or set precedence.

(?: )

Negate the reporting of a capture group.

Using these basic regular expressions, you can match various kinds of characters and also indicate how many times and where to match them. For example, placing characters inside square brackets ([]) lets you match any single character or a range. So, [FGz] matches a single F, G, or z, whereas [A-Za-z] will match any uppercase or lowercase letter.

The backslash () precedes a designator for special characters, such as a tab (t), digit (d), or newline (n), which is a line ending character in text files.

There are several ways to indicate how many times to match a character. Placing a number inside curly brackets indicates you want to match it that many times. For example, d{4} matches four digits in a row, and d{1,4} matches a digit between one and four times.

The ?, *, and + characters provide a useful shorthand notation for the number of matches. For example, the plus sign (+) after a character indicates to match it one or more times. So, the expression a+ would find the aa characters in the string aardvark.

Additionally, parentheses indicate a /capture group/, which you can use to specify just a portion of the matched text to display in the query results. This is useful for reporting back just a part of a matched expression. For example, if you were hunting for an HH:MM:SS time format in text and wanted to report only the hour, you could use an expression such as (d{2}):d{2}:d{2}. This looks for two digits (d{2}) of the hour followed by a colon, another two digits for the minutes and a colon, and then the two-digit seconds. By placing the first d{2} inside parentheses, you can extract only those two digits, even though the entire expression matches the full time.

[[file:ch13.xhtml#ch13tab2][Table 13-2]] shows examples of combining regular expressions to capture different portions of the sentence “The game starts at 7 p.m. on May 2, 2019.”

*Table 13-2:* Regular Expression Matching Examples

*Expression*

*What it matches*

*Result*

.+

Any character one or more times

The game starts at 7 p.m. on May 2, 2019.

d{1,2} (?:a.m.|p.m.)

One or two digits followed by a space and /a.m./ or /p.m./ in a noncapture group

7 p.m.

^w+

One or more word characters at the start

The

w+.$

One or more word characters followed by any character at the end

2019.

May|June

Either of the words /May/ or /June/

May

d{4}

Four digits

2019

May d, d{4}

/May/ followed by a space, digit, comma, space, and four digits

May 2, 2019

These results show the usefulness of regular expressions for selecting only the parts of the string that interest us. For example, to find the time, we use the expression d{1,2} (?:a.m.|p.m.) to look for either one or two digits because the time could be a single or double digit followed by a space. Then we look for either a.m. or p.m.; the pipe symbol separating the terms indicates the either-or condition, and placing them in parentheses separates the logic from the rest of the expression. We need the ?: symbol to indicate that we don't want to treat the terms inside the parentheses as a capture group, which would report a.m. or p.m. only. The ?: ensures that the full match will be returned.

You can use any of these regular expressions in pgAdmin by placing the text and regular expression inside the substring(string from pattern) function to return the matched text. For example, to find the four-digit year, use the following query:

SELECT substring('The game starts at 7 p.m. on May 2, 2019.' from 'd{4}');

This query should return 2019, because we specified that the pattern should look for any digit that is four characters long, and 2019 is the only digit in this string that matches these criteria. You can check out sample substring() queries for all the examples in [[file:ch13.xhtml#ch13tab2][Table 13-2]] in the book's code resources at /[[https://www.nostarch.com/practicalSQL/]]/.

The lesson here is that if you can identify a pattern in the text, you can use a combination of regular expression symbols to locate it. This technique is particularly useful when you have repeating patterns in text that you want to turn into a set of data to analyze. Let's practice how to use regular expression functions using a real-world example.

 /Turning Text to Data with Regular Expression Functions/


A sheriff's department in one of the Washington, D.C., suburbs publishes daily reports that detail the date, time, location, and description of incidents the department investigates. These reports would be great to analyze, except they post the information in Microsoft Word documents saved as PDF files, which is not the friendliest format for importing into a database.

If I copy and paste incidents from the PDF into a text editor, the result is blocks of text that look something like [[file:ch13.xhtml#ch13list1][Listing 13-1]]:

➊ 4/16/17-4/17/17
➋ 2100-0900 hrs.
➌ 46000 Block Ashmere Sq.
➍ Sterling
➎ Larceny: ➏The victim reported that a
  bicycle was stolen from their opened
  garage door during the overnight hours.
➐ C0170006614

  04/10/17
  1605 hrs.
  21800 block Newlin Mill Rd.
  Middleburg
  Larceny: A license plate was reported
  stolen from a vehicle.
  SO170006250

/Listing 13-1: Crime reports text/

Each block of text includes dates ➊, times ➋, a street address ➌, city or town ➍, the type of crime ➎, and a description of the incident ➏. The last piece of information is a code ➐ that might be a unique ID for the incident, although we'd have to check with the sheriff's department to be sure. There are slight inconsistencies. For example, the first block of text has two dates (4/16/17-4/17/17) and two times (2100-0900 hrs.), meaning the exact time of the incident is unknown and likely occurred within that time span. The second block has one date and time.

If you compile these reports regularly, you can expect to find some good insights that could answer important questions: Where do crimes tend to occur? Which crime types occur most frequently? Do they happen more often on weekends or weekdays? Before you can start answering these questions, you'll need to extract the text into table columns using regular expressions.

** Creating a Table for Crime Reports


I've collected five of the crime incidents into a file named /crime_reports.csv/ that you can download at /[[https://www.nostarch.com/practicalSQL/]]/. Download the file and save it on your computer. Then use the code in [[file:ch13.xhtml#ch13list2][Listing 13-2]] to build a table that has a column for each data element you can parse from the text using a regular expression.

CREATE TABLE crime_reports (
    crime_id bigserial PRIMARY KEY,
    date_1 timestamp with time zone,
    date_2 timestamp with time zone,
    street varchar(250),
    city varchar(100),
    crime_type varchar(100),
    description text,
    case_number varchar(50),
    original_text text NOT NULL
);

COPY crime_reports (original_text)
FROM '/C:YourDirectory/crime_reports.csv'
WITH (FORMAT CSV, HEADER OFF, QUOTE '"');

/Listing 13-2: Creating and loading the crime_reports table/

Run the CREATE TABLE statement in [[file:ch13.xhtml#ch13list2][Listing 13-2]], and then use COPY to load the text into the column original_text. The rest of the columns will be NULL until we fill them.

When you run SELECT original_text FROM crime_reports; in pgAdmin, the results grid should display five rows and the first several words of each report. When you hover your cursor over any cell, pgAdmin shows all the text in that row, as shown in [[file:ch13.xhtml#ch13fig1][Figure 13-1]].

[[../images/f0218-01.jpg]]

/Figure 13-1: Displaying additional text in the pgAdmin results grid/

Now that you've loaded the text you'll be parsing, let's explore this data using PostgreSQL regular expression functions.

** Matching Crime Report Date Patterns


The first piece of data we want to extract from the report original_text is the date or dates of the crime. Most of the reports have one date, although one has two. The reports also have associated times, and we'll combine the extracted date and time into a timestamp. We'll fill date_1 with the first (or only) date and time in each report. In cases where a second date or second time exists, we'll create a timestamp and add it to date_2.

For extracting data, we'll use the regexp_match(string, pattern) function, which is similar to substring() with a few exceptions. One is that it returns each match as text in an array. Also, if there are no matches, it returns NULL. As you might recall from [[file:ch05.xhtml#ch05][Chapter 5]], arrays are a list of elements; in one exercise, you used an array to pass a list of values into the percentile_cont() function to calculate quartiles. I'll show you how to work with results that come back as an array when we parse the crime reports.

*NOTE*

/The regexp_match() function was introduced in PostgreSQL 10 and is not available in earlier versions./

To start, let's use regexp_match() to find dates in each of the five incidents in crime_reports. The general pattern to match is MM/DD/YY, although there may be one or two digits for both the month and date. Here's a regular expression that matches the pattern:

d{1,2}/d{1,2}/d{2}

In this expression, d{1,2} indicates the month. The numbers inside the curly brackets specify that you want at least one digit and at most two digits. Next, you want to look for a forward slash (/), but because a forward slash can have special meaning in regular expressions, you must /escape/ that character by placing a backslash () in front of it, like this /. Escaping a character in this context simply means we want to treat it as a literal rather than letting it take on special meaning. So, the combination of the backslash and forward slash (/) indicates you want a forward slash.

Another d{1,2} follows for a single- or double-digit day of the month. The expression ends with a second escaped forward slash and d{2} to indicate the two-digit year. Let's pass the expression d{1,2}/d{1,2}/d{2} to regexp_match(), as shown in [[file:ch13.xhtml#ch13list3][Listing 13-3]]:

SELECT crime_id,
       regexp_match(original_text, 'd{1,2}/d{1,2}/d{2}')
FROM crime_reports;

/Listing 13-3: Using regexp_match() to find the first date/

Run that code in pgAdmin, and the results should look like this:

crime_id    regexp_match
--------    ------------
       1    {4/16/17}
       2    {4/8/17}
       3    {4/4/17}
       4    {04/10/17}
       5    {04/09/17}

Note that each row shows the first date listed for the incident, because regexp_match() returns the first match it finds by default. Also note that each date is enclosed in curly brackets. That's PostgreSQL indicating that regexp_match() returns each result in an array, or list of elements. In “Extracting Text from the regexp_match() Result” on [[file:ch13.xhtml#page_224][page 224]], I'll show you how to access those elements from the array. You can also read more about using arrays in PostgreSQL at /[[https://www.postgresql.org/docs/current/static/arrays.html]]/.

** Matching the Second Date When Present


We've successfully extracted the first date from each report. But recall that one of the five incidents has a second date. To find and display all the dates in the text, you must use the related regexp_matches() function and pass in an option in the form of the flag g, as shown in [[file:ch13.xhtml#ch13list4][Listing 13-4]].

SELECT crime_id,
       regexp_matches(original_text, 'd{1,2}/d{1,2}/d{2}', 'g'➊)
FROM crime_reports;

/Listing 13-4: Using the regexp_matches() function with the 'g' flag/

The regexp_matches() function, when supplied the g flag ➊, differs from regexp_match() by returning each match the expression finds as a row in the results rather than returning just the first match.

Run the code again with this revision; you should now see two dates for the incident that has a crime_id of 1, like this:

crime_id    regexp_matches
--------    --------------
       1    {4/16/17}
       1    {4/17/17}
       2    {4/8/17}
       3    {4/4/17}
       4    {04/10/17}
       5    {04/09/17}

Any time a crime report has a second date, we want to load it and the associated time into the date_2 column. Although adding the g flag shows us all the dates, to extract just the second date in a report, we can use the pattern we always see when two dates exist. In [[file:ch13.xhtml#ch13list1][Listing 13-1]], the first block of text showed the two dates separated by a hyphen, like this:

4/16/17-4/17/17

This means you can switch back to regexp_match() and write a regular expression to look for a hyphen followed by a date, as shown in [[file:ch13.xhtml#ch13list5][Listing 13-5]].

SELECT crime_id,
       regexp_match(original_text, '-d{1,2}/d{1,2}/d{2}')
FROM crime_reports;

/Listing 13-5: Using regexp_match() to find the second date/

Although this query finds the second date in the first item (and returns a NULL for the rest), there's an unintended consequence: it displays the hyphen along with it.

crime_id    regexp_match
--------    ------------
       1    {-4/17/17}
       2    
       3    
       4
       5

You don't want to include the hyphen, because it's an invalid format for the timestamp data type. Fortunately, you can specify the exact part of the regular expression you want to return by placing parentheses around it to create a capture group, like this:

-(d{1,2}/d{1,2}/d{1,2})

This notation returns only the part of the regular expression you want. Run the modified query in [[file:ch13.xhtml#ch13list6][Listing 13-6]] to report only the data in parentheses.

SELECT crime_id,
       regexp_match(original_text, '-(d{1,2}/d{1,2}/d{1,2})')
FROM crime_reports;

/Listing 13-6: Using a capture group to return only the date/

The query in [[file:ch13.xhtml#ch13list6][Listing 13-6]] should return just the second date without the leading hyphen, as shown here:

crime_id    regexp_match
--------    ------------
       1    {4/17/17}
       2
       3
       4
       5

The process you've just completed is typical. You start with text to analyze, and then write and refine the regular expression until it finds the data you want. So far, we've created regular expressions to match the first date and a second date, if it exists. Now, let's use regular expressions to extract additional data elements.

** Matching Additional Crime Report Elements


In this section, we'll capture times, addresses, crime type, description, and case number from the crime reports. Here are the expressions for capturing this information:

*First hour /d{2}n(d{4})*

The first hour, which is the hour the crime was committed or the start of the time range, always follows the date in each crime report, like this:

4/16/17-4/17/17
2100-0900 hrs.

To find the first hour, we start with an escaped forward slash and d{2}, which represents the two-digit year preceding the first date (17). The n character indicates the newline because the hour always starts on a new line, and d{4} represents the four-digit hour (2100). Because we just want to return the four digits, we put d{4} inside parentheses as a capture group.

*Second hour /d{2}nd{4}-(d{4})*

If the second hour exists, it will follow a hyphen, so we add a hyphen and another d{4} to the expression we just created for the first hour. Again, the second d{4} goes inside a capture group, because 0900 is the only hour we want to return.

*Street hrs.n(d+ .+(?:Sq.|Plz.|Dr.|Ter.|Rd.))*

In this data, the street always follows the time's hrs. designation and a newline (n), like this:

04/10/17
1605 hrs.
21800 block Newlin Mill Rd.

The street address always starts with some number that varies in length and ends with an abbreviated suffix of some kind. To describe this pattern, we use d+ to match any digit that appears one or more times. Then we specify a space and look for any character one or more times using the dot wildcard and plus sign (.+) notation. The expression ends with a series of terms separated by the alternation pipe symbol that looks like this: (?:Sq.|Plz.|Dr.|Ter.|Rd.). The terms are inside parentheses, so the expression will match one or another of those terms. When we group terms like this, if we don't want the parentheses to act as a capture group, we need to add ?: to negate that effect.

*NOTE*

/In a large data set, it's likely roadway names would end with suffixes beyond the five in our regular expression. After making an initial pass at extracting the street, you can run a query to check for unmatched rows to find additional suffixes to match./

*City (?:Sq.|Plz.|Dr.|Ter.|Rd.)n(w+ w+|w+)n*

Because the city always follows the street suffix, we reuse the terms separated by the alternation symbol we just created for the street. We follow that with a newline (n) and then use a capture group to look for two words or one word (w+ w+|w+) before a final newline, because a town or city name can be more than a single word.

*Crime type n(?:w+ w+|w+)n(.*):*

The type of crime always precedes a colon (the only time a colon is used in each report) and might consist of one or more words, like this:

/--snip--/
Middleburg
Larceny: A license plate was reported
stolen from a vehicle.
SO170006250
/--snip--/

To create an expression that matches this pattern, we follow a newline with a nonreporting capture group that looks for the one- or two-word city. Then we add another newline and match any character that occurs zero or more times before a colon using (.*):.

*Description :s(.+)(?:C0|SO)*

The crime description always comes between the colon after the crime type and the case number. The expression starts with the colon, a space character (s), and then a capture group to find any character that appears one or more times using the .+ notation. The nonreporting capture group (?:C0|SO) tells the program to stop looking when it encounters either C0 or SO, the two character pairs that start each case number (a C followed by a zero, and an S followed by a capital O). We have to do this because the description might have one or more line breaks.

*Case number (?:C0|SO)[0-9]+*

The case number starts with either C0 or SO, followed by a set of digits. To match this pattern, the expression looks for either C0 or SO in a nonreporting capture group followed by any digit from 0 to 9 that occurs one or more times using the [0-9] range notation.

Now let's pass these regular expressions to regexp_match() to see them in action. [[file:ch13.xhtml#ch13list7][Listing 13-7]] shows a sample regexp_match() query that retrieves the case number, first date, crime type, and city:

SELECT
    regexp_match(original_text, '(?:C0|SO)[0-9]+') AS case_number,
    regexp_match(original_text, 'd{1,2}/d{1,2}/d{2}') AS date_1,
    regexp_match(original_text, 'n(?:w+ w+|w+)n(.*):') AS crime_type,
    regexp_match(original_text, '(?:Sq.|Plz.|Dr.|Ter.|Rd.)n(w+ w+|w+)n')
        AS city
FROM crime_reports;

/Listing 13-7: Matching case number, date, crime type, and city/

Run the code, and the results should look like this:

[[../images/prog_page_224.jpg]]

After all that wrangling, we've transformed the text into a structure that is more suitable for analysis. Of course, you would have to include many more incidents to count the frequency of crime type by city or the number of crimes per month to identify any trends.

To load each parsed element into the table's columns, we'll create an UPDATE query. But before you can insert the text into a column, you'll need to learn how to extract the text from the array that regexp_match() returns.

** Extracting Text from the regexp_match() Result


In [[file:ch13.xhtml#ch13lev3sec1][“Matching Crime Report Date Patterns”]] on [[file:ch13.xhtml#page_218][page 218]], I mentioned that regexp_match() returns an array containing text values. Two clues reveal that these are text values. The first is that the data type designation in the column header shows text[] instead of text. The second is that each result is surrounded by curly brackets. [[file:ch13.xhtml#ch13fig2][Figure 13-2]] shows how pgAdmin displays the results of the query in [[file:ch13.xhtml#ch13list7][Listing 13-7]].

[[../images/f0224-01.jpg]]

/Figure 13-2: Array values in the pgAdmin results grid/

The crime_reports columns we want to update are not array types, so rather than passing in the array values returned by regexp_match(), we need to extract the values from the array first. We do this by using array notation, as shown in [[file:ch13.xhtml#ch13list8][Listing 13-8]].

SELECT
    crime_id,
  ➊ (regexp_match(original_text, '(?:C0|SO)[0-9]+'))[1]➋
        AS case_number
FROM crime_reports;

/Listing 13-8: Retrieving a value from within an array/

First, we wrap the regexp_match() function ➊ in parentheses. Then, at the end, we provide a value of 1, which represents the first element in the array, enclosed in square brackets ➋. The query should produce the following results:

crime_id    case_number
--------    -----------
       1    C0170006614
       2    C0170006162
       3    C0170006079
       4    SO170006250
       5    SO170006211

Now the data type designation in the pgAdmin column header should show text instead of text[], and the values are no longer enclosed in curly brackets. We can now insert these values into crime_reports using an UPDATE query.

** Updating the crime_reports Table with Extracted Data


With each element currently available as text, we can update columns in the crime_reports table with the appropriate data from the original crime report. To start, [[file:ch13.xhtml#ch13list9][Listing 13-9]] combines the extracted first date and time into a single timestamp value for the column date_1.

  UPDATE crime_reports
➊ SET date_1 =
  (
    ➋ (regexp_match(original_text, 'd{1,2}/d{1,2}/d{2}'))[1]
        ➌ || ' ' ||
    ➍ (regexp_match(original_text, '/d{2}n(d{4})'))[1]
        ➎ ||' US/Eastern'
➏ )::timestamptz;

  SELECT crime_id,
         date_1,
         original_text
  FROM crime_reports;

/Listing 13-9: Updating the crime_reports date_1 column/

Because the date_1 column is of type timestamp, we must provide an input in that data type. To do that, we'll use the PostgreSQL double-pipe (||) concatenation operator to combine the extracted date and time in a format that's acceptable for timestamp with time zone input. In the SET clause ➊, we start with the regex pattern that matches the first date ➋. Next, we concatenate the date with a space using two single quotes ➌ and repeat the concatenation operator. This step combines the date with a space before connecting it to the regex pattern that matches the time ➍. Then we include the time zone for the Washington, D.C., area by concatenating that at the end of the string ➎ using the US/Eastern designation. Concatenating these elements creates a string in the pattern of MM/DD/YY HHMM TIMEZONE, which is acceptable as a timestamp input. We cast the string to a timestamp with time zone data type ➏ using the PostgreSQL double-colon shorthand and the timestamptz abbreviation.

When you run the UPDATE portion of the code, PostgreSQL should return the message UPDATE 5. Running the SELECT statement in pgAdmin should show the now-filled date_1 column alongside a portion of the original_text column, like this:

[[../images/prog_page_226.jpg]]

At a glance, you can see that date_1 accurately captures the first date and time that appears in the original text and puts it into a useable format that we can analyze. Note that if you're not in the Eastern time zone, the timestamps will instead reflect your pgAdmin client's time zone. As you learned in [[file:ch11.xhtml#lev175][“Setting the Time Zone”]] on [[file:ch11.xhtml#page_178][page 178]], you can use the command SET timezone TO 'US/Eastern'; to change the client to reflect Eastern time.

** Using CASE to Handle Special Instances


You could write an UPDATE statement for each remaining data element, but combining those statements into one would be more efficient. [[file:ch13.xhtml#ch13list10][Listing 13-10]] updates all the crime_reports columns using a single statement while handling inconsistent values in the data.


UPDATE crime_reports
SET date_1➊ =
    (
      (regexp_match(original_text, 'd{1,2}/d{1,2}/d{2}'))[1]
          || ' ' ||
      (regexp_match(original_text, '/d{2}n(d{4})'))[1]
          ||' US/Eastern'
    )::timestamptz,
            
    date_2➋ =
    CASE➌
        WHEN➍ (SELECT regexp_match(original_text, '-(d{1,2}/d{1,2}/d{1,2})') IS NULL➎)
                AND (SELECT regexp_match(original_text, '/d{2}nd{4}-(d{4})') IS NOT NULL➏)
        THEN➐
          ((regexp_match(original_text, 'd{1,2}/d{1,2}/d{2}'))[1]
              || ' ' ||
          (regexp_match(original_text, '/d{2}nd{4}-(d{4})'))[1]
              ||' US/Eastern'
          )::timestamptz
        WHEN➑ (SELECT regexp_match(original_text, '-(d{1,2}/d{1,2}/d{1,2})') IS NOT NULL)
                AND (SELECT regexp_match(original_text, '/d{2}nd{4}-(d{4})') IS NOT NULL)
        THEN
          ((regexp_match(original_text, '-(d{1,2}/d{1,2}/d{1,2})'))[1]
              || ' ' ||
          (regexp_match(original_text, '/d{2}nd{4}-(d{4})'))[1]
              ||' US/Eastern'
          )::timestamptz

        ELSE NULL➒
    END,
    street = (regexp_match(original_text, 'hrs.n(d+ .+(?:Sq.|Plz.|Dr.|Ter.|Rd.))'))[1],
    city = (regexp_match(original_text,
                           '(?:Sq.|Plz.|Dr.|Ter.|Rd.)n(w+ w+|w+)n'))[1],
    crime_type = (regexp_match(original_text, 'n(?:w+ w+|w+)n(.*):'))[1],
    description = (regexp_match(original_text, ':s(.+)(?:C0|SO)'))[1],
    case_number = (regexp_match(original_text, '(?:C0|SO)[0-9]+'))[1];

/Listing 13-10: Updating all crime_reports columns/

This UPDATE statement might look intimidating, but it's not if we break it down by column. First, we use the same code from [[file:ch13.xhtml#ch13list9][Listing 13-9]] to update the date_1 column ➊. But to update date_2 ➋, we need to account for the inconsistent presence of a second date and time. In our limited data set, there are three possibilities:

1. A second hour exists but not a second date. This occurs when a report covers a range of hours on one date.

2. A second date and second hour exist. This occurs when a report covers more than one date.

3. Neither a second date nor a second hour exists.

To insert the correct value in date_2 for each scenario, we use the CASE statement syntax you learned in [[file:ch12.xhtml#lev216][“Reclassifying Values with CASE”]] on [[file:ch12.xhtml#page_207][page 207]] to test for each possibility. After the CASE keyword ➌, we use a series of WHEN ... THEN statements to check for the first two conditions and provide the value to insert; if neither condition exists, we use an ELSE keyword to provide a NULL.

The first WHEN statement ➍ checks whether regexp_match() returns a NULL ➎ for the second date and a value for the second hour (using IS NOT NULL ➏). If that condition evaluates as true, the THEN statement ➐ concatenates the first date with the second hour to create a timestamp for the update.

The second WHEN statement ➑ checks that regexp_match() returns a value for the second hour and second date. If true, the THEN statement concatenates the second date with the second hour to create a timestamp.

If neither of the two WHEN statements returns true, the ELSE statement ➒ provides a NULL for the update because there is only a first date and first time.

*NOTE*

/The WHEN statements handle the possibilities that exist in our small sample data set. If you are working with more data, you might need to handle additional variations, such as a second date but not a second time./

When we run the full query in [[file:ch13.xhtml#ch13list10][Listing 13-10]], PostgreSQL should report UPDATE 5. Success! Now that we've updated all the columns with the appropriate data while accounting for elements that have additional data, we can examine all the columns of the table and find the parsed elements from original_text. [[file:ch13.xhtml#ch13list11][Listing 13-11]] queries four of the columns:

SELECT date_1,
       street,
       city,
       crime_type
FROM crime_reports;

/Listing 13-11: Viewing selected crime data/

The results of the query should show a nicely organized set of data that looks something like this:

[[../images/prog_page_228.jpg]]

You've successfully transformed raw text into a table that can answer questions and reveal storylines about crime in this area.

** The Value of the Process


Writing regular expressions and coding a query to update a table can take time, but there is value to identifying and collecting data this way. In fact, some of the best data sets you'll encounter are those you build yourself. Everyone can download the same data sets, but the ones you build are yours alone. You get to be first person to find and tell the story behind the data.

Also, after you set up your database and queries, you can use them again and again. In this example, you could collect crime reports every day (either by hand or by automating downloads using a programming language such as Python) for an ongoing data set that you can mine continually for trends.

In the next section, we'll finish our exploration of regular expressions using additional PostgreSQL functions.

 /Using Regular Expressions with WHERE/


You've filtered queries using LIKE and ILIKE in WHERE clauses. In this section, you'll learn to use regular expressions in WHERE clauses so you can perform more complex matches.

We use a tilde (~) to make a case-sensitive match on a regular expression and a tilde-asterisk (~*) to perform a case-insensitive match. You can negate either expression by adding an exclamation point in front. For example, !~* indicates to /not/ match a regular expression that is case-insensitive. [[file:ch13.xhtml#ch13list12][Listing 13-12]] shows how this works using the 2010 Census table us_counties_2010 from previous exercises:

  SELECT geo_name
  FROM us_counties_2010
➊ WHERE geo_name ~* '(.+lade.+|.+lare.+)'
  ORDER BY geo_name;

  SELECT geo_name
  FROM us_counties_2010
➋ WHERE geo_name ~* '.+ash.+' AND geo_name !~ 'Wash.+'
  ORDER BY geo_name;

/Listing 13-12: Using regular expressions in a WHERE clause/

The first WHERE clause ➊ uses the tilde-asterisk (~*) to perform a case-insensitive match on the regular expression (.+lade.+|.+lare.+) to find any county names that contain either the letters lade or lare between other characters. The results should show eight rows:

geo_name
-------------------
Bladen County
Clare County
Clarendon County
Glades County
Langlade County
Philadelphia County
Talladega County
Tulare County

As you can see, the county names include the letters lade or lare between other characters.

The second WHERE clause ➋ uses the tilde-asterisk (~*) as well as a negated tilde (!~) to find county names containing the letters ash but excluding those starting with Wash. This query should return the following:

geo_name
--------------
Nash County
Wabash County
Wabash County
Wabasha County

All four counties in this output have names that contain the letters ash but don't start with Wash.

These are fairly simple examples, but you can do more complex matches using regular expressions that you wouldn't be able to perform with the wildcards available with just LIKE and ILIKE.

 /Additional Regular Expression Functions/


Let's look at three more regular expression functions you might find useful when working with text. [[file:ch13.xhtml#ch13list13][Listing 13-13]] shows several regular expression functions that replace and split text:

➊ SELECT regexp_replace('05/12/2018', 'd{4}', '2017');

➋ SELECT regexp_split_to_table('Four,score,and,seven,years,ago', ',');

➌ SELECT regexp_split_to_array('Phil Mike Tony Steve', ',');

/Listing 13-13: Regular expression functions to replace and split text/

The regexp_replace(string, pattern, replacement text) function lets you substitute a matched pattern with replacement text. In the example at ➊, we're searching the date string 05/12/2018 for any set of four digits in a row using d{4}. When found, we replace them with the replacement text 2017. The result of that query is 05/12/2017 returned as text.

The regexp_split_to_table(string, pattern) function splits delimited text into rows. [[file:ch13.xhtml#ch13list13][Listing 13-13]] uses this function to split the string 'Four,score,and,seven,years,ago' on commas ➋, resulting in a set of rows that has one word in each row:

regexp_split_to_table
---------------------
Four
score
and
seven
years
ago

Keep this function in mind as you tackle the “Try It Yourself” exercises at the end of the chapter.

The regexp_split_to_array(string, pattern) function splits delimited text into an array. The example splits the string Phil Mike Tony Steve on spaces ➌, returning a text array that should look like this in pgAdmin:

regexp_split_to_array
----------------------
{Phil,Mike,Tony,Steve}

The text[] notation in pgAdmin's column header along with curly brackets around the results confirms that this is indeed an array type, which provides another means of analysis. For example, you could then use a function such as array_length() to count the number of words, as shown in [[file:ch13.xhtml#ch13list14][Listing 13-14]].

SELECT array_length(regexp_split_to_array('Phil Mike Tony Steve', ' '), 1);

/Listing 13-14: Finding an array length/

The query should return 4 because four elements are in this array. You can read more about array_length() and other array functions at /[[https://www.postgresql.org/docs/current/static/functions-array.html]]/.

** Full Text Search in PostgreSQL


PostgreSQL comes with a powerful full text search engine that gives you more options when searching for information in large amounts of text. You're familiar with Google or other web search engines and similar technology that powers search on news websites or research databases, such as LexisNexis. Although the implementation and capability of full text search demands several chapters, here I'll walk you through a simple example of setting up a table for text search and functions for searching using PostgreSQL.

For this example, I assembled 35 speeches by former U.S. presidents who served after World War II through the Gerald R. Ford administration. Consisting mostly of State of the Union addresses, these public texts are available through the Internet Archive at /[[https://archive.org/]]/ and the University of California's American Presidency Project at /[[http://www.presidency.ucsb.edu/ws/index.php/]]/. You can find the data in the /sotu-1946-1977.csv/ file along with the book's resources at /[[https://www.nostarch.com/practicalSQL/]]/.

Let's start with the data types unique to full text search.

 /Text Search Data Types/


PostgreSQL's implementation of text search includes two data types. The tsvector data type represents the text to be searched and to be stored in an optimized form. The tsquery data type represents the search query terms and operators. Let's look at the details of both.

** Storing Text as Lexemes with tsvector


The tsvector data type reduces text to a sorted list of /lexemes/, which are units of meaning in language. Think of lexemes as words without the variations created by suffixes. For example, the tsvector format would store the words /washes/, /washed/, and /washing/ as the lexeme /wash/ while noting each word's position in the original text. Converting text to tsvector also removes small /stop words/ that usually don't play a role in search, such as /the/ or /it/.

To see how this data type works, let's convert a string to tsvector format. [[file:ch13.xhtml#ch13list15][Listing 13-15]] uses the PostgreSQL search function to_tsvector(), which normalizes the text “I am walking across the sitting room to sit with you” to lexemes:

SELECT to_tsvector('I am walking across the sitting room to sit with you.');

/Listing 13-15: Converting text to tsvector data/

Execute the code, and it should return the following output in tsvector format:

'across':4 'room':7 'sit':6,9 'walk':3

The to_tsvector() function reduces the number of words from eleven to four, eliminating words such as /I/, /am/, and /the/, which are not helpful search terms. The function removes suffixes, changing /walking/ to /walk/ and /sitting/ to /sit/. It also orders the words alphabetically, and the number following each colon indicates its position in the original string, taking stop words into account. Note that /sit/ is recognized as being in two positions, one for /sitting/ and one for /sit/.

** Creating the Search Terms with tsquery


The tsquery data type represents the full text search query, again optimized as lexemes. It also provides operators for controlling the search. Examples of operators include the ampersand (&) for AND, the pipe symbol (|) for OR, and the exclamation point (!) for NOT. A special <-> operator lets you search for adjacent words or words a certain distance apart.

[[file:ch13.xhtml#ch13list16][Listing 13-16]] shows how the to_tsquery() function converts search terms to the tsquery data type.

SELECT to_tsquery('walking & sitting');

/Listing 13-16: Converting search terms to tsquery data/

After running the code, you should see that the resulting tsquery data type has normalized the terms into lexemes, which match the format of the data to search:

'walk' & 'sit'

Now you can use terms stored as tsquery to search text optimized as tsvector.

** Using the @@ Match Operator for Searching


With the text and search terms converted to the full text search data types, you can use the double at sign (@@) match operator to check whether a query matches text. The first query in [[file:ch13.xhtml#ch13list17][Listing 13-17]] uses to_tsquery() to search for the words /walking/ and /sitting/, which we combine with the & operator. It returns a Boolean value of true because both /walking/ and /sitting/ are present in the text converted by to_tsvector().

SELECT to_tsvector('I am walking across the sitting room') @@ to_tsquery('walking & sitting');
SELECT to_tsvector('I am walking across the sitting room') @@ to_tsquery('walking & running');

/Listing 13-17: Querying a tsvector type with a tsquery/

However, the second query returns false because both /walking/ and /running/ are not present in the text. Now let's build a table for searching the speeches.

 /Creating a Table for Full Text Search/


Let's start by creating a table to hold the speech text. The code in [[file:ch13.xhtml#ch13list18][Listing 13-18]] creates and fills president_speeches so it contains a column for the original speech text as well as a column of type tsvector. The reason is that we need to convert the original speech text into that tsvector column to optimize it for searching. We can't easily do that conversion during import, so let's handle that as a separate step. Be sure to change the file path to match the location of your saved CSV file:

CREATE TABLE president_speeches (
    sotu_id serial PRIMARY KEY,
    president varchar(100) NOT NULL,
    title varchar(250) NOT NULL,
    speech_date date NOT NULL,
    speech_text text NOT NULL,
    search_speech_text tsvector
);

COPY president_speeches (president, title, speech_date, speech_text)
FROM '/C:YourDirectory/sotu-1946-1977.csv'
WITH (FORMAT CSV, DELIMITER '|', HEADER OFF, QUOTE '@');

/Listing 13-18: Creating and filling the president_speeches table/

After executing the query, run SELECT * FROM president_speeches; to see the data. In pgAdmin, hover your mouse over any cell to see extra words not visible in the results grid. You should see a sizeable amount of text in each row of the speech_text column.

Next, we copy the contents of speech_text to the tsvector column search_speech_text and transform it to that data type at the same time. The UPDATE query in [[file:ch13.xhtml#ch13list19][Listing 13-19]] handles the task:

  UPDATE president_speeches
➊ SET search_speech_text = to_tsvector('english', speech_text);

/Listing 13-19: Converting speeches to tsvector in the search_speech_text column/

The SET clause ➊ fills search_speech_text with the output of to_tsvector(). The first argument in the function specifies the language for parsing the lexemes. We're using the default of english here, but you can substitute spanish, german, french, or whatever language you want to use (some languages may require you to find and install additional dictionaries). The second argument is the name of the input column. Run the code to fill the column.

Finally, we want to index the search_speech_text column to speed up searches. You learned about indexing in [[file:ch07.xhtml#ch07][Chapter 7]], which focused on PostgreSQL's default index type, B-Tree. For full text search, the PostgreSQL documentation recommends using the /Generalized Inverted Index/ (/GIN/; see /[[https://www.postgresql.org/docs/current/static/textsearch-indexes.html]]/). You can add a GIN index using CREATE INDEX in [[file:ch13.xhtml#ch13list20][Listing 13-20]]:

CREATE INDEX search_idx ON president_speeches USING gin(search_speech_text);

/Listing 13-20: Creating a GIN index for text search/

The GIN index contains an entry for each lexeme and its location, allowing the database to find matches more quickly.

*NOTE*

/Another way to set up a column for search is to create an index on a text column using the to_tsvector() function. See/ [[https://www.postgresql.org/docs/current/static/textsearch-tables.html]] /for details/.

Now you're ready to use search functions.

 /Searching Speech Text/


Thirty-two years' worth of presidential speeches is fertile ground for exploring history. For example, the query in [[file:ch13.xhtml#ch13list21][Listing 13-21]] lists the speeches in which the president mentioned Vietnam:

  SELECT president, speech_date
  FROM president_speeches
➊ WHERE search_speech_text @@ to_tsquery('Vietnam')
  ORDER BY speech_date;

/Listing 13-21: Finding speeches containing the word/ Vietnam

In the WHERE clause, the query uses the double at sign (@@) match operator ➊ between the search_speech_text column (of data type tsvector) and the query term /Vietnam/, which to_tsquery() transforms into tsquery data. The results should list 10 speeches, showing that the first mention of Vietnam came up in a 1961 special message to Congress by John F. Kennedy and became a recurring topic starting in 1966 as America's involvement in the Vietnam War escalated.

president            speech_date
-----------------    -----------
John F. Kennedy      1961-05-25
Lyndon B. Johnson    1966-01-12
Lyndon B. Johnson    1967-01-10
Lyndon B. Johnson    1968-01-17
Lyndon B. Johnson    1969-01-14
Richard M. Nixon     1970-01-22
Richard M. Nixon     1972-01-20
Richard M. Nixon     1973-02-02
Gerald R. Ford       1975-01-15
Gerald R. Ford       1977-01-12

Before we try more searches, let's add a method for showing the location of our search term in the text.

** Showing Search Result Locations


To see where our search terms appear in text, we can use the ts_headline() function. It displays one or more highlighted search terms surrounded by adjacent words. Options for this function give you flexibility in how to format the display. [[file:ch13.xhtml#ch13list22][Listing 13-22]] highlights how to display a search for a specific instance of /Vietnam/ using ts_headline():

SELECT president,
       speech_date,
     ➊ ts_headline(speech_text, to_tsquery('Vietnam'),
                 ➋ 'StartSel = <,
                    StopSel = >,
                    MinWords=5,
                    MaxWords=7,
                    MaxFragments=1')
FROM president_speeches
WHERE search_speech_text @@ to_tsquery('Vietnam');

/Listing 13-22: Displaying search results with ts_headline()/

To declare ts_headline() ➊, we pass the original speech_text column rather than the tsvector column we used in the search and relevance functions as the first argument. Then, as the second argument, we pass a to_tsquery() function that specifies the word to highlight. We follow this with a third argument that lists optional formatting parameters ➋ separated by commas. Here, we specify the characters to identify the start and end of the highlighted word (StartSel and StopSel). We also set the minimum and maximum number of words to display (MinWords and MaxWords), plus the maximum number of fragments to show using MaxFragments. These settings are optional, and you can adjust them according to your needs.

The results of this query should show at most seven words per speech, highlighting the word /Vietnam/:

[[../images/prog_page_235.jpg]]

Using this technique, we can quickly see the context of the term we searched. You might also use this function to provide flexible display options for a search feature on a web application. Let's continue trying forms of searches.

** Using Multiple Search Terms


As another example, we could look for speeches in which a president mentioned the word /transportation/ but didn't discuss /roads/. We might want to do this to find speeches that focused on broader policy rather than a specific roads program. To do this, we use the syntax in [[file:ch13.xhtml#ch13list23][Listing 13-23]]:

  SELECT president,
       speech_date
     ➊ ts_headline(speech_text, to_tsquery('transportation & !roads'),
                   'StartSel = <,
                    StopSel = >,
                    MinWords=5,
                    MaxWords=7,
                    MaxFragments=1')
  FROM president_speeches
➋ WHERE search_speech_text @@ to_tsquery('transportation & !roads');

/Listing 13-23: Finding speeches with the word/ transportation /but not/ roads

Again, we use ts_headline() ➊ to highlight the terms our search finds. In the to_tsquery() function in the WHERE clause ➋, we pass transportation and roads, combining them with the ampersand (&) operator. We use the exclamation point (!) in front of roads to indicate that we want speeches that do not contain this word. This query should find eight speeches that fit the criteria. Here are the first four rows:

[[../images/prog_page_236.jpg]]

Notice that the highlighted words in the ts_headline column include transportation and transport. The reason is that the to_tsquery() function converted transportation to the lexeme transport for the search term. This database behavior is extremely useful in helping to find relevant related words.

** Searching for Adjacent Words


Finally, we'll use the distance (<->) operator, which consists of a hyphen between the less than and greater than signs, to find adjacent words. Alternatively, you can place a number between the signs to find terms that many words apart. For example, [[file:ch13.xhtml#ch13list24][Listing 13-24]] searches for any speeches that include the word /military/ immediately followed by /defense/:

SELECT president,
       speech_date,
       ts_headline(speech_text, to_tsquery('military <-> defense'),
                   'StartSel = <,
                    StopSel = >,
                    MinWords=5,
                    MaxWords=7,
                    MaxFragments=1')
FROM president_speeches
WHERE search_speech_text @@ to_tsquery('military <-> defense');

/Listing 13-24: Finding speeches where/ defense /follows/ military

This query should find four speeches, and because to_tsquery() converts the search terms to lexemes, the words identified in the speeches should include plurals, such as /military defenses/. The following shows the four speeches that have the adjacent terms:

[[../images/prog_page_237.jpg]]

If you changed the query terms to military <2> defense, the database would return matches where the terms are exactly two words apart, as in the phrase “our military and defense commitments.”

 /Ranking Query Matches by Relevance/


You can also rank search results by relevance using two of PostgreSQL's full text search functions. These functions are helpful when you're trying to understand which piece of text, or speech in this case, is most relevant to your particular search terms.

One function, ts_rank(), generates a rank value (returned as a variable-precision real data type) based on how often the lexemes you're searching for appear in the text. The other function, ts_rank_cd(), considers how close the lexemes searched are to each other. Both functions can take optional arguments to take into account document length and other factors. The rank value they generate is an arbitrary decimal that's useful for sorting but doesn't have any inherent meaning. For example, a value of 0.375 generated during one query isn't directly comparable to the same value generated during a different query.

As an example, [[file:ch13.xhtml#ch13list25][Listing 13-25]] uses ts_rank() to rank speeches containing all the words /war/, /security/, /threat/, and /enemy/:

  SELECT president,
         speech_date,
       ➊ ts_rank(search_speech_text,
                 to_tsquery('war & security & threat & enemy')) AS score
  FROM president_speeches
➋ WHERE search_speech_text @@ to_tsquery('war & security & threat & enemy')
  ORDER BY score DESC/
  LIMIT 5

/Listing 13-25: Scoring relevance with ts_rank()/

In this query, the ts_rank() function ➊ takes two arguments: the search_speech_text column and the output of a to_tsquery() function containing the search terms. The output of the function receives the alias score. In the WHERE clause ➋ we filter the results to only those speeches that contain the search terms specified. Then we order the results in score in descending order and return just five of the highest-ranking speeches. The results should be as follows:

president               speech_date    score
--------------------    -----------    ---------
Harry S. Truman         1946-01-21      0.257522
Lyndon B. Johnson       1968-01-17      0.186296
Dwight D. Eisenhower    1957-01-10      0.140851
Harry S. Truman         1952-01-09     0.0982469
Richard M. Nixon        1972-01-20     0.0973585

Harry S. Truman's 1946 State of the Union message, just four months after the end of World War II, contains the words /war/, /security/, /threat/, and /enemy/ more often than the other speeches. However, it also happens to be the longest speech in the table (which you can determine by using char_length(), as you learned earlier in the chapter). The length of the speeches influences these rankings because ts_rank() factors in the number of matching terms in a given text. Lyndon B. Johnson's 1968 State of the Union address, delivered at the height of the Vietnam War, comes in second.

It would be ideal to compare frequencies between speeches of identical lengths to get a more accurate ranking, but this isn't always possible. However, we can factor in the length of each speech by adding a normalization code as a third parameter of the ts_rank() function, as shown in [[file:ch13.xhtml#ch13list26][Listing 13-26]]:

SELECT president,
       speech_date,
       ts_rank(search_speech_text,
               to_tsquery('war & security & threat & enemy'), 2➊)::numeric
               AS score
FROM president_speeches
WHERE search_speech_text @@ to_tsquery('war & security & threat & enemy')
ORDER BY score DESC
LIMIT 5;

/Listing 13-26: Normalizing ts_rank() by speech length/

Adding the optional code 2 ➊ instructs the function to divide the score by the length of the data in the search_speech_text column. This quotient then represents a score normalized by the document length, giving an apples-to-apples comparison among the speeches. The PostgreSQL documentation at /[[https://www.postgresql.org/docs/current/static/textsearch-controls.html]]/ lists all the options available for text search, including using the document length and dividing by the number of unique words.

After running the code in [[file:ch13.xhtml#ch13list26][Listing 13-26]], the rankings should change:

president               speech_date    score
--------------------    -----------    ------------
Lyndon B. Johnson       1968-01-17     0.0000728288
Dwight D. Eisenhower    1957-01-10     0.0000633609
Richard M. Nixon        1972-01-20     0.0000497998
Harry S. Truman         1952-01-09     0.0000365366
Dwight D. Eisenhower    1958-01-09     0.0000355315

In contrast to the ranking results in [[file:ch13.xhtml#ch13list25][Listing 13-25]], Johnson's 1968 speech now tops the rankings, and Truman's 1946 message falls out of the top five. This might be a more meaningful ranking than the first sample output, because we normalized it by length. But four of the five top-ranked speeches are the same between the two sets, and you can be reasonably certain that each of these four is worthy of closer examination to understand more about wartime presidential speeches.

** Wrapping Up


Far from being boring, text offers abundant opportunities for data analysis. In this chapter, you've learned valuable techniques for turning ordinary text into data you can extract, quantify, search, and rank. In your work or studies, keep an eye out for routine reports that have facts buried inside chunks of text. You can use regular expressions to dig them out, turn them into structured data, and analyze them to find trends. You can also use search functions to analyze the text.

In the next chapter, you'll learn how PostgreSQL can help you analyze geographic information.


*TRY IT YOURSELF*

Use your new text-wrangling skills to tackle these tasks:

1. The style guide of a publishing company you're writing for wants you to avoid commas before suffixes in names. But there are several names like Alvarez, Jr. and Williams, Sr. in your database. Which functions can you use to remove the comma? Would a regular expression function help? How would you capture just the suffixes to place them into a separate column?

2. Using any one of the State of the Union addresses, count the number of unique words that are five characters or more. (Hint: You can use regexp_split_to_table() in a subquery to create a table of words to count.) Bonus: Remove commas and periods at the end of each word.

3. Rewrite the query in [[file:ch13.xhtml#ch13list25][Listing 13-25]] using the ts_rank_cd() function instead of ts_rank(). According to the PostgreSQL documentation, ts_rank_cd() computes cover density, which takes into account how close the lexeme search terms are to each other. Does using the ts_rank_cd() function significantly change the results?


\_date,\\
       ts\_rank(search\_speech\_text,\\
               to\_tsquery('war & security & threat & enemy'), 2➊)::numeric\\
               AS score\\
FROM president\_speeches\\
WHERE search\_speech\_text @@ to\_tsquery('war & security & threat & enemy')\\
ORDER BY score DESC\\
LIMIT 5;

/Listing 13-26: Normalizing ts\_rank() by speech length/

Adding the optional code 2 ➊ instructs the function to divide the score by the length of the data in the search\_speech\_text column. This quotient then represents a score normalized by the document length, giving an apples-to-apples comparison among the speeches. The PostgreSQL documentation at /[[https://www.postgresql.org/docs/current/static/textsearch-controls.html]]/ lists all the options available for text search, including using the document length and dividing by the number of unique words.

After running the code in [[file:ch13.xhtml#ch13list26][Listing 13-26]], the rankings should change:

president               speech\_date    score\\
--------------------    -----------    ------------\\
Lyndon B. Johnson       1968-01-17     0.0000728288\\
Dwight D. Eisenhower    1957-01-10     0.0000633609\\
Richard M. Nixon        1972-01-20     0.0000497998\\
Harry S. Truman         1952-01-09     0.0000365366\\
Dwight D. Eisenhower    1958-01-09     0.0000355315

In contrast to the ranking results in [[file:ch13.xhtml#ch13list25][Listing 13-25]], Johnson's 1968 speech now tops the rankings, and Truman's 1946 message falls out of the top five. This might be a more meaningful ranking than the first sample output, because we normalized it by length. But four of the five top-ranked speeches are the same between the two sets, and you can be reasonably certain that each of these four is worthy of closer examination to understand more about wartime presidential speeches.

**** Wrapping Up
    :PROPERTIES:
    :CUSTOM_ID: lev234
    :CLASS: h3
    :END:

Far from being boring, text offers abundant opportunities for data analysis. In this chapter, you've learned valuable techniques for turning ordinary text into data you can extract, quantify, search, and rank. In your work or studies, keep an eye out for routine reports that have facts buried inside chunks of text. You can use regular expressions to dig them out, turn them into structured data, and analyze them to find trends. You can also use search functions to analyze the text.

In the next chapter, you'll learn how PostgreSQL can help you analyze geographic information.

<<ch13sb1>>
*TRY IT YOURSELF*

Use your new text-wrangling skills to tackle these tasks:

1. The style guide of a publishing company you're writing for wants you to avoid commas before suffixes in names. But there are several names like Alvarez, Jr. and Williams, Sr. in your database. Which functions can you use to remove the comma? Would a regular expression function help? How would you capture just the suffixes to place them into a separate column?

2. Using any one of the State of the Union addresses, count the number of unique words that are five characters or more. (Hint: You can use regexp\_split\_to\_table() in a subquery to create a table of words to count.) Bonus: Remove commas and periods at the end of each word.

3. Rewrite the query in [[file:ch13.xhtml#ch13list25][Listing 13-25]] using the ts\_rank\_cd() function instead of ts\_rank(). According to the PostgreSQL documentation, ts\_rank\_cd() computes cover density, which takes into account how close the lexeme search terms are to each other. Does using the ts\_rank\_cd() function significantly change the results?


