* 9.INSPECTING AND MODIFYING DATA*


If you asked me to propose a toast to a newly minted class of data analysts, I'd probably raise my glass and say, “May your data always be free of errors and may it always arrive perfectly structured!” Life would be ideal if these sentiments were feasible. In reality, you'll sometimes receive data in such a sorry state that it's hard to analyze without modifying it in some way. This is called /dirty data/, which is a general label for data with errors, missing values, or poor organization that makes standard queries ineffective. When data is converted from one file type to another or when a column receives the wrong data type, information can be lost. Typos and spelling inconsistencies can also result in dirty data. Whatever the cause may be, dirty data is the bane of the data analyst.

In this chapter, you'll use SQL to clean up dirty data as well as perform other useful maintenance tasks. You'll learn how to examine data to assess its quality and how to modify data and tables to make analysis easier. But the techniques you'll learn will be useful for more than just cleaning data. The ability to make changes to data and tables gives you options for updating or adding new information to your database as it becomes available, elevating your database from a static collection to a living record.

Let's begin by importing our data.

** Importing Data on Meat, Poultry, and Egg Producers


For this example, we'll use a directory of U.S. meat, poultry, and egg producers. The Food Safety and Inspection Service (FSIS), an agency within the U.S. Department of Agriculture, compiles and updates this database every month. The FSIS is responsible for inspecting animals and food at more than 6,000 meat processing plants, slaughterhouses, farms, and the like. If inspectors find a problem, such as bacterial contamination or mislabeled food, the agency can issue a recall. Anyone interested in agriculture business, food supply chain, or outbreaks of foodborne illnesses will find the directory useful. Read more about the agency on its site at /[[https://www.fsis.usda.gov/]]/.

The file we'll use comes from the directory's page on /[[https://www.data.gov/]]/, a website run by the U.S. federal government that catalogs thousands of data sets from various federal agencies (/[[https://catalog.data.gov/dataset/meat-poultry-and-egg-inspection-directory-by-establishment-name/]]/). We'll examine the original data as it was available for download, with the exception of the ZIP Codes column (I'll explain why later). You'll find the data in the file /MPI_Directory_by_Establishment_Name.csv/ along with other resources for this book at /[[https://www.nostarch.com/practicalSQL/]]/.

To import the file into PostgreSQL, use the code in [[file:ch09.xhtml#ch09list1][Listing 9-1]] to create a table called meat_poultry_egg_inspect and use COPY to add the CSV file to the table. As in previous examples, use pgAdmin to connect to your analysis database, and then open the Query Tool to run the code. Remember to change the path in the COPY statement to reflect the location of your CSV file.

  CREATE TABLE meat_poultry_egg_inspect (
    ➊ est_number varchar(50) CONSTRAINT est_number_key PRIMARY KEY,
      company varchar(100),
      street varchar(100),
      city varchar(30),
      st varchar(2),
      zip varchar(5),
      phone varchar(14),
      grant_date date,
    ➋ activities text,
      dbas text
  );

➌ COPY meat_poultry_egg_inspect
  FROM '/C:YourDirectory/MPI_Directory_by_Establishment_Name.csv'
  WITH (FORMAT CSV, HEADER, DELIMITER ',');

  ➍ CREATE INDEX company_idx ON meat_poultry_egg_inspect (company);

/Listing 9-1: Importing the FSIS Meat, Poultry, and Egg Inspection Directory/

The meat_poultry_egg_inspect table has 10 columns. We add a natural primary key constraint to the est_number column ➊, which contains a unique value for each row that identifies the establishment. Most of the remaining columns relate to the company's name and location. You'll use the activities column ➋, which describes activities at the company, in the “Try It Yourself” exercise at the end of this chapter. We set the activities and dbas columns to text, a data type that in PostgreSQL affords us up to 1GB of characters, because some of the strings in the columns are thousands of characters long. We import the CSV file ➌ and then create an index on the company column ➍ to speed up searches for particular companies.

For practice, let's use the count() aggregate function introduced in [[file:ch08.xhtml#ch08][Chapter 8]] to check how many rows are in the meat_poultry_egg_inspect table:

SELECT count(*) FROM meat_poultry_egg_inspect;

The result should show 6,287 rows. Now let's find out what the data contains and determine whether we can glean useful information from it as is, or if we need to modify it in some way.

** Interviewing the Data Set


Interviewing data is my favorite part of analysis. We interview a data set to discover its details: what it holds, what questions it can answer, and how suitable it is for our purposes, the same way a job interview reveals whether a candidate has the skills required for the position.

The aggregate queries you learned in [[file:ch08.xhtml#ch08][Chapter 8]] are a useful interviewing tool because they often expose the limitations of a data set or raise questions you may want to ask before drawing conclusions in your analysis and assuming the validity of your findings.

For example, the meat_poultry_egg_inspect table's rows describe food producers. At first glance, we might assume that each company in each row operates at a distinct address. But it's never safe to assume in data analysis, so let's check using the code in [[file:ch09.xhtml#ch09list2][Listing 9-2]]:

SELECT company,
       street,
       city,
       st,
       count(*) AS address_count
FROM meat_poultry_egg_inspect
GROUP BY company, street, city, st
HAVING count(*) > 1
ORDER BY company, street, city, st;

/Listing 9-2: Finding multiple companies at the same address/

Here, we group companies by unique combinations of the company, street, city, and st columns. Then we use count(*), which returns the number of rows for each combination of those columns and gives it the alias address_count. Using the HAVING clause introduced in [[file:ch08.xhtml#ch08][Chapter 8]], we filter the results to show only cases where more than one row has the same combination of values. This should return all duplicate addresses for a company.

The query returns 23 rows, which means there are close to two dozen cases where the same company is listed multiple times at the same address:

[[../images/prog_page_132.jpg]]

This is not necessarily a problem. There may be valid reasons for a company to appear multiple times at the same address. For example, two types of processing plants could exist with the same name. On the other hand, we may have found data entry errors. Either way, it's sound practice to eliminate concerns about the validity of a data set before relying on it, and the result should prompt us to investigate individual cases before we draw conclusions. However, this data set has other issues that we need to look at before we can get meaningful information from it. Let's work through a few examples.

 /Checking for Missing Values/


Let's start checking for missing values by asking a basic question: how many of the meat, poultry, and egg processing companies are in each state? Finding out whether we have values from all states and whether any rows are missing a state code will serve as another useful check on the data. We'll use the aggregate function count() along with GROUP BY to determine this, as shown in [[file:ch09.xhtml#ch09list3][Listing 9-3]]:

SELECT st,
       count(*) AS st_count
FROM meat_poultry_egg_inspect
GROUP BY st
ORDER BY st;

/Listing 9-3: Grouping and counting states/

The query is a simple count similar to the examples in [[file:ch08.xhtml#ch08][Chapter 8]]. When you run the query, it tallies the number of times each state postal code (st) appears in the table. Your result should include 57 rows, grouped by the state postal code in the column st. Why more than the 50 U.S. states? Because the data includes Puerto Rico and other unincorporated U.S. territories, such as Guam and American Samoa. Alaska (AK) is at the top of the results with a count of 17 establishments:

st    st_count
--    --------
AK          17
AL          93
AR          87
AS           1
/--snip--/
WA         139
WI         184
WV          23
WY           1
             3

However, the row at the bottom of the list has a count of 3 and a NULL value in the st_count column. To find out what this means, let's query the rows where the st column has NULL values.

*NOTE*

/Depending on the database implementation, NULL values will either appear first or last in a sorted column. In PostgreSQL, they appear last by default. The ANSI SQL standard doesn't specify one or the other, but it lets you add NULLS FIRST or NULLS LAST to an ORDER BY clause to specify a preference. For example, to make NULL values appear first in the preceding query, the clause would read ORDER BY st NULLS FIRST./

In [[file:ch09.xhtml#ch09list4][Listing 9-4]], we use the technique covered in “Using NULL to Find Rows with Missing Values” on [[file:ch06.xhtml#page_83][page 83]], adding a WHERE clause with the st column and the IS NULL keywords to find which rows are missing a state code:

SELECT est_number,
       company,
       city,
       st,
       zip
FROM meat_poultry_egg_inspect
WHERE st IS NULL;

/Listing 9-4: Using IS NULL to find missing values in the st column/

This query returns three rows that don't have a value in the st column:

[[../images/prog_page_133.jpg]]

If we want an accurate count of establishments per state, these missing values would lead to an incorrect result. To find the source of this dirty data, it's worth making a quick visual check of the original file downloaded from /[[https://www.data.gov/]]/. Unless you're working with files in the gigabyte range, you can usually open a CSV file in a text editor and search for the row. If you're working with larger files, you might be able to examine the source data using utilities such as grep (on Linux and macOS) or findstr (on Windows). In this case, a visual check confirms that, indeed, there was no state listed in those rows in the CSV file, so the error is organic to the data, not one introduced during import.

In our interview of the data so far, we've discovered that we'll need to add missing values to the st column to clean up this table. Let's look at what other issues exist in our data set and make a list of cleanup tasks.

 /Checking for Inconsistent Data Values/


Inconsistent data is another factor that can hamper our analysis. We can check for inconsistently entered data within a column by using GROUP BY with count(). When you scan the unduplicated values in the results, you might be able to spot variations in the spelling of names or other attributes.

For example, many of the 6,200 companies in our table are multiple locations owned by a few multinational food corporations, such as Cargill or Tyson Foods. To find out how many locations each company owns, we would try to count the values in the company column. Let's see what happens when we do, using the query in [[file:ch09.xhtml#ch09list5][Listing 9-5]]:

SELECT company,
       count(*) AS company_count
FROM meat_poultry_egg_inspect
GROUP BY company
ORDER BY company ASC;

/Listing 9-5: Using GROUP BY and count() to find inconsistent company names/

Scrolling through the results reveals a number of cases in which a company's name is spelled several different ways. For example, notice the entries for the Armour-Eckrich brand:

company                        company_count
---------------------------    -------------
/--snip--/
Armour - Eckrich Meats, LLC                1
Armour-Eckrich Meats LLC                   3
Armour-Eckrich Meats, Inc.                 1
Armour-Eckrich Meats, LLC                  2
/--snip--/

At least four different spellings are shown for seven establishments that are likely owned by the same company. If we later perform any aggregation by company, it would help to standardize the names so all of the items counted or summed are grouped properly. Let's add that to our list of items to fix.

 /Checking for Malformed Values Using length()/


It's a good idea to check for unexpected values in a column that should be consistently formatted. For example, each entry in the zip column in the meat_poultry_egg_inspect table should be formatted in the style of U.S. ZIP Codes with five digits. However, that's not what is in our data set.

Solely for the purpose of this example, I replicated an error I've committed before. When I converted the original Excel file to a CSV file, I stored the ZIP Code in the “General” number format in the spreadsheet instead of as a text value. By doing so, any ZIP Code that begins with a zero, such as 07502 for Paterson, NJ, lost the leading zero because an integer can't start with a zero. As a result, 07502 appears in the table as 7502. You can make this error in a variety of ways, including by copying and pasting data into Excel columns set to “General.” After being burned a few times, I learned to take extra caution with numbers that should be formatted as text.

My deliberate error appears when we run the code in [[file:ch09.xhtml#ch09list6][Listing 9-6]]. The example introduces length(), a /string function/ that counts the number of characters in a string. We combine length() with count() and GROUP BY to determine how many rows have five characters in the zip field and how many have a value other than five. To make it easy to scan the results, we use length() in the ORDER BY clause.

SELECT length(zip),
       count(*) AS length_count
FROM meat_poultry_egg_inspect
GROUP BY length(zip)
ORDER BY length(zip) ASC;

/Listing 9-6: Using length() and count() to test the zip column/

The results confirm the formatting error. As you can see, 496 of the ZIP Codes are four characters long, and 86 are three characters long, which means these numbers originally had two leading zeros that my conversion erroneously eliminated:

length    length_count
------    ------------
     3              86
     4             496
     5            5705

Using the WHERE clause, we can check the details of the results to see which states these shortened ZIP Codes correspond to, as shown in [[file:ch09.xhtml#ch09list7][Listing 9-7]]:

  SELECT st,
         count(*) AS st_count
  FROM meat_poultry_egg_inspect
➊ WHERE length(zip) < 5
  GROUP BY st
  ORDER BY st ASC;

/Listing 9-7: Filtering with length() to find short zip values/

The length() function inside the WHERE clause ➊ returns a count of rows where the ZIP Code is less than five characters for each state code. The result is what we would expect. The states are largely in the Northeast region of the United States where ZIP Codes often start with a zero:

st    st_count
--    --------
CT          55
MA         101
ME          24
NH          18
NJ         244
PR          84
RI          27
VI           2
VT          27

Obviously, we don't want this error to persist, so we'll add it to our list of items to correct. So far, we need to correct the following issues in our data set:

- Missing values for three rows in the st column
- Inconsistent spelling of at least one company's name
- Inaccurate ZIP Codes due to file conversion

Next, we'll look at how to use SQL to fix these issues by modifying your data.

** Modifying Tables, Columns, and Data


Almost nothing in a database, from tables to columns and the data types and values they contain, is set in concrete after it's created. As your needs change, you can add columns to a table, change data types on existing columns, and edit values. Fortunately, you can use SQL to modify, delete, or add to existing data and structures. Given the issues we discovered in the meat_poultry_egg_inspect table, being able to modify our database will come in handy.

To make changes to our database, we'll use two SQL commands: the first command, ALTER TABLE, is part of the ANSI SQL standard and provides options to ADD COLUMN, ALTER COLUMN, and DROP COLUMN, among others. Typically, PostgreSQL and other databases include implementation-specific extensions to ALTER TABLE that provide an array of options for managing database objects (see /[[https://www.postgresql.org/docs/current/static/sql-altertable.html]]/). For our exercises, we'll stick with the core options.

The second command, UPDATE, also included in the SQL standard, allows you to change values in a table's columns. You can supply criteria using the WHERE clause to choose which rows to update.

Let's explore the basic syntax and options for both commands, and then use them to fix the issues in our data set.

*WHEN TO TOSS YOUR DATA*

If your interview of the data reveals too many missing values or values that defy common sense---such as numbers ranging in the billions when you expected thousands---it's time to reevaluate its use. The data may not be reliable enough to serve as the foundation of your analysis.

If you suspect as much, the first step is to revisit the original data file. Make sure you imported it correctly and that values in all the source columns are located in the same columns in the table. You might need to open the original spreadsheet or CSV file and do a visual comparison. The second step is to call the agency or company that produced the data to confirm what you see and seek an explanation. You might also ask for advice from others who have used the same data.

More than once I've had to toss a data set after determining that it was poorly assembled or simply incomplete. Sometimes, the amount of work required to make a data set usable undermines its usefulness. These situations require you to make a tough judgment call. But it's better to start over or find an alternative than to use bad data that can lead to faulty conclusions.

 /Modifying Tables with ALTER TABLE/


We can use the ALTER TABLE statement to modify the structure of tables. The following examples show the syntax for common operations that are part of standard ANSI SQL. The code for adding a column to a table looks like this:

ALTER TABLE /table/ ADD COLUMN /column data_type/;

Similarly, we can remove a column with the following syntax:

ALTER TABLE /table/ DROP COLUMN /column/;

To change the data type of a column, we would use this code:

ALTER TABLE /table/ ALTER COLUMN /column/ SET DATA TYPE /data_type/;

Adding a NOT NULL constraint to a column will look like the following:

ALTER TABLE /table/ ALTER COLUMN /column/ SET NOT NULL;

Note that in PostgreSQL and some other systems, adding a constraint to the table causes all rows to be checked to see whether they comply with the constraint. If the table has millions of rows, this could take a while.

Removing the NOT NULL constraint looks like this:

ALTER TABLE /table/ ALTER COLUMN /column/ DROP NOT NULL;

When you execute an ALTER TABLE statement with the placeholders filled in, you should see a message that reads ALTER TABLE in the pgAdmin output screen. If an operation violates a constraint or if you attempt to change a column's data type and the existing values in the column won't conform to the new data type, PostgreSQL returns an error. But PostgreSQL won't give you any warning about deleting data when you drop a column, so use extra caution before dropping a column.

 /Modifying Values with UPDATE/


The UPDATE statement modifies the data in a column in all rows or in a subset of rows that meet a condition. Its basic syntax, which would update the data in every row in a column, follows this form:

UPDATE /table/
SET /column/ = /value/;

We first pass UPDATE the name of the table to update, and then pass the SET clause the column that contains the values to change. The new value to place in the column can be a string, number, the name of another column, or even a query or expression that generates a value. We can update values in multiple columns at a time by adding additional columns and source values, and separating each column and value statement with a comma:

UPDATE /table/
SET /column_a/ = /value/,
    /column_b/ = /value/;

To restrict the update to particular rows, we add a WHERE clause with some criteria that must be met before the update can happen:

UPDATE /table/
SET /column/ = /value/
WHERE /criteria/;

We can also update one table with values from another table. Standard ANSI SQL requires that we use a /subquery/, a query inside a query, to specify which values and rows to update:

UPDATE /table/
SET /column/ = (SELECT /column/
              FROM /table_b/
              WHERE /table.column/ = /table_b.column/)
WHERE EXISTS (SELECT /column/
              FROM /table_b/
              WHERE /table.column/ = /table_b.column/);

The value portion of the SET clause is a subquery, which is a SELECT statement inside parentheses that generates the values for the update. Similarly, the WHERE EXISTS clause uses a SELECT statement to generate values that serve as the filter for the update. If we didn't use this clause, we might inadvertently set some values to NULL without planning to. (If this syntax looks somewhat complicated, that's okay. I'll cover subqueries in detail in [[file:ch12.xhtml#ch12][Chapter 12]].)

Some database managers offer additional syntax for updating across tables. PostgreSQL supports the ANSI standard but also a simpler syntax using a FROM clause for updating values across tables:

UPDATE /table/
SET /column/ = /table_b.column/
FROM /table_b/
WHERE /table.column = table_b.column/;

When you execute an UPDATE statement, PostgreSQL returns a message stating UPDATE along with the number of rows affected.

 /Creating Backup Tables/


Before modifying a table, it's a good idea to make a copy for reference and backup in case you accidentally destroy some data. [[file:ch09.xhtml#ch09list8][Listing 9-8]] shows how to use a variation of the familiar CREATE TABLE statement to make a new table based on the existing data and structure of the table we want to duplicate:

CREATE TABLE meat_poultry_egg_inspect_backup AS
SELECT * FROM meat_poultry_egg_inspect;

/Listing 9-8: Backing up a table/

After running the CREATE TABLE statement, the result should be a pristine copy of your table with the new specified name. You can confirm this by counting the number of records in both tables with one query:

SELECT
    (SELECT count(*) FROM meat_poultry_egg_inspect) AS original,
    (SELECT count(*) FROM meat_poultry_egg_inspect_backup) AS backup;

The results should return a count of 6,287 from both tables, like this:

original    backup
--------    ------
    6287      6287

If the counts match, you can be sure your backup table is an exact copy of the structure and contents of the original table. As an added measure and for easy reference, we'll use ALTER TABLE to make copies of column data within the table we're updating.

*NOTE*

/Indexes are not copied when creating a table backup using the CREATE TABLE statement. If you decide to run queries on the backup, be sure to create a separate index on that table./

 /Restoring Missing Column Values/


Earlier in this chapter, the query in [[file:ch09.xhtml#ch09list4][Listing 9-4]] revealed that three rows in the meat_poultry_egg_inspect table don't have a value in the st column:

[[../images/prog_page_140.jpg]]

To get a complete count of establishments in each state, we need to fill those missing values using an UPDATE statement.

** Creating a Column Copy


Even though we've backed up this table, let's take extra caution and make a copy of the st column within the table so we still have the original data if we make some dire error somewhere! Let's create the copy and fill it with the existing st column values using the SQL statements in [[file:ch09.xhtml#ch09list9][Listing 9-9]]:

➊ ALTER TABLE meat_poultry_egg_inspect ADD COLUMN st_copy varchar(2);

  UPDATE meat_poultry_egg_inspect
➋ SET st_copy = st;

/Listing 9-9: Creating and filling the st_copy column with ALTER TABLE and UPDATE/

The ALTER TABLE statement ➊ adds a column called st_copy using the same varchar data type as the original st column. Next, the UPDATE statement's SET clause ➋ fills our newly created st_copy column with the values in column st. Because we don't specify any criteria using a WHERE clause, values in every row are updated, and PostgreSQL returns the message UPDATE 6287. Again, it's worth noting that on a very large table, this operation could take some time and also substantially increase the table's size. Making a column copy in addition to a table backup isn't entirely necessary, but if you're the patient, cautious type, it can be worthwhile.

We can confirm the values were copied properly with a simple SELECT query on both columns, as in [[file:ch09.xhtml#ch09list10][Listing 9-10]]:

SELECT st,
       st_copy
FROM meat_poultry_egg_inspect
ORDER BY st;

/Listing 9-10: Checking values in the st and st_copy columns/

The SELECT query returns 6,287 rows showing both columns holding values except the three rows with missing values:

st    st_copy
--    -------
AK    AK
AK    AK
AK    AK
AK    AK
/--snip--/

Now, with our original data safely stored in the st_copy column, we can update the three rows with missing state codes. This is now our in-table backup, so if something goes drastically wrong while we're updating the missing data in the original column, we can easily copy the original data back in. I'll show you how after we apply the first updates.

** Updating Rows Where Values Are Missing


To update those rows missing values, we first find the values we need with a quick online search: Atlas Inspection is located in Minnesota; Hall-Namie Packing is in Alabama; and Jones Dairy is in Wisconsin. Add those states to the appropriate rows using the code in [[file:ch09.xhtml#ch09list11][Listing 9-11]]:

  UPDATE meat_poultry_egg_inspect
  SET st = 'MN'
➊ WHERE est_number = 'V18677A';

  UPDATE meat_poultry_egg_inspect
  SET st = 'AL'
  WHERE est_number = 'M45319+P45319';

  UPDATE meat_poultry_egg_inspect
  SET st = 'WI'
  WHERE est_number = 'M263A+P263A+V263A';

/Listing 9-11: Updating the st column for three establishments/

Because we want each UPDATE statement to affect a single row, we include a WHERE clause ➊ for each that identifies the company's unique est_number, which is the table's primary key. When we run each query, PostgreSQL responds with the message UPDATE 1, showing that only one row was updated for each query.

If we rerun the code in [[file:ch09.xhtml#ch09list4][Listing 9-4]] to find rows where st is NULL, the query should return nothing. Success! Our count of establishments by state is now complete.

** Restoring Original Values


What happens if we botch an update by providing the wrong values or updating the wrong rows? Because we've backed up the entire table and the st column within the table, we can easily copy the data back from either location. [[file:ch09.xhtml#ch09list12][Listing 9-12]] shows the two options.

➊ UPDATE meat_poultry_egg_inspect
  SET st = st_copy;

➋ UPDATE meat_poultry_egg_inspect original
  SET st = backup.st
  FROM meat_poultry_egg_inspect_backup backup
  WHERE original.est_number = backup.est_number;

/Listing 9-12: Restoring original st column values/

To restore the values from the backup column in meat_poultry_egg_inspect you created in [[file:ch09.xhtml#ch09list9][Listing 9-9]], run an UPDATE query ➊ that sets st to the values in st_copy. Both columns should again have the identical original values. Alternatively, you can create an UPDATE ➋ that sets st to values in the st column from the meat_poultry_egg_inspect_backup table you made in [[file:ch09.xhtml#ch09list8][Listing 9-8]].

 /Updating Values for Consistency/


In [[file:ch09.xhtml#ch09list5][Listing 9-5]] we discovered several cases where a single company's name was entered inconsistently. If we want to aggregate data by company name, such inconsistencies will hinder us from doing so.

Here are the spelling variations of Armour-Eckrich Meats in [[file:ch09.xhtml#ch09list5][Listing 9-5]]:

/--snip--/
Armour - Eckrich Meats, LLC
Armour-Eckrich Meats LLC
Armour-Eckrich Meats, Inc.
Armour-Eckrich Meats, LLC
/--snip--/

We can standardize the spelling of this company's name by using an UPDATE statement. To protect our data, we'll create a new column for the standardized spellings, copy the names in company into the new column, and work in the new column to avoid tampering with the original data. [[file:ch09.xhtml#ch09list13][Listing 9-13]] has the code for both actions:

ALTER TABLE meat_poultry_egg_inspect ADD COLUMN company_standard varchar(100);

UPDATE meat_poultry_egg_inspect
SET company_standard = company;

/Listing 9-13: Creating and filling the company_standard column/

Now, let's say we want any name in company that contains the string Armour to appear in company_standard as Armour-Eckrich Meats. (This assumes we've checked all entries containing Armour and want to standardize them.) We can update all the rows matching the string Armour by using a WHERE clause. Run the two statements in [[file:ch09.xhtml#ch09list14][Listing 9-14]]:

  UPDATE meat_poultry_egg_inspect
  SET company_standard = 'Armour-Eckrich Meats'
➊ WHERE company LIKE 'Armour%';

  SELECT company, company_standard
  FROM meat_poultry_egg_inspect
  WHERE company LIKE 'Armour%';

/Listing 9-14: Using an UPDATE statement to modify field values that match a string/

The important piece of this query is the WHERE clause that uses the LIKE keyword ➊ that was introduced with filtering in [[file:ch02.xhtml#ch02][Chapter 2]]. Including the wildcard syntax % at the end of the string Armour updates all rows that start with those characters regardless of what comes after them. The clause lets us target all the varied spellings used for the company's name. The SELECT statement in [[file:ch09.xhtml#ch09list14][Listing 9-14]] returns the results of the updated company_standard column next to the original company column:

company                        company_standard
---------------------------    --------------------
Armour-Eckrich Meats LLC       Armour-Eckrich Meats
Armour - Eckrich Meats, LLC    Armour-Eckrich Meats
Armour-Eckrich Meats LLC       Armour-Eckrich Meats
Armour-Eckrich Meats LLC       Armour-Eckrich Meats
Armour-Eckrich Meats, Inc.     Armour-Eckrich Meats
Armour-Eckrich Meats, LLC      Armour-Eckrich Meats
Armour-Eckrich Meats, LLC      Armour-Eckrich Meats

The values for Armour-Eckrich in company_standard are now standardized with consistent spelling. If we want to standardize other company names in the table, we would create an UPDATE statement for each case. We would also keep the original company column for reference.

 /Repairing ZIP Codes Using Concatenation/


Our final fix repairs values in the zip column that lost leading zeros as the result of my deliberate data faux pas. For companies in Puerto Rico and the U.S. Virgin Islands, we need to restore two leading zeros to the values in zip because (aside from an IRS processing facility in Holtsville, NY) they're the only locations in the United States where ZIP Codes start with two zeros. Then, for the other states, located mostly in New England, we'll restore a single leading zero.

We'll use UPDATE again but this time in conjunction with the double-pipe /string operator/ (||), which performs /concatenation/. Concatenation combines two or more string or non-string values into one. For example, inserting || between the strings abc and 123 results in abc123. The double-pipe operator is a SQL standard for concatenation supported by PostgreSQL. You can use it in many contexts, such as UPDATE queries and SELECT, to provide custom output from existing as well as new data.

First, [[file:ch09.xhtml#ch09list15][Listing 9-15]] makes a backup copy of the zip column in the same way we made a backup of the st column earlier:

ALTER TABLE meat_poultry_egg_inspect ADD COLUMN zip_copy varchar(5);

UPDATE meat_poultry_egg_inspect
SET zip_copy = zip;

/Listing 9-15: Creating and filling the zip_copy column/

Next, we use the code in [[file:ch09.xhtml#ch09list16][Listing 9-16]] to perform the first update:

  UPDATE meat_poultry_egg_inspect
➊ SET zip = '00' || zip
➋ WHERE st IN('PR','VI') AND length(zip) = 3;

/Listing 9-16: Modifying codes in the zip column missing two leading zeros/

We use SET to set the zip column ➊ to a value that is the result of the concatenation of the string 00 and the existing content of the zip column. We limit the UPDATE to only those rows where the st column has the state codes PR and VI ➋ using the IN comparison operator from [[file:ch02.xhtml#ch02][Chapter 2]] and add a test for rows where the length of zip is 3. This entire statement will then only update the zip values for Puerto Rico and the Virgin Islands. Run the query; PostgreSQL should return the message UPDATE 86, which is the number of rows we expect to change based on our earlier count in [[file:ch09.xhtml#ch09list6][Listing 9-6]].

Let's repair the remaining ZIP Codes using a similar query in [[file:ch09.xhtml#ch09list17][Listing 9-17]]:

UPDATE meat_poultry_egg_inspect
SET zip = '0' || zip
WHERE st IN('CT','MA','ME','NH','NJ','RI','VT') AND length(zip) = 4;

/Listing 9-17: Modifying codes in the zip column missing one leading zero/

PostgreSQL should return the message UPDATE 496. Now, let's check our progress. Earlier in the chapter, when we aggregated rows in the zip column by length, we found 86 rows with three characters and 496 with four:

length    count
------    -----
     3       86
     4      496
     5     5705

Using the same query in [[file:ch09.xhtml#ch09list6][Listing 9-6]] now returns a more desirable result: all the rows have a five-digit ZIP Code.

length    count
------    -----
     5     6287

In this example we used concatenation, but you can employ additional SQL string functions to modify data with UPDATE by changing words from uppercase to lowercase, trimming unwanted spaces, replacing characters in a string, and more. I'll discuss additional string functions in [[file:ch13.xhtml#ch13][Chapter 13]] when we consider advanced techniques for working with text.

 /Updating Values Across Tables/


In [[file:ch09.xhtml#lev142][“Modifying Values with UPDATE”]] on [[file:ch09.xhtml#page_138][page 138]], I showed the standard ANSI SQL and PostgreSQL-specific syntax for updating values in one table based on values in another. This syntax is particularly valuable in a relational database where primary keys and foreign keys establish table relationships. It's also useful when data in one table may be necessary context for updating values in another.

For example, let's say we're setting an inspection date for each of the companies in our table. We want to do this by U.S. regions, such as Northeast, Pacific, and so on, but those regional designations don't exist in our table. However, they /do/ exist in a data set we can add to our database that also contains matching st state codes. This means we can use that other data as part of our UPDATE statement to provide the necessary information. Let's begin with the New England region to see how this works.

Enter the code in [[file:ch09.xhtml#ch09list18][Listing 9-18]], which contains the SQL statements to create a state_regions table and fill the table with data:

CREATE TABLE state_regions (
    st varchar(2) CONSTRAINT st_key PRIMARY KEY,
    region varchar(20) NOT NULL
);

COPY state_regions
FROM '/C:YourDirectory/state_regions.csv'
WITH (FORMAT CSV, HEADER, DELIMITER ',');

/Listing 9-18: Creating and filling a state_regions table/

We'll create two columns in a state_regions table: one containing the two-character state code st and the other containing the region name. We set the primary key constraint to the st column, which holds a unique st_key value to identify each state. In the data you're importing, each state is present and assigned to a U.S. Census region, and territories outside the United States are labeled as outlying areas. We'll update the table one region at a time.

Next, let's return to the meat_poultry_egg_inspect table, add a column for inspection dates, and then fill in that column with the New England states. [[file:ch09.xhtml#ch09list19][Listing 9-19]] shows the code:

  ALTER TABLE meat_poultry_egg_inspect ADD COLUMN inspection_date date;

➊ UPDATE meat_poultry_egg_inspect inspect
➋ SET inspection_date = '2019-12-01'
➌ WHERE EXISTS (SELECT state_regions.region
                FROM state_regions
                WHERE inspect.st = state_regions.st
                      AND state_regions.region = 'New England');

/Listing 9-19: Adding and updating an inspection_date column/

The ALTER TABLE statement creates the inspection_date column in the meat_poultry_egg_inspect table. In the UPDATE statement, we start by naming the table using an alias of inspect to make the code easier to read ➊. Next, the SET clause assigns a date value of 2019-12-01 to the new inspection_date column ➋. Finally, the WHERE EXISTS clause includes a subquery that connects the meat_poultry_egg_inspect table to the state_regions table we created in [[file:ch09.xhtml#ch09list18][Listing 9-18]] and specifies which rows to update ➌. The subquery (in parentheses, beginning with SELECT) looks for rows in the state_regions table where the region column matches the string New England. At the same time, it joins the meat_poultry_egg_inspect table with the state_regions table using the st column from both tables. In effect, the query is telling the database to find all the st codes that correspond to the New England region and use those codes to filter the update.

When you run the code, you should receive a message of UPDATE 252, which is the number of companies in New England. You can use the code in [[file:ch09.xhtml#ch09list20][Listing 9-20]] to see the effect of the change:

SELECT st, inspection_date
FROM meat_poultry_egg_inspect
GROUP BY st, inspection_date
ORDER BY st;

/Listing 9-20: Viewing updated inspection_date values/

The results should show the updated inspection dates for all New England companies. The top of the output shows Connecticut has received a date, for example, but states outside New England remain NULL because we haven't updated them yet:

st    inspection_date
--    ---------------
/--snip--/
CA
CO
CT    2019-12-01
DC
/--snip--/

To fill in dates for additional regions, substitute a different region for New England in [[file:ch09.xhtml#ch09list19][Listing 9-19]] and rerun the query.

** Deleting Unnecessary Data


The most irrevocable way to modify data is to remove it entirely. SQL includes options to remove rows and columns from a table along with options to delete an entire table or database. We want to perform these operations with caution, removing only data or tables we don't need. Without a backup, the data is gone for good.

*NOTE*

/It's easy to exclude unwanted data in queries using a WHERE clause, so decide whether you truly need to delete the data or can just filter it out. Cases where deleting may be the best solution include data with errors or data imported incorrectly./

In this section, we'll use a variety of SQL statements to delete unnecessary data. For removing rows from a table, we'll use the DELETE FROM statement. To remove a column from a table, we'll use ALTER TABLE. And to remove a whole table from the database, we'll use the DROP TABLE statement.

Writing and executing these statements is fairly simple, but doing so comes with a caveat. If deleting rows, a column, or a table would cause a violation of a constraint, such as the foreign key constraint covered in [[file:ch07.xhtml#ch07][Chapter 7]], you need to deal with that constraint first. That might involve removing the constraint, deleting data in another table, or deleting another table. Each case is unique and will require a different way to work around the constraint.

 /Deleting Rows from a Table/


Using a DELETE FROM statement, we can remove all rows from a table, or we can use a WHERE clause to delete only the portion that matches an expression we supply. To delete all rows from a table, use the following syntax:

DELETE FROM /table_name/;

If your table has a large number of rows, it might be faster to erase the table and create a fresh version using the original CREATE TABLE statement. To erase the table, use the DROP TABLE command discussed in [[file:ch09.xhtml#lev154][“Deleting a Table from a Database”]] on [[file:ch09.xhtml#page_148][page 148]].

To remove only selected rows, add a WHERE clause along with the matching value or pattern to specify which ones you want to delete:

DELETE FROM /table_name/ WHERE expression;

For example, if we want our table of meat, poultry, and egg processors to include only establishments in the 50 U.S. states, we can remove the companies in Puerto Rico and the Virgin Islands from the table using the code in [[file:ch09.xhtml#ch09list21][Listing 9-21]]:

DELETE FROM meat_poultry_egg_inspect
WHERE st IN('PR','VI');

/Listing 9-21: Deleting rows matching an expression/

Run the code; PostgreSQL should return the message DELETE 86. This means the 86 rows where the st column held either PR or VI have been removed from the table.

 /Deleting a Column from a Table/


While working on the zip column in the meat_poultry_egg_inspect table earlier in this chapter, we created a backup column called zip_copy. Now that we've finished working on fixing the issues in zip, we no longer need zip_copy. We can remove the backup column, including all the data within the column, from the table by using the DROP keyword in the ALTER TABLE statement.

The syntax for removing a column is similar to other ALTER TABLE statements:

ALTER TABLE /table_name/ DROP COLUMN /column_name/;

The code in [[file:ch09.xhtml#ch09list22][Listing 9-22]] removes the zip_copy column:

ALTER TABLE meat_poultry_egg_inspect DROP COLUMN zip_copy;

/Listing 9-22: Removing a column from a table using DROP/

PostgreSQL returns the message ALTER TABLE, and the zip_copy column should be deleted.

 /Deleting a Table from a Database/


The DROP TABLE statement is a standard ANSI SQL feature that deletes a table from the database. This statement might come in handy if, for example, you have a collection of backups, or /working tables/, that have outlived their usefulness. It's also useful in other situations, such as when you need to change the structure of a table significantly; in that case, rather than using too many ALTER TABLE statements, you can just remove the table and create another one by running a new CREATE TABLE statement.

The syntax for the DROP TABLE command is simple:

DROP TABLE /table_name/;

For example, [[file:ch09.xhtml#ch09list23][Listing 9-23]] deletes the backup version of the meat_poultry_egg_inspect table:

DROP TABLE meat_poultry_egg_inspect_backup;

/Listing 9-23: Removing a table from a database using DROP/

Run the query; PostgreSQL should respond with the message DROP TABLE to indicate the table has been removed.

** Using Transaction Blocks to Save or Revert Changes


The alterations you made on data using the techniques in this chapter so far are final. That is, after you run a DELETE or UPDATE query (or any other query that alters your data or database structure), the only way to undo the change is to restore from a backup. However, you can check your changes before finalizing them and cancel the change if it's not what you intended. You do this by wrapping the SQL statement within a /transaction block/, which is a group of statements you define using the following keywords at the beginning and end of the query:

*START TRANSACTION* signals the start of the transaction block. In PostgreSQL, you can also use the non-ANSI SQL *BEGIN* keyword.

*COMMIT* signals the end of the block and saves all changes.

*ROLLBACK* signals the end of the block and reverts all changes.

Usually, database programmers employ a transaction block to define the start and end of a sequence of operations that perform one unit of work in a database. An example is when you purchase tickets to a Broadway show. A successful transaction might involve two steps: charging your credit card and reserving your seats so someone else can't buy them. A database programmer would either want both steps in the transaction to happen (say, when your card charge goes through) or neither of them to happen (if your card is declined or you cancel at checkout). Defining both steps as one transaction keeps them as a unit; if one step fails, the other is canceled too. You can learn more details about transactions and PostgreSQL at /[[https://www.postgresql.org/docs/current/static/tutorial-transactions.html]]/.

We can apply this transaction block technique to review changes a query makes and then decide whether to keep or discard them. Using the meat_poultry_egg_inspect table, let's say we're cleaning dirty data related to the company AGRO Merchants Oakland LLC. The table has three rows listing the company, but one row has an extra comma in the name:

company
---------------------------
AGRO Merchants Oakland LLC
AGRO Merchants Oakland LLC
AGRO Merchants Oakland, LLC

We want the name to be consistent, so we'll remove the comma from the third row using an UPDATE query, as we did earlier. But this time we'll check the result of our update before we make it final (and we'll purposely make a mistake we want to discard). [[file:ch09.xhtml#ch09list24][Listing 9-24]] shows how to do this using a transaction block:

➊ START TRANSACTION;

  UPDATE meat_poultry_egg_inspect
➋ SET company = 'AGRO Merchantss Oakland LLC'
  WHERE company = 'AGRO Merchants Oakland, LLC';

➌ SELECT company
  FROM meat_poultry_egg_inspect
  WHERE company LIKE 'AGRO%'
  ORDER BY company;

➍ ROLLBACK;

/Listing 9-24: Demonstrating a transaction block/

We'll run each statement separately, beginning with START TRANSACTION; ➊. The database responds with the message START TRANSACTION, letting you know that any succeeding changes you make to data will not be made permanent unless you issue a COMMIT command. Next, we run the UPDATE statement, which changes the company name in the row where it has an extra comma. I intentionally added an extra s in the name used in the SET clause ➋ to introduce a mistake.

When we view the names of companies starting with the letters AGRO using the SELECT statement ➌, we see that, oops, one company name is misspelled now:

company
---------------------------
AGRO Merchants Oakland LLC
AGRO Merchants Oakland LLC
AGRO Merchantss Oakland LLC

Instead of rerunning the UPDATE statement to fix the typo, we can simply discard the change by running the ROLLBACK; ➍ command. When we rerun the SELECT statement to view the company names, we're back to where we started:

company
---------------------------
AGRO Merchants Oakland LLC
AGRO Merchants Oakland LLC
AGRO Merchants Oakland, LLC

From here, you could correct your UPDATE statement by removing the extra s and rerun it, beginning with the START TRANSACTION statement again. If you're happy with the changes, run COMMIT; to make them permanent.

*NOTE*

/When you start a transaction, any changes you make to the data aren't visible to other database users until you execute COMMIT./

Transaction blocks are often used in more complex database systems. Here you've used them to try a query and either accept or reject the changes, saving you time and headaches. Next, let's look at another way to save time when updating lots of data.

** Improving Performance When Updating Large Tables


Because of how PostgreSQL works internally, adding a column to a table and filling it with values can quickly inflate the table's size. The reason is that the database creates a new version of the existing row each time a value is updated, but it doesn't delete the old row version. (You'll learn how to clean up these old rows when I discuss database maintenance in [[file:ch17.xhtml#lev303][“Recovering Unused Space with VACUUM”]] on [[file:ch17.xhtml#page_314][page 314]].) For small data sets, the increase is negligible, but for tables with hundreds of thousands or millions of rows, the time required to update rows and the resulting extra disk usage can be substantial.

Instead of adding a column and filling it with values, we can save disk space by copying the entire table and adding a populated column during the operation. Then, we rename the tables so the copy replaces the original, and the original becomes a backup.

[[file:ch09.xhtml#ch09list25][Listing 9-25]] shows how to copy meat_poultry_egg_inspect into a new table while adding a populated column. To do this, first drop the meat_poultry_egg_inspect_backup table we made earlier. Then run the CREATE TABLE statement.

  CREATE TABLE meat_poultry_egg_inspect_backup AS
➊ SELECT *,
       ➋ '2018-02-07'::date AS reviewed_date
  FROM meat_poultry_egg_inspect;

/Listing 9-25: Backing up a table while adding and filling a new column/

The query is a modified version of the backup script in [[file:ch09.xhtml#ch09list8][Listing 9-8]]. Here, in addition to selecting all the columns using the asterisk wildcard ➊, we also add a column called reviewed_date by providing a value cast as a date data type ➋ and the AS keyword. That syntax adds and fills reviewed_date, which we might use to track the last time we checked the status of each plant.

Then we use [[file:ch09.xhtml#ch09list26][Listing 9-26]] to swap the table names:

➊ ALTER TABLE meat_poultry_egg_inspect RENAME TO meat_poultry_egg_inspect_temp;
➋ ALTER TABLE meat_poultry_egg_inspect_backup
    RENAME TO meat_poultry_egg_inspect;
➌ ALTER TABLE meat_poultry_egg_inspect_temp
    RENAME TO meat_poultry_egg_inspect_backup;

/Listing 9-26: Swapping table names using ALTER TABLE/

Here we use ALTER TABLE with a RENAME TO clause to change a table name. Then we use the first statement to change the original table name to one that ends with _temp ➊. The second statement renames the copy we made with [[file:ch09.xhtml#ch09list24][Listing 9-24]] to the original name of the table ➋. Finally, we rename the table that ends with _temp to the ending _backup ➌. The original table is now called meat_poultry_egg_inspect_backup, and the copy with the added column is called meat_poultry_egg_inspect.

By using this process, we avoid updating rows and having the database inflate the size of the table. When we eventually drop the _backup table, the remaining data table is smaller and does not require cleanup.

** Wrapping Up


Gleaning useful information from data sometimes requires modifying the data to remove inconsistencies, fix errors, and make it more suitable for supporting an accurate analysis. In this chapter you learned some useful tools to help you assess dirty data and clean it up. In a perfect world, all data sets would arrive with everything clean and complete. But such a perfect world doesn't exist, so the ability to alter, update, and delete data is indispensable.

Let me restate the important tasks of working safely. Be sure to back up your tables before you start making changes. Make copies of your columns, too, for an extra level of protection. When I discuss database maintenance for PostgreSQL later in the book, you'll learn how to back up entire databases. These few steps of precaution will save you a world of pain.

In the next chapter, we'll return to math to explore some of SQL's advanced statistical functions and techniques for analysis.


*TRY IT YOURSELF*

In this exercise, you'll turn the meat_poultry_egg_inspect table into useful information. You need to answer two questions: how many of the plants in the table process meat, and how many process poultry?

The answers to these two questions lie in the activities column. Unfortunately, the column contains an assortment of text with inconsistent input. Here's an example of the kind of text you'll find in the activities column:

Poultry Processing, Poultry Slaughter
Meat Processing, Poultry Processing
Poultry Processing, Poultry Slaughter

The mishmash of text makes it impossible to perform a typical count that would allow you to group processing plants by activity. However, you can make some modifications to fix this data. Your tasks are as follows:

1. Create two new columns called meat_processing and poultry_processing in your table. Each can be of the type boolean.

2. Using UPDATE, set meat_processing = TRUE on any row where the activities column contains the text /Meat Processing/. Do the same update on the poultry_processing column, but this time look for the text /Poultry Processing/ in activities.

3. Use the data from the new, updated columns to count how many plants perform each type of activity. For a bonus challenge, count how many plants perform both activities.


ally drop the \_backup table, the remaining data table is smaller and does not require cleanup.

**** Wrapping Up
    :PROPERTIES:
    :CUSTOM_ID: lev157
    :CLASS: h3
    :END:

Gleaning useful information from data sometimes requires modifying the data to remove inconsistencies, fix errors, and make it more suitable for supporting an accurate analysis. In this chapter you learned some useful tools to help you assess dirty data and clean it up. In a perfect world, all data sets would arrive with everything clean and complete. But such a perfect world doesn't exist, so the ability to alter, update, and delete data is indispensable.

Let me restate the important tasks of working safely. Be sure to back up your tables before you start making changes. Make copies of your columns, too, for an extra level of protection. When I discuss database maintenance for PostgreSQL later in the book, you'll learn how to back up entire databases. These few steps of precaution will save you a world of pain.

In the next chapter, we'll return to math to explore some of SQL's advanced statistical functions and techniques for analysis.

<<ch09sb1>>
*TRY IT YOURSELF*

In this exercise, you'll turn the meat\_poultry\_egg\_inspect table into useful information. You need to answer two questions: how many of the plants in the table process meat, and how many process poultry?

The answers to these two questions lie in the activities column. Unfortunately, the column contains an assortment of text with inconsistent input. Here's an example of the kind of text you'll find in the activities column:

Poultry Processing, Poultry Slaughter\\
Meat Processing, Poultry Processing\\
Poultry Processing, Poultry Slaughter

The mishmash of text makes it impossible to perform a typical count that would allow you to group processing plants by activity. However, you can make some modifications to fix this data. Your tasks are as follows:

1. Create two new columns called meat\_processing and poultry\_processing in your table. Each can be of the type boolean.

2. Using UPDATE, set meat\_processing = TRUE on any row where the activities column contains the text /Meat Processing/. Do the same update on the poultry\_processing column, but this time look for the text /Poultry Processing/ in activities.

3. Use the data from the new, updated columns to count how many plants perform each type of activity. For a bonus challenge, count how many plants perform both activities.


